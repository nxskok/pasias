## One-sample cholesterol

 The data set [here](http://ritsokiguess.site/datafiles/cholest.csv) contains cholesterol
measurements for heart attack patients (at several different times) as
well as for a group of control patients. We will focus on the control
patients in this question. 


(a) Read in and display (some of) the data.

Solution

This is (as you might guess) a \verb=.csv=, so:

```{r}
my_url <- "http://ritsokiguess.site/datafiles/cholest.csv"
cholest <- read_csv(my_url)
cholest
```     

Note for yourself that there are 30 observations (and some missing
ones), and a column called \verb=control= that is the one we'll be
working with.

Extra: the 2-day, 4-day and 14-day columns need to be referred to with funny "backticks" around their names, because a column name cannot contain a `-` or start with a number. This is not a problem here, since we won't be using those columns, but if we wanted to, this would not work:

```{r, error=TRUE}
cholest %>% summarize(xbar = mean(2-Day))
```

because it is looking for a column called `Day`, which doesn't exist. The meaning of `2-Day` is "take the column called `Day` and subtract it from 2". To make this work, we have to supply the backticks ourselves:

```{r}
cholest %>% summarize(xbar = mean(`2-Day`, na.rm = TRUE))
```

This column also has missing values (at the bottom), so here I've asked to remove the missing values^[In R, missing values are labelled `NA`, and `rm` is Unix/C shorthand for *remove*.] before working out the mean. Otherwise the mean is, unhelpfully, missing as well.

You might imagine that dealing with column names like this would get annoying. There is a package called `janitor` that has a function called `clean_names` to save you the trouble. Install it first, then load it:

```{r}
library(janitor)
```

and then pipe your dataframe into `clean_names` and see what happens:

```{r}
cholest %>% clean_names() -> cholest1
cholest1
```

These are all legit column names; the `-` has been replaced by an underscore, and each of the first three column names has gained an `x` on the front so that it no longer starts with a number. This then works:

```{r}
cholest1 %>% summarize(xbar = mean(x2_day, na.rm = TRUE))
```



$\blacksquare$


(b) Make a suitable plot of the cholesterol levels of the
control patients, and comment briefly on the shape of the
distribution. 

Solution

There is one quantitative variable, so a histogram, as ever:

```{r luprands}
ggplot(cholest, aes(x=control)) + geom_histogram(bins=6)
```     

Pick a number of bins that shows the shape reasonably well. Too many
or too few won't. (Sturges' rule says 6, since there are 30
observations and $2^5=32$.) Seven bins also works, but by the time you
get to 8 bins or more, you are starting to lose a clear picture of the
shape. Four bins is, likewise, about as low as you can go before
getting too crude a picture. 

Choosing one of these numbers of bins will make it clear that the
distribution is somewhat skewed to the right.



$\blacksquare$


(c) It is recommended that people in good health, such as the
Control patients here, keep their cholesterol level below 200. Is
there evidence that the mean cholesterol level of the population of
people of which the Control patients are a sample is less than 200? Show that you understand the process,
and state your conclusion in the context of the data.

Solution

The word "evidence" means to do a hypothesis test and get a
P-value. Choose an $\alpha$ first, such as 0.05. 

Testing a mean implies a one-sample $t$-test. We are trying to
prove that the mean is less than 200, so that's our alternative:
$H_a: \mu < 200$, and therefore the null is that the mean is equal
to 200: $H_0: \mu = 200$. (You might think it makes more logical
sense to have $H_0: \mu \ge 200$, which is also fine. As long as
the null hypothesis has an equals in it in a logical place, you
are good.)

```{r}
with(cholest, t.test(control, mu=200, alternative = "less"))
```

This is also good:

```{r}
t.test(cholest$control, mu=200, alternative = "less")
``` 

I like the first version better because a lot of what we do later
involves giving a data frame, and then working with things in that
data frame. This is more like that.

This test is *one*-sided because we are looking for evidence of
*less*; if the mean is actually *more* than 200, we don't
care about that. For a one-sided test, R requires you to say which
side you are testing.

The P-value is not (quite) less than 0.05, so we cannot quite reject
the null. Therefore, there is no evidence that the mean cholesterol
level (of the people of which the control group are a sample) is less
than 200. Or, this mean is not significantly less than 200. Or, we
conclude that this mean is equal to 200. Or, we conclude that this
mean could be 200. Any of those.

If you chose a different $\alpha$, draw the right conclusion for the
$\alpha$ you chose. For example, with $\alpha=0.10$, we *do* have
evidence that the mean is less than 200. Being consistent is more
important than getting the same answer as me.  

Writing out all the steps correctly shows that you understand the process. Anything less doesn't. 


$\blacksquare$


(d) What values could the population mean cholesterol level take? You
might need to get some more output to determine this.

Solution


This is *not* quoting the sample mean, giving that as your answer, and then stopping. The sample mean should, we hope, be somewhere the population mean, but it is almost certainly not the same as the population mean, because there is variability due to random sampling. (This is perhaps the most important thing in all of Statistics: recognizing that variability exists and dealing with it.)

With that in mind, the question means to get a range of values that the population mean could
be: that is to say, a confidence interval. The one that came out
of the previous output is one-sided, to go with the one-sided
test, but confidence intervals for us are two-sided, so we have to
run the test again, but two-sided, to get it. To do that, take out
the "alternative", thus (you can also take out the null mean,
since a confidence interval has no null hypothesis):

```{r}
with(cholest, t.test(control))
```     

With 95\% confidence, the population mean cholesterol level is between
184.8 and 201.5. 

*You need to state the interval, and you also need to round off
the decimal places to something sensible.* This is because in your
statistical life, you are providing results to someone else *in a
manner that they can read and understand.* They do not have time to go
searching in some output, or to fish through some excessive number of
decimal places. If that's what you give them, they will ask you to
rewrite your report, wasting everybody's time when you could have done
it right the first time.

How many decimal places is a good number? Look back at your data. In
this case, the cholesterol values are whole numbers (zero decimal
places). A confidence interval is talking about a mean. In this case,
we have a sample size of 30, which is between 10 and 100, so we can
justify one extra decimal place beyond the data, here one decimal
altogether, or two *at the absolute outside*. (Two is more
justifiable if the sample size is bigger than 100.) See, for example,
[this](https://www2.southeastern.edu/Academics/Faculty/dgurney/Math241/StatTopics/SciNot.htm), 
in particular the piece at the bottom.

$\blacksquare$


(e) Explain briefly why you would be reasonably happy to trust
the $t$ procedures in this question. (There are two points you need
to make.)

Solution

The first thing is to look back at the graph you made
earlier. This was skewed to the right ("moderately" or
"somewhat" or however you described it). This would seem to say
that the $t$ procedures were not very trustworthy, since the
population distribution doesn't look very normal in shape.

However, the second thing is to look at the sample size. We have
the central limit theorem, which says (for us) that the larger the
sample is, the less the normality matters, when it comes to
estimating the mean. Here, the sample size is 30, which, for the
central limit theorem, is large enough to overcome moderate
non-normality in the data.

My take, which I was trying to guide you towards, is that our
non-normality was not too bad, and so our sample size is large
enough to trust the $t$ procedures we used.

Extra 1: \textbf{There is nothing magical about a sample size of 30.}
What matters is the tradeoff between sample size and the extent of
the non-normality. If your data is less normal, you need a larger
sample size to overcome it. Even a sample size of 500 might not be
enough if your distribution is very skewed, or if you have extreme
outliers. 

The place $n=30$ comes from is back from the days when we only
ever used printed tables. In most textbooks, if you printed the
$t$-table on one page in a decent-sized font, you'd get to about
29 df before running out of space. Then they would say "$\infty$
df" and put the normal-distribution $z$ numbers in. If the df you
needed was bigger than what you had in the table, you used this
last line: that is, you called the sample "large". Try it in
your stats textbooks: I bet the df go up to 30, then you get a few
more, then the $z$ numbers. 

Extra 2: By now you are probably thinking that this is very
subjective, and so it is. What actually matters is the shape of
the thing called the *sampling distribution of the sample mean*. That is to say, what kind of sample means you might get
in repeated samples from your population. The problem is that you
don't know what the population looks like.^[If you did, all
your problems would be over.] But we can fake it up, in a couple
of ways: we can play what-if and pretend we know what the population
looks like (to get some understanding for "populations like
that"), or we can use a technique called the "bootstrap" that
will tell us what kind of sample means we might get from the
population that *our* sample came from (this seems like magic
and, indeed, is).

The moral of the story is that the central limit theorem is more
powerful than you think.

To illustrate my first idea, let's pretend the population looks like this,
with a flat top:

```{r, echo=F}
set.seed(457299)
toplot <- tribble(
~x, ~y,
-0.1, 0,
0,0,
0,1,
1,1,
1,0,
1.1,0
)
ggplot(toplot, aes(x=x, y=y)) + geom_line()
```   

Only values between 0 and 1 are possible, and each of those is equally
likely. Not very normal in shape. So let's take some random samples of
size *three*, not in any sense a large sample, from this "uniform"
population, and see what kind of sample means we get. This technique
is called **simulation**: rather than working out the answer by
math, we're letting the computer approximate the answer for us. Here's
one simulated sample:

```{r}
u <- runif(3)
u
mean(u)
``` 

and here's the same thing 1000 times, including a histogram of the
sample means:

```{r toeber}
tibble(sim = 1:1000) %>% 
  rowwise() %>% 
  mutate(my_sample = list(runif(3))) %>% 
  mutate(my_mean = mean(my_sample)) %>% 
  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12)
``` 

This is our computer-generated assessment of what the sampling
distribution of the sample mean looks like. Isn't this looking like a
normal distribution?

Let's take a moment to realize what this is saying. If the population
looks like the flat-topped uniform distribution, the central limit
theorem kicks in for a sample of size *three*, and thus if your
population looks like this, $t$ procedures will be perfectly good for
$n=3$ or bigger, *even though the population isn't normal*.

Thus, when you're thinking about whether to use a $t$-test or
something else (that we'll learn about later), the distribution shape
matters, *but so does the sample size*.

I should say a little about my code. I'm not expecting you to figure
out details now (we see the ideas properly in simulating power of
tests), but in words, one line at a time:

\begin{itemize}
\item generate 1000 ("many") samples each of 3 observations from a
uniform distribution
\item for each sample, work out the mean of it
\item turn those sample means into a data frame with a column called
\verb=value=
\item make a histogram of those.
\end{itemize}

Now, the central limit theorem doesn't always work as nicely as this,
but maybe a sample size of 30 is large enough to overcome the skewness
that we had:

```{r schauffhusen}
ggplot(cholest, aes(x=control)) + geom_histogram(bins=6)
```     

That brings us to my second idea above.

The sample that we had is in some sense an "estimate of the
population". To think about the sampling distribution of the sample
mean, we need more estimates of the population. How might we get
those? The curious answer is to *sample from the sample*. This is
the idea behind the *bootstrap*. (This is what Lecture 3c is about.) The name comes from the
expression "pulling yourself up by your own bootstraps", meaning
"to begin an enterprise or recover from a setback without any outside
help" (from [here](https://www.yourdictionary.com/pull-oneself-up-by-one-s-bootstraps)), 
something that should be difficult or impossible. How is it
possible to understand a sampling distribution with only one sample?

We have to be a bit careful. Taking a sample from the sample would
give us the original sample back. So, instead, we sample 
*with replacement*, so that each bootstrap sample is different:

```{r}
sort(cholest$control)
sort(sample(cholest$control, replace=TRUE))
``` 

A bootstrap sample contains repeats of the original data values, and
misses some of the others. Here, the original data had values 160 and 162 that are missing in the bootstrap sample; the original data had one value 166,  but the bootstrap sample has *four*!
I sorted the data and the bootstrap sample
to make this clearer; you will not need to sort. This is a perfectly good bootstrap sample:

```{r}
sample(cholest$control, replace = TRUE)
```


So now we know what to do: take lots of bootstrap samples, work out
the mean of each, plot the means, and see how normal it looks. The
only new idea here is the sampling with replacement:

```{r schuomarchers}
tibble(sim = 1:1000) %>% 
  rowwise() %>% 
  mutate(my_sample = list(sample(cholest$control, replace = TRUE))) %>% 
  mutate(my_mean = mean(my_sample)) %>% 
  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12)
``` 

That looks pretty normal, not obviously skewed, and so the $t$
procedures we used will be reliable enough.


$\blacksquare$



