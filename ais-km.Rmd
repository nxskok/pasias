##  Clustering the Australian athletes


 Recall the Australian athlete data (that we've seen so many
times before). This time, we'll do some K-means clustering, and then
see whether athletes of certain genders and certain sports tend to end
up in the same cluster.



(a) Read in the data from
[link](http://www.utsc.utoronto.ca/~butler/c32/ais.txt), recalling
that the data values are separated by tabs. Display (some of) the
data set.

Solution


So, `read_tsv`. 
```{r }
my_url <- "http://www.utsc.utoronto.ca/~butler/c32/ais.txt"
athletes <- read_tsv(my_url)
athletes
```

     


(b) From your data frame, select only the columns that are numbers
(or get rid of the ones that are text), and standardize all of the
columns you have left. This is, done the best way, a slick piece of
code. Display what you get.

Solution


This, in fact:
```{r }
athletes %>% select_if(is.numeric) %>% mutate_all(~ scale(.)) -> athletes.s
athletes.s
```

 

The columns have weird names, possibly because `scale` expects
a matrix or data frame (to standardize each column), and here it's
getting the columns one at a time.

Elsewhere, I stuck `scale()` on the end, which produces a
*matrix*, which I should then display the top of (it has 200-plus rows):

```{r }
athletes %>% select_if(is.numeric) %>% scale() %>% head()
```

 

I (at this moment) like the first one better, but these preferences
tend to change over time (as will yours).

The first athlete has a `WCC` value that is very large compared
to the others.


(c) Make a data frame that contains the total within-cluster sum
of squares from a K-means clustering for each number of clusters
from 2 to 20.

Solution


I'm going to attempt a slick way of doing this, and then I'll talk
about how I'd expect *you* to tackle this. First, though, I
set the random number seed so that everything comes out the same
every time I run it:
```{r }
set.seed(457299)
```

     
Here we go:
```{r }
withinss <- tibble(clusters = 2:20) %>%
  rowwise() %>% 
  mutate(wss = kmeans(athletes.s, clusters, nstart = 20)$tot.withinss)
withinss
```

     

A one-liner, kinda. Remember that `kmeans` expects a single number of clusters, a value like 5, rather than a collection of possible numbers of clusters in a vector, so to do each of them, we need to work rowwise (and do one row at a time).

The advantage to this is that it looks exactly like
the `kmeans` that you would write.

All right then, what is a better way to do this? First write
a function to take a number of clusters and a data frame and return
the total within-cluster sum of squares:

```{r }
twss <- function(i, x) {
  ans <- kmeans(x, i, nstart = 20)
  ans$tot.withinss
}
```

 

and test it (against my answer above):

```{r }
twss(3, athletes.s)
```

 

Check (with a few extra decimals).

Then calculate
all the total within-cluster sum of squares values by making a little
data frame with all your numbers of clusters:

```{r }
tibble(clusters = 2:20)
```

 
and then make a pipeline and save it, using `rowwise` and your function:

```{r }
tibble(clusters = 2:20) %>%
  rowwise() %>% 
  mutate(wss = twss(clusters, athletes.s)) -> withinss
withinss
```

This is better because the `mutate` line is simpler; you have off-loaded the details of the thinking to your function. Read this as "for each number of clusters, work out the total within-cluster sum of squares for that number of clusters." The important thing here is what you are doing, not how you are doing it; if you care about the how-you-are-doing-it, go back and look at your function. Remember that business about how you can only keep track of seven things, plus or minus two, at once? When you write a function, you are saving some of the things you have to keep track of.


(d) Use the data frame you just created to make a scree plot. What
does the scree plot tell you?

Solution


`ggscreeplot` is for principal components; this one you can
plot directly, with the points joined by lines:
```{r }
ggplot(withinss, aes(x = clusters, y = wss)) + geom_point() + geom_line()
```

     

On this plot, you are looking for "elbows", but ones sufficiently
far down the mountain. For example, that's an elbow at 4 clusters, but
it's still up the mountain, which means that the total within-cluster
sum of squares is quite large and that the athletes within those 4
clusters might be quite dissimilar from each other. I see an elbow at
12 clusters and possibly others at 14, 16 and 19; these are nearer the bottom
of the mountain, so that the athletes within a cluster will be quite
similar to each other. With over 200 athletes, there's no problem
having as many as 19 clusters, because that will still offer you some
insight. 

So I'm thinking 12 clusters (because I want to have a fairly small
number of clusters to interpret later).

The other thing I'm thinking is I could have put a bigger number of
clusters on the scree plot. The `wss` axis should go all the
way down to 0 for 202 clusters, with each athlete in one cluster. So
you could make the point that even 20 clusters is still a fair way up
the mountain.


(e) Using a sensible number of clusters as deduced from your scree
plot, run a K-means cluster analysis. Don't forget the
`nstart`! 

Solution


This:

```{r }
athletes.km <- kmeans(athletes.s, 12, nstart = 20)
```

 

or for your chosen number of clusters. 

I don't think there's any great need to display the output, since the
most interesting thing is which athletes are in which cluster, which
we'll get to next.


(f) Make a data frame consisting of the athletes' sport and
gender, and which of your clusters they belong to, taking the
appropriate things from the appropriate one of your data frames.

Solution


```{r }
athletes2 <- tibble(
  gender = athletes$Sex,
  sport = athletes$Sport,
  cluster = athletes.km$cluster
)
athletes2
```

     


(g) Using the data frame you created in the previous part, display
all the athletes in some of your clusters. Do the athletes within a
cluster appear to have anything in common? (If a cluster has more
than 10 athletes in it, make sure to look at them all.)

Solution


Let's start with my cluster 1. I'm putting a `print(n=Inf)`
on the end of each of these, to make sure all the cluster members
get shown, at least in the Console:
```{r }
athletes2 %>% filter(cluster == 1) %>% print(n = Inf)
```

     

These are almost all female, and if you remember back to our study of
height and weight for these data, these are the kinds of sport that
are played by shorter, lighter people.
Cluster 2:

```{r }
athletes2 %>% filter(cluster == 2) %>% print(n = Inf)
```

 

Males, apparently some of the more muscular ones, but not the field
athletes. 

```{r }
athletes2 %>% filter(cluster == 3) %>% print(n = Inf)
```

 

This is an odd one, since there is one male rower (rowers tend to be
fairly big) along with a bunch of females mostly from sports involving
running. I have a feeling this rower is a "cox", whose job is
*not* to row, but to sit in the boat and keep everybody in time
by yelling out "stroke" in rhythm. Since the cox is not rowing, they
need to be light in weight.

Let's investigate:

```{r }
athletes %>%
  select(gender = Sex, sport = Sport, ht = Ht, wt = Wt) %>%
  mutate(cluster = athletes.km$cluster) -> athletes2a
athletes2a %>% filter(sport == "Row", cluster == 3)
```

 

How does this athlete compare to the other rowers?

```{r }
athletes2a %>%
  filter(sport == "Row") %>%
  select(ht, wt) %>%
  summary()
```

 

The rower that is in cluster 3 is almost the lightest, and also almost
the shortest, of all the rowers.
Cluster 4:

```{r }
athletes2 %>% filter(cluster == 4) %>% print(n = Inf)
```

 

Males, but possibly more muscular ones.

```{r }
athletes2 %>% filter(cluster == 5) %>% print(n = Inf)
```

 

More males, from similar sports. I wonder what makes these last two
clusters different?

One more:

```{r }
athletes2 %>% filter(cluster == 6) %>% print(n = Inf)
```

 

These are three of our "big guys", by the looks of it.


(h) Add the cluster membership to the data frame you read in from
the file, and do a discriminant analysis treating the clusters as
known groups. You can display the output.

Solution


`MASS` is already loaded (for me), so:
```{r }
athletes.3 <- athletes %>%
  mutate(cluster = athletes.km$cluster) %>%
  lda(cluster ~ RCC + WCC + Hc + Hg + Ferr + BMI + SSF + `%Bfat` + LBM + Ht + Wt, data = .)
```

     

We can display all the output now. The
problem here, with 12 groups and 11 variables, is that there is rather
a lot of it:

```{r }
athletes.3
```

 


(i) How many linear discriminants do you have? How many do you
think are important?

Solution


Proportion of trace, at the bottom of the output.

It's hard to draw the line here. The first two, or maybe the first
seven, or something like that. Your call.


(j) Which variables seem to be important in distinguishing the
clusters? Look only at the linear discriminants that you judged to
be important.

Solution


Look at the coefficients of linear discriminants.
This is rather large, since I had 12 clusters, and thus there are
11 `LD`s.

If we go back to my thought of only using two linear discriminants:
LD1 is mostly `RCC` positively and `BMI` negatively, in
that an athlete with large `RCC` and small `BMI` will
tend to score high (positive) on LD1. `BMI` is the familiar
body fat index. LD2 depends on `RCC` again, but this time
negatively, and maybe percent body fat and `LBM`. And so on, if
you went on.

It may be that `RCC` is just very variable anyway, since it
seems to appear just about everywhere.

Extra: we can also look at the means on each variable by cluster,
which is part of the output, in "Group Means".
Perhaps the easiest thing to eyeball here is the cluster in which a
variable is noticeably biggest (or possibly smallest). For example,
`WCC` is highest in cluster 4, and while Ferritin is high
there, it is higher still in cluster 5. `BMI` is highest in
cluster 6 and lowest in clusters 1 and 3. Height is smallest in
cluster 1, with weight being smallest there as well, and weight is
much the biggest in cluster 6. 


(k) Draw a biplot (which shows the first two LDs), drawing the
clusters in different colours. Comment briefly on anything
especially consistent or inconsistent with what you've seen so far.

Solution


The thing to get the colours is to feed a `groups` into
`ggbiplot`. I suspect I need the `factor` in there
because the clusters are numbers and I want them treated as
categorical (the numbers are labels). Also, note that we will have
a lot of colours here, so I am trying to make them more
distinguishable using `scale_colour_brewer` from the
`RColorBrewer` package (loaded at the beginning):
```{r }
ggbiplot(athletes.3, groups = factor(athletes2$cluster)) +
  scale_colour_brewer(palette = "Paired")
```

     

What the biplot shows, that we haven't seen any hint of so far, is
that the clusters are pretty well separated on LD1 and LD2: there is
not a great deal of overlap. 

Anyway, low LD1 means high on BMI and low on RCC, as we saw
before. The arrow for RCC points down as well as right, so it's part
of LD2 as well. There isn't much else that points up or down, but
percent body fat and LBM do as much as anything. This is all pretty
much what we saw before.

As to where the clusters fall on the picture:



* Cluster 1 in light blue was "small and light": small BMI, so
ought to be on the right. This cluster's RCC was also small, which
on balance puts them on the left, but then they should be *top*
left because RCC points down. I dunno.

* Cluster 2 in dark blue was "more muscular males", mid-right,
so above average on LD1 but about average on LD2.

* Cluster 3, light green, was "running females" (mostly), lower
left, so below average on both LD1 and LD2.

* Cluster 4, dark green, "more muscular males" again. There is a
lot of overlap with cluster 2.

* Cluster 5, pink, was "yet more males".  Mostly above average on
LD1 and below average on LD2. The latter was what distinguished
these from clusters 4 and 2.

* Cluster 6, red, was "big guys". The biggest on LD1 and almost
the biggest on LD2.


There is something a bit confusing in LD1, which contrasts RCC and
BMI. You would expect, therefore, RCC and BMI to be negatively
correlated, but if you look at the cluster means, that isn't really
the story: for example, cluster 1 has almost the lowest mean on both
variables, and the highest RCC, in cluster 11, goes with a middling
BMI. 

I like these colours much better than the default ones. Much easier to
tell apart.
In any case, RCC and BMI seem to be important, so let's plot them
against each other, coloured by cluster:

```{r }
athletes %>%
  mutate(cluster = factor(athletes2$cluster)) %>%
  ggplot(aes(x = RCC, y = BMI, colour = cluster)) +
  geom_point() + scale_colour_brewer(palette = "Paired")
```

 

I decided to create a column called `cluster` in the data
frame, so that the legend would have a nice clear title. (If you do
the `factor(athletes2$cluster)` in the `ggplot`, that
is what will appear as the legend title.)

There seems to be very little relationship here, in terms of an
overall trend on the plot. But at least these two variables do
*something* to distinguish the clusters. It's not as clear as
using LD1 and LD2 (as it won't be, since they're designed to be the
best at separating the groups), but you can see that the clusters are
at least somewhat distinct.

The "paired" part of the colour palette indicates that successive
colours come in pairs: light and dark of blue, green, red, orange,
purple and brown (if you think of yellow as being "light brown" or
brown as being "dark yellow", like bananas).

A good resource for RColorBrewer is
[link](https://moderndata.plot.ly/create-colorful-graphs-in-r-with-rcolorbrewer-and-plotly/). The
"qualitative palettes" shown there are for distinguishing groups
(what we want here); the sequential palettes are for distinguishing
values on a continuous scale, and the diverging palettes are for
drawing attention to high and low.



