\question A university requires all its incoming students to take a standardized test of their verbal and mathematical abilities. Each student receives a verbal and a math score that is a percentile of all the students that took each test: thus a score of 100 means that the student was the highest of everyone that took the test, and a score of 0 means that the student scored the lowest of everybody on that test. The university claims that it needs to know these test results because they do a good job of predicting each student's (cumulative) GPA at the end of first year. The two test scores and the GPA at the end of first year are recorded for a sample of students, and the results are in \url{http://ritsokiguess.site/datafiles/STAC32/gpa.csv}.

\bP

\p Read in and display (some of) the data. How many students were sampled?

\bS

No surprises here:

```{r}
my_url <- "http://ritsokiguess.site/datafiles/STAC32/gpa.csv"
students <- read_csv(my_url)
students
```

There are 40 rows and hence 40 students in the sample.

Just for a change, there is no data-organizing story here. The data came from an old textbook, and I did what any sane person would do: I copied the values into a spreadsheet and saved it as a `.csv`.

\eS

\p Make a suitable plot of the response variable against each of the explanatory variables.

\bS

This means figuring out that `gpa` had better be the response variable, because that comes at the end of the first year, and the other two test scores are known at the beginning. (The first-year GPA really is, or might be, a "response" to the student's test scores, and thus their presumed mathematical and verbal abilities.)

The best way to do this is in one shot, with facets. This means making the two $x$-columns longer first. I got lazy this time:

```{r}
students %>% 
  pivot_longer(-gpa)
```

If you leave out `names_to` and `values_to`, the columns get called `name` and `value`, which actually works perfectly well for our purposes here. Of course, it's also fine to give them more meaningful names.

Then, as we've seen elsewhere, we plot the response variable against whatever we called the values, and then facet by whatever we called the names of these variables. Using `scales="free"` is better because the $x$-variables might be on different scales, although here they are both percentiles and so it doesn't matter much. Or, argue that *because* they are both on the same scale, you don't need the `scales="free"` *here* even though you normally would. (This last is, I guess, showing the greatest awareness of what is going on.):


```{r}
students %>% 
  pivot_longer(-gpa) %>% 
  ggplot(aes(x = value, y = gpa)) + geom_point() +
  facet_wrap(~name, scales = "free")
```

If you cannot make the facets work, get the plots one at a time. It is better to get them both together, but it is better than nothing to get the graphs somehow.

\eS

\p What do you see on your plots? Explain briefly.

\bS

I see, in both cases, a weak upward trend. I also see an outlier on each plot; on the left one, at the bottom middle with a `math` score around 80 and the lowest `gpa`, and on the right one at the bottom right with a verbal score near 100.

I want you to be clear that these are plots of the *response* against the explanatory variables, and so you are hoping to see *a trend* on these. This is in sharp contrast to residual plots (later) where you are hoping to see *nothing*.

Extra: there is always a grey area between what is part of a weak trend and what is an outlier. My take is that all the other points are part of those weak trends, and only the points I named are clearly off the trends. 

Extra extra: the two outliers I named are two different students, because they have different `gpa` values: the lowest of all, something like 1.3, on the left, and just above 1.5 on the right.

\eS

\p Fit a suitable regression, and display the results.

\bS

The `gpa` is the response, and the other two variables are explanatory:

```{r}
gpa.1 <- lm(gpa ~ math + verbal, data = students)
summary(gpa.1)
```

\eS

\p Does it make practical sense that the Estimates for `math` and `verbal` are positive? Explain briefly.

\bS

The positive estimates mean that as math score increases (for any verbal score), or as verbal score increases (for any math score), predicted GPA goes up. That is to say, higher test scores are associated with a higher GPA. This makes sense, since you'd expect higher test scores to go with a better ability to learn.

You could be less charitable than this, but be careful: though the relationships on the scatterplots are weak, they are a lot stronger than chance, as evidenced by the small P-values. So you *cannot* say that these Estimates came out positive by chance; they are significantly positive, meaning that if you were to sample another set of students, you would almost certainly see positive estimates again.

\eS

\p Make the usual residual plots for a multiple regression. What do you think is the biggest problem revealed by your plots? Describe briefly how this problem shows up on your graphs.

\bS

The usual plots are three (or four, depending how you count):

- residuals against fitted values
- normal quantile plot of residuals
- residuals against the $x$s, here `verbal` and `math`.

The first two are straightforward:

```{r}
ggplot(gpa.1, aes(x = .fitted, y = .resid)) + geom_point()
```

and

```{r}
ggplot(gpa.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()
```

The plot of residuals against fitted looks pretty random, except for those two points at the bottom, which look *too* low and therefore seem to be outliers. This is supported by the normal quantile plot: all of the residuals look normal enough except for the two most negative ones, which are *too* negative.

With that in mind, you might expect to see those negative residuals on the residuals vs $x$s as well. Before we get to that, though, we have to make a dataframe with the residuals and the original data in it. This is most easily done using `augment` from the `broom` package; augment the model with the data, rather than the other way around:

```{r}
gpa.1 %>% augment(students)
```

and then the cleanest way to proceed is to pivot this longer, getting both $x$s in one column. I am doing this the lazy way again, getting columns called `name` and `value` by default:

```{r}
gpa.1 %>% augment(students) %>% 
  pivot_longer(c(verbal, math))
```

Then plot `gpa` against `value` facetted by `name`, as we did for the first plot in this question. Use `scales = "free"` or explain why you don't need to, as before:

```{r}
gpa.1 %>% augment(students) %>% 
  pivot_longer(c(verbal, math)) %>% 
  ggplot(aes(x = value, y = .resid)) + geom_point() +
  facet_wrap(~name, scales = "free")
```

Once again, this is best, but if you cannot get to this, do the plots one at a time. If you don't think of `augment`, make a dataframe yourself with all the right stuff in it, eg. like this, where this is the way to pull the residuals out as a column:

```{r}
students %>% mutate(residual = resid(gpa.1))
```

and then go ahead and make a plot out of this. I haven't done it this way in class, so if you find this idea somewhere, you should say where you got it from.

Anyway, the same two negative residuals show up at the bottom of these plots as well, with otherwise random scatter, supporting the conclusion we had before: those two low outliers are the biggest problem.

\eS

\p Display all the data for the observations with the two most negative residuals. 

\bS

According to all the plots, the two most negative residuals are the only ones below about $-0.75$, so the strategy is to get the data and the residuals in the same place and `filter`, most easily done by `augment` again:

```{r}
gpa.1 %>% augment(students) %>% 
  filter(.resid < -0.75)
```

This part is a bit of a giveaway that the problem you would find is something to do with residuals!

\eS

\p What does it mean in the context of these data for an observation to have a very negative residual? For the two students with the most negative residuals, what else seems to be unusual about them? Explain briefly.

\bS

A very negative residual means that the observed value of the response variable is a lot *less* than the predicted value. Here, that means the student's actual GPA is a lot less than their predicted GPA, based on their test scores. If you did the previous part using `augment`, you have the `.fitted` values right there and you can see this directly: the first student was predicted to get a GPA of 2.65 but only managed 1.54, and the second was predicted to get 2.15 but only achieved 1.22.

(1.54 is not actually the second-lowest GPA; there are, from your first graph, four `gpa` values below 1.5 and one just above, which must be that 1.54.)

What I think is also unusual about these two students is that they have very *unbalanced* test scores: a high one and a very low one (actually the low one is the lowest of all the students\endnote{In this dataset, I mean; the percentiles were calculated on a larger group of students.} in each case). 

You might imagine that the student who scored low on `verbal` and pretty high on `math` would study math at university, and maybe found out that university math was a good bit harder than high school math. Or you might imagine that being successful at university requires at least some verbal and mathematical ability both.

This is long enough for you, but I have an Extra that I want to explore:

You might imagine that a student with a high Verbal score might come to university to study something related to language: English, say, or Social Sciences, whereas someone with a high Math score would tend to study something with a lot of math in it: Math itself or Computer Science or Physics, for example. From that point of view, what might determine a student's success is their *higher* score on their two tests,\endnote{Which goes against what I said above about those two outlying students, but let's follow this and see where it goes.} since they will (or might) spend most of their time in courses related to their stronger ability.

The first thing is to make a column with that maximum score in it. This is what you are probably thinking of:

```{r}
students %>% mutate(hi = max(verbal, math))
```

Oh. Every student got a `hi` of 100, even though only one student got a high of 100 in actual fact.

So what happened? This goes back to how `max` actually works: it is what R calls "vectorized", meaning that it looks at the *whole* column of `verbal` values, and the *whole* column of `math` values, and takes the largest of all of the numbers in both columns. Sometimes this is what you want, but in this case it is not.

This has been an R "gotcha" for a long time, and the approved way around it is to use `pmax` instead, thus:

```{r}
students %>% mutate(hi = pmax(verbal, math))
```

If you check down the `hi` column, this is the right thing: the larger of the `verbal` and `math` values *in each row*. 

The logic to this is that `max` of two columns is actually a *number*:

```{r}
with(students, max(verbal, math))
```

This is 100 because the largest of all the numbers in both columns is 100 (one student scored 100 on `verbal`). The `mutate` hid this rather; it needs an answer that is as long as the number of rows in the dataframe, so it repeated (the R term is "recycled") the value 100 enough times to fill the column. This meant that you got an answer without any messages; however, it was the *wrong* answer!

`pmax`, on the other hand, gives you a vector answer as long as the input vectors (40, because 40 rows, here):

```{r}
with(students, pmax(verbal, math))
```

and that, which is the  right answer as well as the right length, will be used as is. `pmax`, if you are wondering, stands for "parallel max"; it computes the maxima in parallel, that is, by rows.

If you looked at the Extra on the end of the Comp Sci 101 question, you might be wondering whether the `rowwise` idea from there will work here; after all, the idea is to look along each row and find the maximum of `verbal` and `math`, so this ought to work:

```{r}
students %>% 
  rowwise() %>% 
  mutate(hi = max(verbal, math)) -> students
students
```

and indeed it does. I think, once you have seen `rowwise`, that this is the most natural way to go, but it requires a decent understanding of how `max` works. What is happening here is that `rowwise` puts each row in a "box" of its own, so that `max(verbal, math)` now means to consider *only* the `verbal` and `math` in the row we're looking at. That is the same logic as this:

```{r}
students %>% 
  mutate(hi2 = map2_dbl(verbal, math, ~max(.x, .y)))
```
where, in English, that reads "for each of the values in `verbal` and `math` (in parallel), work out the max of the two  of them, and store it in `hi2`".

All right, does `hi` have any predictive value for `gpa`? A graph first:

```{r}
ggplot(students, aes(x = hi, y = gpa)) + geom_point()
```

Only kinda, and we still have those two outliers bottom right, with `hi` near 80 and at 100, that don't fit the weakish trend. These are the same two outliers as before, and the discussion above would make us suspect that they would fit badly in this model also.

We can try a regression:

```{r}
gpa.2 <- lm(gpa ~ hi, data = students)
summary(gpa.2)
```

This is a significant upward trend, as before, but the R-squared is not very big. How big was R-squared before?

```{r}
glance(gpa.1)
```

Much bigger. Now, we cannot compare these two models via a test, since neither one of them is contained in the other,\endnote{The statistical word is ``nested'', but I didn't want to confuse you with that and the tidyverse ``nest'', which is a different idea.} but even though the model with only the maximum is simpler, its R-squared is *much* smaller, and so we really seem to need the extra complexity of the larger model: it is "worth having", since the model fits much better. What that actually *means* is that you can predict a student's first-year GPA much more accurately if you know both their verbal and mathematical scores, as compared to just knowing the bigger one.

Having said that, the R-squared of the larger, better model is still only 68%, so that about a third of the variability in GPA is still *not* explained by this model. According to the regression, the unexplained variability is "randomness", but no doubt you can come up with factors that would explain some of it. In your first year, I'm sure you met people who did better or worse than their incoming grades would suggest. Or maybe you were such a person yourself. Either way, I'm sure you wondered about why that was.

\eS

\eP




