##  How not to get heart disease


 What is associated with heart disease? In a study, a large
number of variables were measured, as follows:



* `age` (years)

* `sex` male or female

* `pain.type` Chest pain type (4 values: typical angina,
atypical angina, non-anginal pain, asymptomatic)

* `resting.bp` Resting blood pressure, on admission to hospital

* `serum.chol` Serum cholesterol

* `high.blood.sugar`: greater than 120, yes or no

* `electro` resting electrocardiographic results (normal,
having ST-T, hypertrophy)

* `max.hr` Maximum heart rate

* `angina` Exercise induced angina (yes or no)

* `oldpeak` ST depression induced by exercise relative to
rest. See [link](http://lifeinthefastlane.com/ecg-library/st-segment/).

* `slope` Slope of peak exercise ST segment. Sloping up,
flat or sloping down

* `colored` number of major vessels (0--3) coloured by fluoroscopy

* `thal` normal, fixed defect, reversible defect

* `heart.disease` yes, no (response)


I don't know what most of those are, but we will not let that stand in
our way. Our aim is to find out what variables are associated with
heart disease, and what values of those variables give high
probabilities of heart disease being present. The data are in
[link](https://raw.githubusercontent.com/nxskok/datafiles/master/heartf.csv).



(a) Read in the data. Display the first few lines and convince
yourself that those values are reasonable.


Solution

```{r heart-1, child="packages.Rmd", message = FALSE}
```


A `.csv` file, so:
```{r heart-2 }
my_url <- "https://raw.githubusercontent.com/nxskok/datafiles/master/heartf.csv"
heart <- read_csv(my_url)
heart
```

     

You should check that the variables that should be numbers actually
are, that the variables that should be categorical have (as far as is
shown) the right values as per my description above, and you should
make some comment in that direction.

My variables appear to be correct, apart possibly for that variable
`X1` which is actually just the row number.
    
$\blacksquare$

(b) In a logistic regression, what probability will be
predicted here? Explain briefly but convincingly. (Is each line of
the data file one observation or a summary of several?)


Solution


Each line of the data file is a single observation, not
frequencies of yes and no (like the premature babies
question, later, is). The response variable is a factor, so the first level
is the baseline and the *second* level is the one
predicted. R puts factor levels alphabetically, so `no` is
first and `yes` is second. That is, a logistic regression
will predict the probability that a person *does* have heart disease.
I want to see that logic (which is why I said "convincingly"):
one observation per line, and therefore that the second level of
the factor is predicted, which is `yes`. 
    
$\blacksquare$

(c) <a name="part:heart-first">*</a> Fit a logistic regression predicting heart disease from
everything else (if you have a column called `X` or
`X1`, ignore that), and display the results.


Solution


A lot of typing, since there are so many variables. Don't forget
that the response variable *must* be a factor:
```{r heart-3 }
heart.1 <- glm(factor(heart.disease) ~ age + sex + pain.type + resting.bp + serum.chol +
  high.blood.sugar + electro + max.hr + angina + oldpeak + slope + colored + thal,
family = "binomial", data = heart
)
```

 

You can split this over several lines (and probably should), but make
sure to end each line in such a way that there is unambiguously more
to come, for example with a plus or a comma (though probably the fact
that you have an unclosed bracket will be enough). 

The output is rather lengthy:

```{r heart-4 }
summary(heart.1)
```

 

I didn't ask you for further comment, but note that quite a lot of
these variables are factors, so you get slopes for things like
`pain.typeatypical`. When you have a factor in a model, there
is a slope for each level except for the first, which is a baseline
(and its slope is taken to be zero). That would be
`asymptomatic` for `pain.type`. The $t$-tests for the
other levels of `pain.type` say whether that level of pain
type differs significantly (in terms of probability of heart disease)
from the baseline level. Here, pain type `atypical` is not
significantly different from the baseline, but the other two pain
types, `nonanginal` and `typical`, *are*
significantly different. If you think about this from an ANOVA-like
point of view, the question about `pain.type`'s significance is
really "is there at least one of the pain types that is different from the others", and if we're thinking about whether we should keep
`pain.type` in the logistic regression, this is the kind of
question we should be thinking about. 
    
$\blacksquare$

(d) Quite a lot of our explanatory variables are factors. To
assess whether the factor as a whole should stay or can be removed,
looking at the slopes won't help us very much (since they tell us
whether the other levels of the factor differ from the baseline,
which may not be a sensible comparison to make). To assess which
variables are candidates to be removed, factors included (properly),
we can use `drop1`. Feed `drop1` a fitted model and
the words `test="Chisq"` (take care of the capitalization!)
and you'll get a list of P-values. Which variable is the one that
you would remove first? Explain briefly.


Solution


Following the instructions:

```{r heart-5 }
drop1(heart.1, test = "Chisq")
```

 

The highest P-value, 0.5525, goes with `high.blood.sugar`, so
this one comes out first. (The P-value for `age` is almost as
high, 0.5427, so you might guess that this will be next.)

You might be curious about how these compare with the P-values on
`summary`. These two P-values are almost the same as the ones
on `summary`, because they are a two-level factor and a numeric
variable respectively, and so the tests are equivalent in the two
cases. (The P-values are not identical because the tests on
`summary` and `drop1` are the kind of thing that would
be identical on a regular regression but are only "asymptotically the same" 
in logistic regression, so you'd expect them to be close
without being the same, as here. "Asymptotically the same" means
that if you had an infinitely large sample size, they'd be identical,
but our sample size of 200-odd individuals is not infinitely large!
Anyway, the largest P-value on the `summary` is 0.9965, which
goes with `electroSTT`. `electro`, though, is a factor
with three levels; this P-value says that `STT` is almost
identical (in its effects on heart disease) with the baseline
`hypertrophy`. But there is a third level, `normal`,
which is a bit different from `hypertrophy`. So the factor
`electro` overall has some effect on heart disease, which is
reflected in the `drop1` P-value of 0.12: this might go later,
but it has to stay for now because at least one of its levels is
different from the others in its effect on heart disease. (In backward
elimination, multi-level factors are removed in their entirety if
*none* of their levels have a different effect from any of the
others.) 

The power just went out here, so I am using my laptop on battery on
its own screen, rather than on the big screen I have in my office,
which is much better.

$\blacksquare$

(e) I'm not going to make you do the whole backward elimination
(I'm going to have you use `step` for that later), but do one
step: that is, fit a model removing the variable you think should be
removed, using `update`, and then run `drop1` again to
see which variable will be removed next.


Solution


`update` is the obvious choice here, since we're making a
small change to a *very* big model:
```{r heart-6 }
heart.2 <- update(heart.1, . ~ . - high.blood.sugar)
drop1(heart.2, test = "Chisq")
```

   

The power is back. 

The next variable to go is indeed `age`, with a P-value that
has hardly changed: it is now 0.5218.
    
$\blacksquare$

(f) Use `step` to do a backward elimination to find which
variables have an effect on heart disease. Display your final model
(which you can do by saving the output from `step` in a
variable, and asking for the summary of that. In `step`,
you'll need to specify a starting model (the one from part
(<a href="#part:heart-first">here</a>)), the direction of elimination, and the test
to display the P-value for (the same one as you used in
`drop1`). (Note: the actual decision to keep or drop explanatory variables is based on AIC rather than the P-value, with the result that `step` will sometimes keep variables you would have dropped, with P-values around 0.10.)


Solution


The hints ought to lead you to this:
```{r heart-7, size="tiny"}
heart.3 <- step(heart.1, direction = "backward", test = "Chisq")
```

       

The output is very long.
In terms of AIC, which is what `step` uses, `age`
hangs on for a bit, but eventually gets eliminated. 

There are a lot of variables left.
      
$\blacksquare$

(g) Display the summary of the model that came out of `step`.


Solution


This:

```{r heart-8 }
summary(heart.3)
```

 

Not all of the P-values in the `step` output wound up being
less than 0.05, but they are all at least reasonably small. As
discussed above, some of the P-values in the `summary` are
definitely *not* small, but they go with factors where there are
significant effects *somewhere*. For example, `thalnormal`
is not significant (that is, `normal` is not significantly
different from the baseline `fixed`), but the other level
`reversible` *is* different from `fixed`. You might
be wondering about `slope`: on the `summary` there is
nothing close to significance, but on the `step` output,
`slope` has at least a reasonably small P-value of 0.088. This
is because the significant difference does not involve the baseline:
it's actually between `flat` with a positive slope and
`upsloping` with a negative one. 
  
$\blacksquare$

(h) We are going to make a large number of predictions. Create and save
a data frame that contains predictions for all combinations of representative
values for all the variables in the model that came out of
`step`. By "representative" I mean all the values for a
categorical variable, and the five-number summary for a numeric
variable. (Note that you will get a *lot* of predictions.)


Solution

This is exactly what `predictions` with `variables` does, so that you don't have to do anything to get hold of the representative variables yourself. 

The hard work is in listing all the variables. The easiest way to make sure you have them all is to look at the `summary` of your best model (mine was called `heart.3`) first, and copy them from the `Call` at the top. This is easier than looking at the table of Coefficients (or `tidy` output) because for categorical variables like `pain.type` you will have to distinguish the name of the variable from its levels. For example, the table of Coefficients has `pain.typeatypical` and `pain.typenonanginal`. Is it obvious to you where the variable name ends and its level begins?^[If it is for you, go right ahead, but for me it wasn't.]

```{r}
p <- predictions(heart.3, variables = c("sex", "pain.type", "resting.bp", "serum.chol", "max.hr", "oldpeak", "slope", "colored", "thal"))
p
```

There are a mere 108,000 rows here (and a fair few columns also). That is fine --- as long as you don't display them all for a grader to have to page through!

$\blacksquare$


(i) Find the largest predicted probability (which is the
predicted probability of heart disease) and display all the
variables that it was a prediction for. 


Solution

The (at current writing) approved way to do this is to use `slice_max`. This finds the rows with maximum value(s) on a variable, which is exactly what we want. It goes like this:

```{r}
p %>% slice_max(predicted, n = 1)
```

The inputs to `slice_max` are the column whose maximum value you want, and the number of rows you want (so `n = 3` would display the *three* rows with the highest predicted probabilities).

Variations: 

- by using `prop` instead of `n`, you can display the `proportion` of rows with the highest values on your variable, such as the 10% of rows with the highest predicted probabilities with `prop = 0.10`
- there is also `slice_min` that displays the rows with the *lowest* values on a variable, or the input proportion of rows with the lowest values
- there are alse `slice_head` and `slice_tail` that display the first and last (respectively) rows in a dataframe. The default display of a dataframe in an R Notebook is thus 

```{r}
p %>% slice_head(n = 10)
```

except that the default display also tells you how many rows there are altogether.

- you may have run into `slice_sample`, which displays a randomly-chosen number or proportion of rows from a dataframe. This is useful after you read in a dataframe from a file, if you want to get a sense of what kind of values you have in your dataframe (for example, if they are ordered by something and looking at the first ten rows won't tell you the whole story, such as having males listed first and you want to check that there are some females as well):

```{r}
p %>% slice_sample(n = 10)
```


If you didn't think of `slice_max`, there are lots of other ways. Find one. Here are some examples:


```{r heart-27 }
p %>% filter(predicted == max(predicted))
```

        
or if you didn't think of that, you can find the maximum first, and
then display the rows with predictions close to it:

```{r heart-28 }
p %>% summarize(m = max(predicted))
p %>% filter(predicted > 0.999998)
```

 

or even find *which* row has the maximum, and then display that row:

```{r heart-29 }
p %>% summarize(row = which.max(predicted))
p %>% slice(13049)
```

 

or sort the rows by `predicted`, descending, and display the top few:

```{r heart-30 }
p %>% arrange(desc(predicted)) %>% slice(1:8)
```

 
        
$\blacksquare$

(j) Compare the `summary` of the final model from
`step` with your highest predicted heart disease
probability and the values of the other variables that make it
up. Are they consistent?


Solution


Since we were predicting the probability of heart disease, a
more positive slope in the model from `step` will be
associated with a higher probability of heart disease. So,
there, we are looking for a couple of things: if the variable
is a factor, we're looking for the level with the most
positive slope (bearing in mind that this might be the
baseline), and for a numeric variable, if the slope is
positive, a *high* value is associated with heart
disease, and if negative, a low value.
Bearing that in mind, we go back to my
`summary(heart.3)`:

```{r}
summary(heart.3)
```


* `sex`: being male has the higher risk, by a lot

* `pain`: all the slopes shown are negative, so the
highest risk goes with the baseline one
`asymptomatic`.

* `resting.bp`: positive slope, so higher risk with
higher value.

* `serum.chol`: same.

* `max.hr`: negative slope, so greatest risk with
*smaller* value.

* `oldpeak`: positive slope, greater risk with
higher value again.

* `slope`: `flat` has greatest risk.

* `colored`: positive slope, so beware of higher
value.

* `thal`: `reversible` has greatest risk.

Then we can do the same thing for the prediction. 

and the highest prediction:

```{r}
p %>% slice_max(predicted, n = 1)
```

For the
numerical variables, we may need to check back to the previous
part to see whether the value shown was high or low. Once you
have done that, you can see that the variable values for the
highest predicted probability do indeed match the ones we
thought should be the highest risk.

Extra: the interesting thing about this is that even after adjusting for
all of the other variables, there is a greater risk of heart
disease if you are male (and the model shows that the risk is
*much* greater). That is to say, it's being male that
makes the difference, not the fact that any of the other
variables are different for males.

It's rather difficult to scan 108,000 predictions to see the effect of being male, but we can do this:

```{r}
predictions(heart.3, variables = "sex")
```


What this does is to choose a single representative value for all the other variables: the mean for a quantitative variable like resting blood pressure, and the most common category for a categorical variable like `pain.type`. If you scan all the way along the two rows, you find that the values for all the variables are the same except for `sex` at the end. The predicted probabilities of heart disease are very different for males and females (much higher for males), especially given that all else really is equal.

To see this graphically, we can use `plot_cap`, and we can include another variable such as resting blood pressure. It's better to list the quantitative one first:

```{r}
plot_cap(heart.3, condition = c("resting.bp", "sex"))
```

As the resting blood pressure increases, the probability of heart disease increases, but the blue line for males is well above the red one for females all the way across. For example, for a 50% chance of heart disease, this will happen for males with a resting blood pressure of about 120, but for females not until the resting blood pressure reaches 190!

Perhaps, therefore, the easiest way to avoid a heart attack is to not be male!
        
$\blacksquare$





