<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 26 Discriminant analysis | Problems and Solutions in Applied Statistics" />
<meta property="og:type" content="book" />
<meta property="og:url" content="http://ritsokiguess.site/pasias" />

<meta property="og:description" content="A set of problems and solutions, in R, on various parts of applied statistics" />
<meta name="github-repo" content="nxskok/pasias" />

<meta name="author" content="Ken Butler" />

<meta name="date" content="2019-01-07" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<meta name="description" content="A set of problems and solutions, in R, on various parts of applied statistics">

<title>Chapter 26 Discriminant analysis | Problems and Solutions in Applied Statistics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/jquery-1.12.4/jquery.min.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/proj4-2.6.2/proj4.min.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.4.1/leaflet.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="index.html#packages-used-somewhere-in-this-book"><span class="toc-section-number">1.1</span> Packages used somewhere in this book</a></li>
</ul></li>
<li><a href="logistic-regression-redux.html#logistic-regression-redux"><span class="toc-section-number">2</span> Logistic regression redux</a>
<ul>
<li><a href="logistic-regression-redux.html#the-brain-of-a-cat"><span class="toc-section-number">2.1</span> The brain of a cat</a></li>
</ul></li>
<li><a href="getting-used-to-r-and-r-studio.html#getting-used-to-r-and-r-studio"><span class="toc-section-number">3</span> Getting used to R and R Studio</a>
<ul>
<li><a href="getting-used-to-r-and-r-studio.html#getting-an-r-studio-cloud-account"><span class="toc-section-number">3.1</span> Getting an R Studio Cloud account</a></li>
<li><a href="getting-used-to-r-and-r-studio.html#getting-started"><span class="toc-section-number">3.2</span> Getting started</a></li>
<li><a href="getting-used-to-r-and-r-studio.html#reading-data-from-a-file"><span class="toc-section-number">3.3</span> Reading data from a file</a></li>
<li><a href="getting-used-to-r-and-r-studio.html#reading-files-different-ways"><span class="toc-section-number">3.4</span> Reading files different ways</a></li>
</ul></li>
<li><a href="reading-in-data.html#reading-in-data"><span class="toc-section-number">4</span> Reading in data</a>
<ul>
<li><a href="reading-in-data.html#orange-juice"><span class="toc-section-number">4.1</span> Orange juice</a></li>
<li><a href="reading-in-data.html#making-soap"><span class="toc-section-number">4.2</span> Making soap</a></li>
<li><a href="reading-in-data.html#handling-shipments"><span class="toc-section-number">4.3</span> Handling shipments</a></li>
</ul></li>
<li><a href="drawing-graphs.html#drawing-graphs"><span class="toc-section-number">5</span> Drawing graphs</a>
<ul>
<li><a href="drawing-graphs.html#orange-juice-1"><span class="toc-section-number">5.1</span> Orange juice</a></li>
<li><a href="drawing-graphs.html#making-soap-1"><span class="toc-section-number">5.2</span> Making soap</a></li>
<li><a href="drawing-graphs.html#handling-shipments-1"><span class="toc-section-number">5.3</span> Handling shipments</a></li>
</ul></li>
<li><a href="data-exploration.html#data-exploration"><span class="toc-section-number">6</span> Data exploration</a>
<ul>
<li><a href="data-exploration.html#north-carolina-births"><span class="toc-section-number">6.1</span> North Carolina births</a></li>
<li><a href="data-exploration.html#more-about-the-nc-births"><span class="toc-section-number">6.2</span> More about the NC births</a></li>
<li><a href="data-exploration.html#nenana-alaska"><span class="toc-section-number">6.3</span> Nenana, Alaska</a></li>
<li><a href="data-exploration.html#computerized-accounting"><span class="toc-section-number">6.4</span> Computerized accounting</a></li>
<li><a href="data-exploration.html#test-scores-in-two-classes"><span class="toc-section-number">6.5</span> Test scores in two classes</a></li>
<li><a href="data-exploration.html#working-with-dataframes"><span class="toc-section-number">6.6</span> Working with dataframes</a></li>
<li><a href="data-exploration.html#tidying-the-jays-data"><span class="toc-section-number">6.7</span> Tidying the Jays data</a></li>
<li><a href="data-exploration.html#cars"><span class="toc-section-number">6.8</span> Cars</a></li>
</ul></li>
<li><a href="one-sample-inference.html#one-sample-inference"><span class="toc-section-number">7</span> One-sample inference</a>
<ul>
<li><a href="one-sample-inference.html#hunter-gatherers-in-australia"><span class="toc-section-number">7.1</span> Hunter-gatherers in Australia</a></li>
<li><a href="one-sample-inference.html#buses-to-boulder"><span class="toc-section-number">7.2</span> Buses to Boulder</a></li>
<li><a href="one-sample-inference.html#length-of-gestation-in-north-carolina"><span class="toc-section-number">7.3</span> Length of gestation in North Carolina</a></li>
<li><a href="one-sample-inference.html#inferring-ice-break-up-in-nenana"><span class="toc-section-number">7.4</span> Inferring ice break-up in Nenana</a></li>
</ul></li>
<li><a href="two-sample-inference.html#two-sample-inference"><span class="toc-section-number">8</span> Two-sample inference</a>
<ul>
<li><a href="two-sample-inference.html#children-and-electronic-devices"><span class="toc-section-number">8.1</span> Children and electronic devices</a></li>
<li><a href="two-sample-inference.html#parking-close-to-the-curb"><span class="toc-section-number">8.2</span> Parking close to the curb</a></li>
<li><a href="two-sample-inference.html#bell-peppers-and-too-much-water"><span class="toc-section-number">8.3</span> Bell peppers and too much water</a></li>
<li><a href="two-sample-inference.html#exercise-and-anxiety-and-bullying-mice"><span class="toc-section-number">8.4</span> Exercise and anxiety and bullying mice</a></li>
<li><a href="two-sample-inference.html#diet-and-growth-in-boys"><span class="toc-section-number">8.5</span> Diet and growth in boys</a></li>
</ul></li>
<li><a href="power-and-sample-size.html#power-and-sample-size"><span class="toc-section-number">9</span> Power and sample size</a>
<ul>
<li><a href="power-and-sample-size.html#simulating-power"><span class="toc-section-number">9.1</span> Simulating power</a></li>
<li><a href="power-and-sample-size.html#calculating-power-and-sample-size-for-estimating-mean"><span class="toc-section-number">9.2</span> Calculating power and sample size for estimating mean</a></li>
<li><a href="power-and-sample-size.html#simulating-power-for-proportions"><span class="toc-section-number">9.3</span> Simulating power for proportions</a></li>
</ul></li>
<li><a href="the-sign-test-and-moods-median-test.html#the-sign-test-and-moods-median-test"><span class="toc-section-number">10</span> The sign test and Mood’s median test</a>
<ul>
<li><a href="the-sign-test-and-moods-median-test.html#running-a-maze"><span class="toc-section-number">10.1</span> Running a maze</a></li>
<li><a href="the-sign-test-and-moods-median-test.html#chocolate-chips"><span class="toc-section-number">10.2</span> Chocolate chips</a></li>
<li><a href="the-sign-test-and-moods-median-test.html#the-power-of-the-sign-test"><span class="toc-section-number">10.3</span> The power of the sign test</a></li>
<li><a href="the-sign-test-and-moods-median-test.html#sugar-in-breakfast-cereals"><span class="toc-section-number">10.4</span> Sugar in breakfast cereals</a></li>
<li><a href="the-sign-test-and-moods-median-test.html#fear-of-math"><span class="toc-section-number">10.5</span> Fear of math</a></li>
<li><a href="the-sign-test-and-moods-median-test.html#medical-instructions"><span class="toc-section-number">10.6</span> Medical instructions</a></li>
</ul></li>
<li><a href="matched-pairs-t-and-sign-test.html#matched-pairs-t-and-sign-test"><span class="toc-section-number">11</span> Matched pairs t and sign test</a>
<ul>
<li><a href="matched-pairs-t-and-sign-test.html#measuring-body-fat"><span class="toc-section-number">11.1</span> Measuring body fat</a></li>
<li><a href="matched-pairs-t-and-sign-test.html#throwing-baseballs-and-softballs"><span class="toc-section-number">11.2</span> Throwing baseballs and softballs</a></li>
<li><a href="matched-pairs-t-and-sign-test.html#throwing-baseballs-and-softballs-again"><span class="toc-section-number">11.3</span> Throwing baseballs and softballs, again</a></li>
<li><a href="matched-pairs-t-and-sign-test.html#changes-in-salary"><span class="toc-section-number">11.4</span> Changes in salary</a></li>
<li><a href="matched-pairs-t-and-sign-test.html#body-fat-revisited"><span class="toc-section-number">11.5</span> Body fat revisited</a></li>
</ul></li>
<li><a href="normal-quantile-plots.html#normal-quantile-plots"><span class="toc-section-number">12</span> Normal quantile plots</a>
<ul>
<li><a href="normal-quantile-plots.html#lengths-of-heliconia-flowers"><span class="toc-section-number">12.1</span> Lengths of heliconia flowers</a></li>
<li><a href="normal-quantile-plots.html#ferritin-and-normality"><span class="toc-section-number">12.2</span> Ferritin and normality</a></li>
</ul></li>
<li><a href="analysis-of-variance.html#analysis-of-variance"><span class="toc-section-number">13</span> Analysis of variance</a>
<ul>
<li><a href="analysis-of-variance.html#movie-ratings-and-lengths"><span class="toc-section-number">13.1</span> Movie ratings and lengths</a></li>
<li><a href="analysis-of-variance.html#deer-and-how-much-they-eat"><span class="toc-section-number">13.2</span> Deer and how much they eat</a></li>
<li><a href="analysis-of-variance.html#movie-ratings-again"><span class="toc-section-number">13.3</span> Movie ratings again</a></li>
<li><a href="analysis-of-variance.html#atomic-weight-of-carbon"><span class="toc-section-number">13.4</span> Atomic weight of carbon</a></li>
<li><a href="analysis-of-variance.html#can-caffeine-improve-your-performance-on-a-test"><span class="toc-section-number">13.5</span> Can caffeine improve your performance on a test?</a></li>
</ul></li>
<li><a href="tidying-data.html#tidying-data"><span class="toc-section-number">14</span> Tidying data</a>
<ul>
<li><a href="tidying-data.html#baseball-and-softball-spaghetti"><span class="toc-section-number">14.1</span> Baseball and softball spaghetti</a></li>
<li><a href="tidying-data.html#ethanol-and-sleep-time-in-rats"><span class="toc-section-number">14.2</span> Ethanol and sleep time in rats</a></li>
<li><a href="tidying-data.html#growth-of-tomatoes"><span class="toc-section-number">14.3</span> Growth of tomatoes</a></li>
<li><a href="tidying-data.html#pain-relief-in-migraine-headaches-again"><span class="toc-section-number">14.4</span> Pain relief in migraine headaches (again)</a></li>
<li><a href="tidying-data.html#location-species-and-disease-in-plants"><span class="toc-section-number">14.5</span> Location, species and disease in plants</a></li>
<li><a href="tidying-data.html#mating-songs-in-crickets"><span class="toc-section-number">14.6</span> Mating songs in crickets</a></li>
<li><a href="tidying-data.html#number-1-songs"><span class="toc-section-number">14.7</span> Number 1 songs</a></li>
<li><a href="tidying-data.html#bikes-on-college"><span class="toc-section-number">14.8</span> Bikes on College</a></li>
<li><a href="tidying-data.html#feeling-the-heat"><span class="toc-section-number">14.9</span> Feeling the heat</a></li>
</ul></li>
<li><a href="regression.html#regression"><span class="toc-section-number">15</span> Regression</a>
<ul>
<li><a href="regression.html#rainfall-in-california"><span class="toc-section-number">15.1</span> Rainfall in California</a></li>
<li><a href="regression.html#carbon-monoxide-in-cigarettes"><span class="toc-section-number">15.2</span> Carbon monoxide in cigarettes</a></li>
<li><a href="regression.html#maximal-oxygen-uptake-in-young-boys"><span class="toc-section-number">15.3</span> Maximal oxygen uptake in young boys</a></li>
<li><a href="regression.html#facebook-friends-and-grey-matter"><span class="toc-section-number">15.4</span> Facebook friends and grey matter</a></li>
<li><a href="regression.html#endogenous-nitrogen-excretion-in-carp"><span class="toc-section-number">15.5</span> Endogenous nitrogen excretion in carp</a></li>
<li><a href="regression.html#sparrowhawks"><span class="toc-section-number">15.6</span> Sparrowhawks</a></li>
<li><a href="regression.html#salaries-of-social-workers"><span class="toc-section-number">15.7</span> Salaries of social workers</a></li>
<li><a href="regression.html#predicting-volume-of-wood-in-pine-trees"><span class="toc-section-number">15.8</span> Predicting volume of wood in pine trees</a></li>
<li><a href="regression.html#tortoise-shells-and-eggs"><span class="toc-section-number">15.9</span> Tortoise shells and eggs</a></li>
<li><a href="regression.html#crickets-revisited"><span class="toc-section-number">15.10</span> Crickets revisited</a></li>
<li><a href="regression.html#roller-coasters"><span class="toc-section-number">15.11</span> Roller coasters</a></li>
<li><a href="regression.html#running-and-blood-sugar"><span class="toc-section-number">15.12</span> Running and blood sugar</a></li>
<li><a href="regression.html#calories-and-fat-in-pizza"><span class="toc-section-number">15.13</span> Calories and fat in pizza</a></li>
<li><a href="regression.html#where-should-the-fire-stations-be"><span class="toc-section-number">15.14</span> Where should the fire stations be?</a></li>
<li><a href="regression.html#being-satisfied-with-hospital"><span class="toc-section-number">15.15</span> Being satisfied with hospital</a></li>
<li><a href="regression.html#handling-shipments-of-chemicals"><span class="toc-section-number">15.16</span> Handling shipments of chemicals</a></li>
<li><a href="regression.html#salaries-of-mathematicians"><span class="toc-section-number">15.17</span> Salaries of mathematicians</a></li>
<li><a href="regression.html#predicting-gpa-of-computer-science-students"><span class="toc-section-number">15.18</span> Predicting GPA of computer science students</a></li>
</ul></li>
<li><a href="dates-and-times.html#dates-and-times"><span class="toc-section-number">16</span> Dates and times</a>
<ul>
<li><a href="dates-and-times.html#dealing-with-dates-in-the-worcester-heart-attack-study"><span class="toc-section-number">16.1</span> Dealing with dates in the Worcester Heart Attack study</a></li>
<li><a href="dates-and-times.html#growth-of-mizuna-lettuce-seeds"><span class="toc-section-number">16.2</span> Growth of Mizuna lettuce seeds</a></li>
<li><a href="dates-and-times.html#types-of-childbirth"><span class="toc-section-number">16.3</span> Types of childbirth</a></li>
<li><a href="dates-and-times.html#wolves-and-caribou"><span class="toc-section-number">16.4</span> Wolves and caribou</a></li>
</ul></li>
<li><a href="functions.html#functions"><span class="toc-section-number">17</span> Functions</a>
<ul>
<li><a href="functions.html#making-some-r-functions"><span class="toc-section-number">17.1</span> Making some R functions</a></li>
<li><a href="functions.html#the-collatz-sequence"><span class="toc-section-number">17.2</span> The Collatz sequence</a></li>
</ul></li>
<li><a href="the-bootstrap.html#the-bootstrap"><span class="toc-section-number">18</span> The Bootstrap</a>
<ul>
<li><a href="the-bootstrap.html#air-conditioning-failures"><span class="toc-section-number">18.1</span> Air conditioning failures</a></li>
<li><a href="the-bootstrap.html#air-conditioning-failures-bootstrapping-the-median"><span class="toc-section-number">18.2</span> Air conditioning failures: bootstrapping the median</a></li>
</ul></li>
<li><a href="bayesian-statistics-with-stan.html#bayesian-statistics-with-stan"><span class="toc-section-number">19</span> Bayesian Statistics with Stan</a>
<ul>
<li><a href="bayesian-statistics-with-stan.html#estimating-proportion-in-favour-from-a-survey"><span class="toc-section-number">19.1</span> Estimating proportion in favour from a survey</a></li>
<li><a href="bayesian-statistics-with-stan.html#bayesian-regression"><span class="toc-section-number">19.2</span> Bayesian regression</a></li>
</ul></li>
<li><a href="logistic-regression.html#logistic-regression"><span class="toc-section-number">20</span> Logistic regression</a>
<ul>
<li><a href="logistic-regression.html#finding-wolf-spiders-on-the-beach"><span class="toc-section-number">20.1</span> Finding wolf spiders on the beach</a></li>
<li><a href="logistic-regression.html#killing-aphids"><span class="toc-section-number">20.2</span> Killing aphids</a></li>
<li><a href="logistic-regression.html#the-effects-of-substance-a"><span class="toc-section-number">20.3</span> The effects of Substance A</a></li>
<li><a href="logistic-regression.html#what-makes-an-animal-get-infected"><span class="toc-section-number">20.4</span> What makes an animal get infected?</a></li>
<li><a href="logistic-regression.html#the-brain-of-a-cat-1"><span class="toc-section-number">20.5</span> The brain of a cat</a></li>
<li><a href="logistic-regression.html#how-not-to-get-heart-disease"><span class="toc-section-number">20.6</span> How not to get heart disease</a></li>
<li><a href="logistic-regression.html#successful-breastfeeding"><span class="toc-section-number">20.7</span> Successful breastfeeding</a></li>
<li><a href="logistic-regression.html#making-it-over-the-mountains"><span class="toc-section-number">20.8</span> Making it over the mountains</a></li>
<li><a href="logistic-regression.html#who-needs-the-most-intensive-care"><span class="toc-section-number">20.9</span> Who needs the most intensive care?</a></li>
<li><a href="logistic-regression.html#go-away-and-dont-come-back"><span class="toc-section-number">20.10</span> Go away and don’t come back!</a></li>
</ul></li>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#logistic-regression-with-ordinal-or-nominal-response"><span class="toc-section-number">21</span> Logistic regression with ordinal or nominal response</a>
<ul>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#do-you-like-your-mobile-phone"><span class="toc-section-number">21.1</span> Do you like your mobile phone?</a></li>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#attitudes-towards-abortion"><span class="toc-section-number">21.2</span> Attitudes towards abortion</a></li>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#finding-non-missing-values"><span class="toc-section-number">21.3</span> Finding non-missing values</a></li>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#european-social-survey-and-voting"><span class="toc-section-number">21.4</span> European Social Survey and voting</a></li>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#alligator-food"><span class="toc-section-number">21.5</span> Alligator food</a></li>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#how-do-you-like-your-steak-the-data"><span class="toc-section-number">21.6</span> How do you like your steak – the data</a></li>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#crimes-in-san-francisco-the-data"><span class="toc-section-number">21.7</span> Crimes in San Francisco – the data</a></li>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#how-do-you-like-your-steak"><span class="toc-section-number">21.8</span> How do you like your steak?</a></li>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#crimes-in-san-francisco"><span class="toc-section-number">21.9</span> Crimes in San Francisco</a></li>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#high-school-and-beyond"><span class="toc-section-number">21.10</span> High School and Beyond</a></li>
<li><a href="logistic-regression-with-ordinal-or-nominal-response.html#what-sports-do-these-athletes-play"><span class="toc-section-number">21.11</span> What sports do these athletes play?</a></li>
</ul></li>
<li><a href="survival-analysis.html#survival-analysis"><span class="toc-section-number">22</span> Survival analysis</a>
<ul>
<li><a href="survival-analysis.html#the-worcester-survey"><span class="toc-section-number">22.1</span> The Worcester survey</a></li>
<li><a href="survival-analysis.html#drug-treatment-programs"><span class="toc-section-number">22.2</span> Drug treatment programs</a></li>
<li><a href="survival-analysis.html#multiple-myeloma"><span class="toc-section-number">22.3</span> Multiple myeloma</a></li>
<li><a href="survival-analysis.html#ovarian-cancer"><span class="toc-section-number">22.4</span> Ovarian cancer</a></li>
</ul></li>
<li><a href="analysis-of-variance-revisited.html#analysis-of-variance-revisited"><span class="toc-section-number">23</span> Analysis of variance revisited</a>
<ul>
<li><a href="analysis-of-variance-revisited.html#acid-rain"><span class="toc-section-number">23.1</span> Acid rain</a></li>
<li><a href="analysis-of-variance-revisited.html#treating-hay-fever"><span class="toc-section-number">23.2</span> Treating hay fever</a></li>
<li><a href="analysis-of-variance-revisited.html#focused-comparisons-of-the-effect-of-caffeine"><span class="toc-section-number">23.3</span> Focused comparisons of the effect of caffeine</a></li>
<li><a href="analysis-of-variance-revisited.html#who-studies-the-most-outside-class"><span class="toc-section-number">23.4</span> Who studies the most outside class?</a></li>
<li><a href="analysis-of-variance-revisited.html#mental-context"><span class="toc-section-number">23.5</span> Mental context</a></li>
<li><a href="analysis-of-variance-revisited.html#trying-on-shirts"><span class="toc-section-number">23.6</span> Trying on shirts</a></li>
<li><a href="analysis-of-variance-revisited.html#productivity-and-research-and-development"><span class="toc-section-number">23.7</span> Productivity and research-and-development</a></li>
<li><a href="analysis-of-variance-revisited.html#treating-leprosy"><span class="toc-section-number">23.8</span> Treating leprosy</a></li>
</ul></li>
<li><a href="multivariate-analysis-of-variance.html#multivariate-analysis-of-variance"><span class="toc-section-number">24</span> Multivariate analysis of variance</a>
<ul>
<li><a href="multivariate-analysis-of-variance.html#fabricated-data"><span class="toc-section-number">24.1</span> Fabricated data</a></li>
<li><a href="multivariate-analysis-of-variance.html#do-characteristics-of-urine-depend-on-obesity"><span class="toc-section-number">24.2</span> Do characteristics of urine depend on obesity?</a></li>
<li><a href="multivariate-analysis-of-variance.html#how-do-height-and-weight-depend-on-sport-played-by-elite-athletes"><span class="toc-section-number">24.3</span> How do height and weight depend on sport played by elite athletes?</a></li>
</ul></li>
<li><a href="repeated-measures.html#repeated-measures"><span class="toc-section-number">25</span> Repeated measures</a>
<ul>
<li><a href="repeated-measures.html#effect-of-drug-on-rat-weight"><span class="toc-section-number">25.1</span> Effect of drug on rat weight</a></li>
<li><a href="repeated-measures.html#social-interaction-among-old-people"><span class="toc-section-number">25.2</span> Social interaction among old people</a></li>
<li><a href="repeated-measures.html#childrens-stress-levels-and-airports"><span class="toc-section-number">25.3</span> Children’s stress levels and airports</a></li>
<li><a href="repeated-measures.html#body-fat-as-repeated-measures"><span class="toc-section-number">25.4</span> Body fat as repeated measures</a></li>
<li><a href="repeated-measures.html#investigating-motor-activity-in-rats"><span class="toc-section-number">25.5</span> Investigating motor activity in rats</a></li>
<li><a href="repeated-measures.html#repeated-measures-with-no-background"><span class="toc-section-number">25.6</span> Repeated measures with no background</a></li>
</ul></li>
<li><a href="discriminant-analysis.html#discriminant-analysis"><span class="toc-section-number">26</span> Discriminant analysis</a>
<ul>
<li><a href="discriminant-analysis.html#telling-whether-a-banknote-is-real-or-counterfeit"><span class="toc-section-number">26.1</span> Telling whether a banknote is real or counterfeit</a></li>
<li><a href="discriminant-analysis.html#urine-and-obesity-what-makes-a-difference"><span class="toc-section-number">26.2</span> Urine and obesity: what makes a difference?</a></li>
<li><a href="discriminant-analysis.html#understanding-a-manova"><span class="toc-section-number">26.3</span> Understanding a MANOVA</a></li>
<li><a href="discriminant-analysis.html#what-distinguishes-people-who-do-different-jobs"><span class="toc-section-number">26.4</span> What distinguishes people who do different jobs?</a></li>
<li><a href="discriminant-analysis.html#observing-children-with-adhd"><span class="toc-section-number">26.5</span> Observing children with ADHD</a></li>
<li><a href="discriminant-analysis.html#growing-corn"><span class="toc-section-number">26.6</span> Growing corn</a></li>
<li><a href="discriminant-analysis.html#understanding-athletes-height-weight-sport-and-gender"><span class="toc-section-number">26.7</span> Understanding athletes’ height, weight, sport and gender</a></li>
</ul></li>
<li><a href="cluster-analysis.html#cluster-analysis"><span class="toc-section-number">27</span> Cluster analysis</a>
<ul>
<li><a href="cluster-analysis.html#sites-on-the-sea-bed"><span class="toc-section-number">27.1</span> Sites on the sea bed</a></li>
<li><a href="cluster-analysis.html#dissimilarities-between-fruits"><span class="toc-section-number">27.2</span> Dissimilarities between fruits</a></li>
<li><a href="cluster-analysis.html#similarity-of-species"><span class="toc-section-number">27.3</span> Similarity of species</a></li>
<li><a href="cluster-analysis.html#rating-beer"><span class="toc-section-number">27.4</span> Rating beer</a></li>
<li><a href="cluster-analysis.html#clustering-the-swiss-bills"><span class="toc-section-number">27.5</span> Clustering the Swiss bills</a></li>
<li><a href="cluster-analysis.html#grouping-similar-cars"><span class="toc-section-number">27.6</span> Grouping similar cars</a></li>
<li><a href="cluster-analysis.html#running-jumping-and-throwing"><span class="toc-section-number">27.7</span> Running, jumping, and throwing</a></li>
<li><a href="cluster-analysis.html#bridges-in-pittsburgh"><span class="toc-section-number">27.8</span> Bridges in Pittsburgh</a></li>
<li><a href="cluster-analysis.html#clustering-the-australian-athletes"><span class="toc-section-number">27.9</span> Clustering the Australian athletes</a></li>
</ul></li>
<li><a href="multidimensional-scaling.html#multidimensional-scaling"><span class="toc-section-number">28</span> Multidimensional Scaling</a>
<ul>
<li><a href="multidimensional-scaling.html#making-a-map-of-wisconsin"><span class="toc-section-number">28.1</span> Making a map of Wisconsin</a></li>
<li><a href="multidimensional-scaling.html#things-that-feel-similar-to-each-other"><span class="toc-section-number">28.2</span> Things that feel similar to each other</a></li>
<li><a href="multidimensional-scaling.html#confusing-letters"><span class="toc-section-number">28.3</span> Confusing letters</a></li>
<li><a href="multidimensional-scaling.html#more-beer-please"><span class="toc-section-number">28.4</span> More beer please</a></li>
<li><a href="multidimensional-scaling.html#feeling-similar-again"><span class="toc-section-number">28.5</span> Feeling similar, again</a></li>
</ul></li>
<li><a href="principal-components-and-factor-analysis.html#principal-components-and-factor-analysis"><span class="toc-section-number">29</span> Principal Components and Factor Analysis</a>
<ul>
<li><a href="principal-components-and-factor-analysis.html#the-weather-somewhere"><span class="toc-section-number">29.1</span> The weather, somewhere</a></li>
<li><a href="principal-components-and-factor-analysis.html#air-pollution"><span class="toc-section-number">29.2</span> Air pollution</a></li>
<li><a href="principal-components-and-factor-analysis.html#a-correlation-matrix"><span class="toc-section-number">29.3</span> A correlation matrix</a></li>
<li><a href="principal-components-and-factor-analysis.html#the-interpersonal-circumplex"><span class="toc-section-number">29.4</span> The Interpersonal Circumplex</a></li>
</ul></li>
<li><a href="frequency-table-analysis.html#frequency-table-analysis"><span class="toc-section-number">30</span> Frequency table analysis</a>
<ul>
<li><a href="frequency-table-analysis.html#college-plans"><span class="toc-section-number">30.1</span> College plans</a></li>
<li><a href="frequency-table-analysis.html#predicting-voting"><span class="toc-section-number">30.2</span> Predicting voting</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="discriminant-analysis" class="section level1" number="26">
<h1><span class="header-section-number">Chapter 26</span> Discriminant analysis</h1>
<p>Packages for this chapter:</p>
<div class="sourceCode" id="cb3338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3338-1"><a href="discriminant-analysis.html#cb3338-1"></a><span class="kw">library</span>(ggbiplot)</span>
<span id="cb3338-2"><a href="discriminant-analysis.html#cb3338-2"></a><span class="kw">library</span>(MASS)</span>
<span id="cb3338-3"><a href="discriminant-analysis.html#cb3338-3"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb3338-4"><a href="discriminant-analysis.html#cb3338-4"></a><span class="kw">library</span>(car)</span></code></pre></div>
<p>(Note: <code>ggbiplot</code> loads <code>plyr</code>, which overlaps a lot with <code>dplyr</code>
(<code>filter</code>, <code>select</code> etc.). We want the <code>dplyr</code> stuff elsewhere, so we
load <code>ggbiplot</code> <em>first</em>, and the things in <code>plyr</code> get hidden, as shown
in the Conflicts. This, despite appearances, is what we want.)</p>
<div id="telling-whether-a-banknote-is-real-or-counterfeit" class="section level2" number="26.1">
<h2><span class="header-section-number">26.1</span> Telling whether a banknote is real or counterfeit</h2>
<p><a name="sec:swiss-money">*</a> A Swiss bank collected a number of known counterfeit
(fake)
bills over time, and sampled a number of known genuine bills of the
same denomination.
Is it possible to tell, from measurements taken from a bill, whether
it is genuine or not? We will explore that issue here. The variables
measured were:</p>
<ul>
<li><p>length</p></li>
<li><p>right-hand width</p></li>
<li><p>left-hand width</p></li>
<li><p>top margin</p></li>
<li><p>bottom margin</p></li>
<li><p>diagonal</p></li>
</ul>
<ol style="list-style-type: lower-alpha">
<li>Read in the data from
<a href="http://www.utsc.utoronto.ca/~butler/d29/swiss1.txt">link</a>, and
check that you have 200 rows and 7 columns altogether.</li>
</ol>
<p>Solution</p>
<p>Check the data file first. It’s aligned in columns, thus:</p>
<div class="sourceCode" id="cb3339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3339-1"><a href="discriminant-analysis.html#cb3339-1"></a>my_url &lt;-<span class="st"> &quot;http://www.utsc.utoronto.ca/~butler/d29/swiss1.txt&quot;</span></span>
<span id="cb3339-2"><a href="discriminant-analysis.html#cb3339-2"></a>swiss &lt;-<span class="st"> </span><span class="kw">read_table</span>(my_url)</span></code></pre></div>
<pre><code>## 
## ── Column specification ──────────────────────────────────────────────────────────────────
## cols(
##   length = col_double(),
##   left = col_double(),
##   right = col_double(),
##   bottom = col_double(),
##   top = col_double(),
##   diag = col_double(),
##   status = col_character()
## )</code></pre>
<div class="sourceCode" id="cb3341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3341-1"><a href="discriminant-analysis.html#cb3341-1"></a>swiss</span></code></pre></div>
<pre><code>## # A tibble: 200 x 7
##    length  left right bottom   top  diag status 
##     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  
##  1   215.  131   131.    9     9.7  141  genuine
##  2   215.  130.  130.    8.1   9.5  142. genuine
##  3   215.  130.  130.    8.7   9.6  142. genuine
##  4   215.  130.  130.    7.5  10.4  142  genuine
##  5   215   130.  130.   10.4   7.7  142. genuine
##  6   216.  131.  130.    9    10.1  141. genuine
##  7   216.  130.  130.    7.9   9.6  142. genuine
##  8   214.  130.  129.    7.2  10.7  142. genuine
##  9   215.  129.  130.    8.2  11    142. genuine
## 10   215.  130.  130.    9.2  10    141. genuine
## # … with 190 more rows</code></pre>
<p>Yep, 200 rows and 7 columns.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Run a multivariate analysis of variance. What do you
conclude? Is it worth running a discriminant analysis? (This is
the same procedure as with basic MANOVAs before.)</li>
</ol>
<p>Solution</p>
<p>Small-m <code>manova</code> will do here:</p>
<div class="sourceCode" id="cb3343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3343-1"><a href="discriminant-analysis.html#cb3343-1"></a>response &lt;-<span class="st"> </span><span class="kw">with</span>(swiss, <span class="kw">cbind</span>(length, left, right, bottom, top, diag))</span>
<span id="cb3343-2"><a href="discriminant-analysis.html#cb3343-2"></a>swiss<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">manova</span>(response <span class="op">~</span><span class="st"> </span>status, <span class="dt">data =</span> swiss)</span>
<span id="cb3343-3"><a href="discriminant-analysis.html#cb3343-3"></a><span class="kw">summary</span>(swiss<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>##            Df  Pillai approx F num Df den Df    Pr(&gt;F)    
## status      1 0.92415   391.92      6    193 &lt; 2.2e-16 ***
## Residuals 198                                             
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>You might be wondering whether you had to go to all that trouble to
make the response variable. Would this work?</p>
<div class="sourceCode" id="cb3345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3345-1"><a href="discriminant-analysis.html#cb3345-1"></a>response2 &lt;-<span class="st"> </span>swiss <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(length<span class="op">:</span>diag)</span>
<span id="cb3345-2"><a href="discriminant-analysis.html#cb3345-2"></a>swiss<span class="fl">.1</span>a &lt;-<span class="st"> </span><span class="kw">manova</span>(response2 <span class="op">~</span><span class="st"> </span>status, <span class="dt">data =</span> swiss)</span></code></pre></div>
<pre><code>## Error in model.frame.default(formula = response2 ~ status, data = swiss, : invalid type (list) for variable &#39;response2&#39;</code></pre>
<p>No, because <code>response2</code> needs to be an R <code>matrix</code>, and it isn’t:</p>
<div class="sourceCode" id="cb3347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3347-1"><a href="discriminant-analysis.html#cb3347-1"></a><span class="kw">class</span>(response2)</span></code></pre></div>
<pre><code>## [1] &quot;tbl_df&quot;     &quot;tbl&quot;        &quot;data.frame&quot;</code></pre>
<p>The error message was a bit cryptic (nothing unusual there), but a
data frame (to R) is a special kind of <code>list</code>, so that R didn’t
like <code>response2</code> being a data frame, which it
thought was a list.</p>
<p>This, however, works, since it turns the data frame into a matrix:</p>
<div class="sourceCode" id="cb3349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3349-1"><a href="discriminant-analysis.html#cb3349-1"></a>response4 &lt;-<span class="st"> </span>swiss <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(length<span class="op">:</span>diag) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()</span>
<span id="cb3349-2"><a href="discriminant-analysis.html#cb3349-2"></a>swiss<span class="fl">.2</span>a &lt;-<span class="st"> </span><span class="kw">manova</span>(response4 <span class="op">~</span><span class="st"> </span>status, <span class="dt">data =</span> swiss)</span>
<span id="cb3349-3"><a href="discriminant-analysis.html#cb3349-3"></a><span class="kw">summary</span>(swiss<span class="fl">.2</span>a)</span></code></pre></div>
<pre><code>##            Df  Pillai approx F num Df den Df    Pr(&gt;F)    
## status      1 0.92415   391.92      6    193 &lt; 2.2e-16 ***
## Residuals 198                                             
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Anyway, the conclusion: the status of a bill (genuine or counterfeit)
definitely has an influence on some or all of those other variables,
since the P-value <span class="math inline">\(2.2 \times 10^{-16}\)</span> (or less) is really small. So
it is worth running a discriminant analysis to figure out where the
differences lie.</p>
<p>As a piece of strategy, for creating the response matrix, you can
always either use <code>cbind</code>, which creates a <code>matrix</code>
directly, or you can use <code>select</code>, which is often easier but
creates a data frame, and then turn <em>that</em> into a <code>matrix</code>
using <code>as.matrix</code>. As long as you end up with a
<code>matrix</code>, it’s all good.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Run a discriminant analysis. Display the output.</li>
</ol>
<p>Solution</p>
<p>Now we forget about all that
<code>response</code> stuff. For a discriminant analysis, the
grouping variable (or combination of the grouping variables)
is the “response”, and the quantitative ones are
“explanatory”:</p>
<div class="sourceCode" id="cb3351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3351-1"><a href="discriminant-analysis.html#cb3351-1"></a>swiss<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(status <span class="op">~</span><span class="st"> </span>length <span class="op">+</span><span class="st"> </span>left <span class="op">+</span><span class="st"> </span>right <span class="op">+</span><span class="st"> </span>bottom <span class="op">+</span><span class="st"> </span>top <span class="op">+</span><span class="st"> </span>diag, <span class="dt">data =</span> swiss)</span>
<span id="cb3351-2"><a href="discriminant-analysis.html#cb3351-2"></a>swiss<span class="fl">.3</span></span></code></pre></div>
<pre><code>## Call:
## lda(status ~ length + left + right + bottom + top + diag, data = swiss)
## 
## Prior probabilities of groups:
## counterfeit     genuine 
##         0.5         0.5 
## 
## Group means:
##              length    left   right bottom    top    diag
## counterfeit 214.823 130.300 130.193 10.530 11.133 139.450
## genuine     214.969 129.943 129.720  8.305 10.168 141.517
## 
## Coefficients of linear discriminants:
##                 LD1
## length  0.005011113
## left    0.832432523
## right  -0.848993093
## bottom -1.117335597
## top    -1.178884468
## diag    1.556520967</code></pre>
<ol start="4" style="list-style-type: lower-alpha">
<li>How many linear
discriminants did you get? Is that making sense? Explain briefly.</li>
</ol>
<p>Solution</p>
<p>I got one discriminant, which makes sense because there are two
groups, and the smaller of 6 (variables, not counting the grouping
one) and <span class="math inline">\(2-1\)</span> is 1.</p>
<ol start="5" style="list-style-type: lower-alpha">
<li><a name="part:big">*</a>
Using your output from the discriminant analysis, describe how
each of the linear discriminants that you got is related to your
original variables. (This can, maybe even should, be done crudely:
“does each variable feature in each linear discriminant: yes or no?”.)</li>
</ol>
<p>Solution</p>
<p>This is the Coefficients of Linear Discriminants. Make a call about whether each of those coefficients is close to zero (small in size compared to the others), or definitely positive or definitely negative.
These are judgement calls: either you can say that LD1
depends mainly on <code>diag</code> (treating the other coefficients
as “small” or close to zero), or you can say that <code>LD1</code>
depends on everything except <code>length</code>.</p>
<ol start="6" style="list-style-type: lower-alpha">
<li>What values of your variable(s) would make <code>LD1</code>
large and positive?</li>
</ol>
<p>Solution</p>
<p>Depending on your answer to the previous part:
If you said that only <code>diag</code> was important, <code>diag</code>
being large would make <code>LD1</code> large and positive.
If you said that everything but <code>length</code> was important,
then it’s a bit more complicated: <code>left</code> and
<code>diag</code> large, <code>right</code>, <code>bottom</code> and
<code>top</code> small (since their coefficients are negative).</p>
<ol start="7" style="list-style-type: lower-alpha">
<li><a name="part:means">*</a> Find the means of each variable for each group (genuine
and counterfeit bills). You can get this from your fitted linear
discriminant object.</li>
</ol>
<p>Solution</p>
<div class="sourceCode" id="cb3353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3353-1"><a href="discriminant-analysis.html#cb3353-1"></a>swiss<span class="fl">.3</span><span class="op">$</span>means</span></code></pre></div>
<pre><code>##              length    left   right bottom    top    diag
## counterfeit 214.823 130.300 130.193 10.530 11.133 139.450
## genuine     214.969 129.943 129.720  8.305 10.168 141.517</code></pre>
<ol start="8" style="list-style-type: lower-alpha">
<li>Plot your linear discriminant(s), however you like. Bear in
mind that there is only one linear discriminant.</li>
</ol>
<p>Solution</p>
<p>With only one linear discriminant, we can plot <code>LD1</code> scores on
the <span class="math inline">\(y\)</span>-axis and the grouping variable on the <span class="math inline">\(x\)</span>-axis. How
you do that is up to you.</p>
<p>Before we start, though, we need the <code>LD1</code> scores. This means
doing predictions. The discriminant scores are in there. We take the
prediction output and make a data frame with all the things in the
original data. My current preference (it changes) is to store the
predictions, and then <code>cbind</code> them with the original data,
thus:</p>
<div class="sourceCode" id="cb3355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3355-1"><a href="discriminant-analysis.html#cb3355-1"></a>swiss.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(swiss<span class="fl">.3</span>)</span>
<span id="cb3355-2"><a href="discriminant-analysis.html#cb3355-2"></a>d &lt;-<span class="st"> </span><span class="kw">cbind</span>(swiss, swiss.pred)</span>
<span id="cb3355-3"><a href="discriminant-analysis.html#cb3355-3"></a><span class="kw">head</span>(d)</span></code></pre></div>
<pre><code>##   length  left right bottom  top  diag  status   class posterior.counterfeit
## 1  214.8 131.0 131.1    9.0  9.7 141.0 genuine genuine          3.245560e-07
## 2  214.6 129.7 129.7    8.1  9.5 141.7 genuine genuine          1.450624e-14
## 3  214.8 129.7 129.7    8.7  9.6 142.2 genuine genuine          1.544496e-14
## 4  214.8 129.7 129.6    7.5 10.4 142.0 genuine genuine          4.699587e-15
## 5  215.0 129.6 129.7   10.4  7.7 141.8 genuine genuine          1.941700e-13
## 6  215.7 130.8 130.5    9.0 10.1 141.4 genuine genuine          1.017550e-08
##   posterior.genuine      LD1
## 1         0.9999997 2.150948
## 2         1.0000000 4.587317
## 3         1.0000000 4.578290
## 4         1.0000000 4.749580
## 5         1.0000000 4.213851
## 6         1.0000000 2.649422</code></pre>
<p>I needed <code>head</code> because <code>cbind</code> makes an old-fashioned
<code>data.frame</code> rather than a <code>tibble</code>, so if you display
it, you get all of it.</p>
<p>This gives the LD1 scores, predicted groups, and posterior
probabilities as well. That saves us having to pick out the other
things later.
The obvious thing is a boxplot. By examining <code>d</code> above (didn’t
you?), you saw that the LD scores were in a column called
<code>LD1</code>:</p>
<div class="sourceCode" id="cb3357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3357-1"><a href="discriminant-analysis.html#cb3357-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> status, <span class="dt">y =</span> LD1)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_boxplot</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/antioch-1.png" width="672"  /></p>
<p>This shows that positive LD1 scores go (almost without exception) with
genuine bills, and negative ones with counterfeit bills.
It also shows that there are three outlier bills, two counterfeit ones
with unusually high LD1 score, and one genuine one with unusually
<em>low</em> LD1 score, at least for a genuine bill.</p>
<p>Or you could do faceted histograms of <code>LD1</code> by <code>status</code>:</p>
<div class="sourceCode" id="cb3358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3358-1"><a href="discriminant-analysis.html#cb3358-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> LD1)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">10</span>) <span class="op">+</span><span class="st"> </span><span class="kw">facet_grid</span>(status <span class="op">~</span><span class="st"> </span>.)</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1684-1.png" width="672"  /></p>
<p>This shows much the same thing as <code>plot(swiss.3)</code> does (try it).</p>
<ol style="list-style-type: lower-roman">
<li>What kind of score on <code>LD1</code> do genuine bills
typically have? What kind of score do counterfeit bills typically
have? What characteristics of a bill, therefore, would you look at
to determine if a bill is genuine or counterfeit?</li>
</ol>
<p>Solution</p>
<p>The genuine bills almost all have a <em>positive</em> score on
LD1, while the counterfeit ones all have a <em>negative</em> one.
This means that the genuine bills (depending on your answer to
(<a href="#part:big">here</a>)) have a large <code>diag</code>, or they have a
large <code>left</code> and <code>diag</code>, and a small
<code>right</code>, <code>bottom</code> and <code>top</code>.
If you look at your table of means in (<a href="#part:means">here</a>), you’ll
see that the genuine bills do indeed have a large <code>diag</code>,
or, depending on your earlier answer, a small <code>right</code>,
<code>bottom</code> and <code>top</code>, but not actually a small
<code>left</code> (the <code>left</code> values are very close for the
genuine and counterfeit coins).
As to that last point, this is easy enough to think about. A
boxplot seems a nice way to display it:</p>
<div class="sourceCode" id="cb3359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3359-1"><a href="discriminant-analysis.html#cb3359-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">y =</span> left, <span class="dt">x =</span> status)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_boxplot</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/gtabita-1.png" width="672"  /></p>
<p>There is a fair bit of overlap: the median is higher for the
counterfeit bills, but the highest value actually belongs to a genuine one.</p>
<p>Compare that to <code>diag</code>:</p>
<div class="sourceCode" id="cb3360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3360-1"><a href="discriminant-analysis.html#cb3360-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">y =</span> diag, <span class="dt">x =</span> status)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_boxplot</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/iggle-1.png" width="672"  /></p>
<p>Here, there is an almost complete separation of the genuine and
counterfeit bills, with just one low outlier amongst the genuine bills
spoiling the pattern.
I didn’t look at the predictions (beyond the discriminant scores),
since this question (as set on an assignment a couple of years ago)
was already too long, but there is no difficulty in doing so.
Everything is in the data frame I called <code>d</code>:</p>
<div class="sourceCode" id="cb3361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3361-1"><a href="discriminant-analysis.html#cb3361-1"></a><span class="kw">with</span>(d, <span class="kw">table</span>(<span class="dt">obs =</span> status, <span class="dt">pred =</span> class))</span></code></pre></div>
<pre><code>##              pred
## obs           counterfeit genuine
##   counterfeit         100       0
##   genuine               1      99</code></pre>
<p>(this labels the rows and columns, which is not necessary but is nice.)</p>
<p>The <code>tidyverse</code> way is to make a data frame out of the actual
and predicted statuses, and then <code>count</code> what’s in there:</p>
<div class="sourceCode" id="cb3363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3363-1"><a href="discriminant-analysis.html#cb3363-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(status, class)</span></code></pre></div>
<pre><code>##        status       class   n
## 1 counterfeit counterfeit 100
## 2     genuine counterfeit   1
## 3     genuine     genuine  99</code></pre>
<p>This gives a “long” table, with frequencies for each of the
combinations for which anything was observed.</p>
<p>Frequency tables are usually wide, and we can make this one so by pivot-wider-ing <code>pred</code>:</p>
<div class="sourceCode" id="cb3365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3365-1"><a href="discriminant-analysis.html#cb3365-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3365-2"><a href="discriminant-analysis.html#cb3365-2"></a><span class="st">  </span><span class="kw">count</span>(status, class) <span class="op">%&gt;%</span></span>
<span id="cb3365-3"><a href="discriminant-analysis.html#cb3365-3"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from =</span> class, <span class="dt">values_from =</span> n)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   status      counterfeit genuine
##   &lt;chr&gt;             &lt;int&gt;   &lt;int&gt;
## 1 counterfeit         100      NA
## 2 genuine               1      99</code></pre>
<p>One of the genuine bills is incorrectly classified as a counterfeit
one (evidently that low outlier on LD1), but every single one of the
counterfeit bills is classified correctly. That missing value is
actually a frequency that is zero, which you can fix up thus:</p>
<div class="sourceCode" id="cb3367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3367-1"><a href="discriminant-analysis.html#cb3367-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3367-2"><a href="discriminant-analysis.html#cb3367-2"></a><span class="st">  </span><span class="kw">count</span>(status, class) <span class="op">%&gt;%</span></span>
<span id="cb3367-3"><a href="discriminant-analysis.html#cb3367-3"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from =</span> class, <span class="dt">values_from =</span> n, <span class="dt">values_fill =</span> <span class="dv">0</span>) </span></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   status      counterfeit genuine
##   &lt;chr&gt;             &lt;int&gt;   &lt;int&gt;
## 1 counterfeit         100       0
## 2 genuine               1      99</code></pre>
<p>which turns any missing values into the zeroes they should be in this
kind of problem.
It would be interesting to see what the posterior probabilities look
like for that misclassified bill:</p>
<div class="sourceCode" id="cb3369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3369-1"><a href="discriminant-analysis.html#cb3369-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(status <span class="op">!=</span><span class="st"> </span>class)</span></code></pre></div>
<pre><code>##    length  left right bottom  top  diag  status       class posterior.counterfeit
## 70  214.9 130.2 130.2      8 11.2 139.6 genuine counterfeit             0.9825773
##    posterior.genuine        LD1
## 70        0.01742267 -0.5805239</code></pre>
<p>On the basis of the six measured variables, this looks a lot more like
a counterfeit bill than a genuine one.
Are there any other bills where there is any doubt? One way to find out is to find the maximum of the two posterior probabilities. If this is small,
there is some doubt about whether the bill is real or fake. 0.99 seems like a very stringent cutoff, but let’s try it and see:</p>
<div class="sourceCode" id="cb3371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3371-1"><a href="discriminant-analysis.html#cb3371-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3371-2"><a href="discriminant-analysis.html#cb3371-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">max.post =</span> <span class="kw">pmax</span>(posterior.counterfeit, posterior.genuine)) <span class="op">%&gt;%</span></span>
<span id="cb3371-3"><a href="discriminant-analysis.html#cb3371-3"></a><span class="st">  </span><span class="kw">filter</span>(max.post <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.99</span>) <span class="op">%&gt;%</span></span>
<span id="cb3371-4"><a href="discriminant-analysis.html#cb3371-4"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span><span class="kw">c</span>(length<span class="op">:</span>diag))</span></code></pre></div>
<pre><code>##     status       class posterior.counterfeit posterior.genuine        LD1  max.post
## 70 genuine counterfeit             0.9825773        0.01742267 -0.5805239 0.9825773</code></pre>
<p>The only one is the bill that was misclassified: it was actually genuine, but
was classified as counterfeit. The posterior probabilities say that it
was pretty unlikely to be genuine, but it was the only bill for which
there was any noticeable doubt at all.</p>
<p>I had to use <code>pmax</code> rather than <code>max</code> there, because I
wanted <code>max.post</code> to contain the larger of the two
corresponding entries: that is, the first entry in <code>max.post</code>
is the larger of the first entry of <code>counterfeit</code> and the first
entry in <code>genuine</code>. If I used <code>max</code> instead, I’d get the
largest of <em>all</em> the entries in <code>counterfeit</code> and
<em>all</em> the entries in <code>genuine</code>, repeated 200 times. (Try
it and see.) <code>pmax</code> stands for “parallel maximum”, that is,
for each row separately. This also should work:</p>
<div class="sourceCode" id="cb3373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3373-1"><a href="discriminant-analysis.html#cb3373-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3373-2"><a href="discriminant-analysis.html#cb3373-2"></a><span class="st">  </span><span class="kw">rowwise</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3373-3"><a href="discriminant-analysis.html#cb3373-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">max.post =</span> <span class="kw">max</span>(posterior.counterfeit, posterior.genuine)) <span class="op">%&gt;%</span></span>
<span id="cb3373-4"><a href="discriminant-analysis.html#cb3373-4"></a><span class="st">  </span><span class="kw">filter</span>(max.post <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.99</span>) <span class="op">%&gt;%</span></span>
<span id="cb3373-5"><a href="discriminant-analysis.html#cb3373-5"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="kw">c</span>(length<span class="op">:</span>diag))</span></code></pre></div>
<pre><code>## # A tibble: 1 x 6
## # Rowwise: 
##   status  class       posterior.counterfeit posterior.genuine    LD1 max.post
##   &lt;chr&gt;   &lt;fct&gt;                       &lt;dbl&gt;             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
## 1 genuine counterfeit                 0.983            0.0174 -0.581    0.983</code></pre>
<p>Because we’re using <code>rowwise</code>, <code>max</code> is applied to the pairs
of values of <code>posterior.counterfeit</code> and <code>posterior.genuine</code>,
<em>taken one row at a time.</em></p>
</div>
<div id="urine-and-obesity-what-makes-a-difference" class="section level2" number="26.2">
<h2><span class="header-section-number">26.2</span> Urine and obesity: what makes a difference?</h2>
<p>A study was made of the characteristics of urine of young
men. The men were classified into four groups based on their degree of
obesity. (The groups are labelled <code>a, b, c, d</code>.) Four variables
were measured, <code>x</code> (which you can ignore), pigment creatinine,
chloride and chlorine. The data are in
<a href="http://www.utsc.utoronto.ca/~butler/d29/urine.csv">link</a> as a
<code>.csv</code> file. There are 45 men altogether.</p>
<p>Yes, you saw this one before. What you found was something like this:</p>
<div class="sourceCode" id="cb3375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3375-1"><a href="discriminant-analysis.html#cb3375-1"></a>my_url &lt;-<span class="st"> &quot;http://www.utsc.utoronto.ca/~butler/d29/urine.csv&quot;</span></span>
<span id="cb3375-2"><a href="discriminant-analysis.html#cb3375-2"></a>urine &lt;-<span class="st"> </span><span class="kw">read_csv</span>(my_url)</span></code></pre></div>
<pre><code>## 
## ── Column specification ──────────────────────────────────────────────────────────────────
## cols(
##   obesity = col_character(),
##   x = col_double(),
##   creatinine = col_double(),
##   chloride = col_double(),
##   chlorine = col_double()
## )</code></pre>
<div class="sourceCode" id="cb3377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3377-1"><a href="discriminant-analysis.html#cb3377-1"></a>response &lt;-<span class="st"> </span><span class="kw">with</span>(urine, <span class="kw">cbind</span>(creatinine, chlorine, chloride))</span>
<span id="cb3377-2"><a href="discriminant-analysis.html#cb3377-2"></a>urine<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">manova</span>(response <span class="op">~</span><span class="st"> </span>obesity, <span class="dt">data =</span> urine)</span>
<span id="cb3377-3"><a href="discriminant-analysis.html#cb3377-3"></a><span class="kw">summary</span>(urine<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>##           Df  Pillai approx F num Df den Df  Pr(&gt;F)  
## obesity    3 0.43144   2.2956      9    123 0.02034 *
## Residuals 41                                         
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Our aim is to understand why this result was significant.</p>
<ol style="list-style-type: lower-alpha">
<li>Read in the data again (copy the code from above) and
obtain a discriminant analysis.</li>
</ol>
<p>Solution</p>
<p>As above, plus:</p>
<div class="sourceCode" id="cb3379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3379-1"><a href="discriminant-analysis.html#cb3379-1"></a>urine<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(obesity <span class="op">~</span><span class="st"> </span>creatinine <span class="op">+</span><span class="st"> </span>chlorine <span class="op">+</span><span class="st"> </span>chloride, <span class="dt">data =</span> urine)</span>
<span id="cb3379-2"><a href="discriminant-analysis.html#cb3379-2"></a>urine<span class="fl">.1</span></span></code></pre></div>
<pre><code>## Call:
## lda(obesity ~ creatinine + chlorine + chloride, data = urine)
## 
## Prior probabilities of groups:
##         a         b         c         d 
## 0.2666667 0.3111111 0.2444444 0.1777778 
## 
## Group means:
##   creatinine chlorine chloride
## a   15.89167 5.275000 6.012500
## b   17.82143 7.450000 5.214286
## c   16.34545 8.272727 5.372727
## d   11.91250 9.675000 3.981250
## 
## Coefficients of linear discriminants:
##                    LD1        LD2         LD3
## creatinine  0.24429462 -0.1700525 -0.02623962
## chlorine   -0.02167823 -0.1353051  0.11524045
## chloride    0.23805588  0.3590364  0.30564592
## 
## Proportion of trace:
##    LD1    LD2    LD3 
## 0.7476 0.2430 0.0093</code></pre>
<ol start="2" style="list-style-type: lower-alpha">
<li>How many linear discriminants were you expecting? Explain briefly.</li>
</ol>
<p>Solution</p>
<p>There are 3 variables and 4 groups, so the smaller of 3 and
<span class="math inline">\(4-1=3\)</span>: that is, 3.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Why do you think we should pay attention to the first two
linear discriminants but not the third? Explain briefly.</li>
</ol>
<p>Solution</p>
<p>The first two ``proportion of
trace’’ values are a lot bigger than the third (or, the third
one is close to 0).</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>Plot the first two linear discriminant scores (against each
other), with each obesity group being a different colour.</li>
</ol>
<p>Solution</p>
<p>First obtain the predictions, and
then make a data frame out of the original data and the
predictions.</p>
<div class="sourceCode" id="cb3381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3381-1"><a href="discriminant-analysis.html#cb3381-1"></a>urine.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(urine<span class="fl">.1</span>)</span>
<span id="cb3381-2"><a href="discriminant-analysis.html#cb3381-2"></a>d &lt;-<span class="st"> </span><span class="kw">cbind</span>(urine, urine.pred)</span>
<span id="cb3381-3"><a href="discriminant-analysis.html#cb3381-3"></a><span class="kw">head</span>(d)</span></code></pre></div>
<pre><code>##   obesity  x creatinine chloride chlorine class posterior.a posterior.b posterior.c
## 1       a 24       17.6     5.15      7.5     b   0.2327008   0.4124974   0.3022445
## 2       a 32       13.4     5.75      7.1     a   0.3599095   0.2102510   0.2633959
## 3       a 17       20.3     4.35      2.3     b   0.2271118   0.4993603   0.2519562
## 4       a 30       22.3     7.55      4.0     b   0.2935374   0.4823766   0.2211991
## 5       a 30       20.5     8.50      2.0     a   0.4774623   0.3258104   0.1933571
## 6       a 27       18.5    10.25      2.0     a   0.6678748   0.1810762   0.1482500
##   posterior.d      x.LD1      x.LD2      x.LD3
## 1 0.052557333  0.3926519 -0.3290621 -0.0704284
## 2 0.166443708 -0.4818807  0.6547023  0.1770694
## 3 0.021571722  0.9745295 -0.3718462 -0.9850425
## 4 0.002886957  2.1880446  0.2069465  0.1364540
## 5 0.003370286  2.0178238  1.1247359  0.2435680
## 6 0.002799004  1.9458323  2.0931546  0.8309276</code></pre>
<p><code>urine</code> produced the first five columns and <code>urine.pred</code>
produced the rest.</p>
<p>To go a more tidyverse way, we can combine the original data frame and
the predictions using <code>bind_cols</code>, but we have to be more
careful that the things we are gluing together are both data frames:</p>
<div class="sourceCode" id="cb3383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3383-1"><a href="discriminant-analysis.html#cb3383-1"></a><span class="kw">class</span>(urine)</span></code></pre></div>
<pre><code>## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot;      &quot;tbl&quot;         &quot;data.frame&quot;</code></pre>
<div class="sourceCode" id="cb3385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3385-1"><a href="discriminant-analysis.html#cb3385-1"></a><span class="kw">class</span>(urine.pred)</span></code></pre></div>
<pre><code>## [1] &quot;list&quot;</code></pre>
<p><code>urine</code> is a <code>tibble</code> all right, but <code>urine.pred</code> is a <code>list</code>. What does it look like?</p>
<div class="sourceCode" id="cb3387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3387-1"><a href="discriminant-analysis.html#cb3387-1"></a><span class="kw">glimpse</span>(urine.pred)</span></code></pre></div>
<pre><code>## List of 3
##  $ class    : Factor w/ 4 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;: 2 1 2 2 1 1 3 3 1 1 ...
##  $ posterior: num [1:45, 1:4] 0.233 0.36 0.227 0.294 0.477 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:45] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:4] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot;
##  $ x        : num [1:45, 1:3] 0.393 -0.482 0.975 2.188 2.018 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:45] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:3] &quot;LD1&quot; &quot;LD2&quot; &quot;LD3&quot;</code></pre>
<p>A data frame is a list for which all the items are the same length,
but some of the things in here are matrices. You can tell because they
have a number of rows, 45, <em>and</em> a number of columns, 3 or
4. They <em>do</em> have the right number of rows, though, so something
like <code>as.data.frame</code> (a base R function) will smoosh them all
into one data frame, grabbing the columns from the matrices:</p>
<div class="sourceCode" id="cb3389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3389-1"><a href="discriminant-analysis.html#cb3389-1"></a><span class="kw">head</span>(<span class="kw">as.data.frame</span>(urine.pred))</span></code></pre></div>
<pre><code>##   class posterior.a posterior.b posterior.c posterior.d      x.LD1      x.LD2      x.LD3
## 1     b   0.2327008   0.4124974   0.3022445 0.052557333  0.3926519 -0.3290621 -0.0704284
## 2     a   0.3599095   0.2102510   0.2633959 0.166443708 -0.4818807  0.6547023  0.1770694
## 3     b   0.2271118   0.4993603   0.2519562 0.021571722  0.9745295 -0.3718462 -0.9850425
## 4     b   0.2935374   0.4823766   0.2211991 0.002886957  2.1880446  0.2069465  0.1364540
## 5     a   0.4774623   0.3258104   0.1933571 0.003370286  2.0178238  1.1247359  0.2435680
## 6     a   0.6678748   0.1810762   0.1482500 0.002799004  1.9458323  2.0931546  0.8309276</code></pre>
<p>You see that the columns that came from matrices have gained two-part names, the first part from the name of the matrix, the second part from the column name within that matrix. Then we can do this:</p>
<div class="sourceCode" id="cb3391"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3391-1"><a href="discriminant-analysis.html#cb3391-1"></a>dd &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(urine, <span class="kw">as.data.frame</span>(urine.pred))</span>
<span id="cb3391-2"><a href="discriminant-analysis.html#cb3391-2"></a>dd</span></code></pre></div>
<pre><code>## # A tibble: 45 x 13
##    obesity     x creatinine chloride chlorine class posterior.a posterior.b posterior.c
##  * &lt;chr&gt;   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
##  1 a          24       17.6     5.15      7.5 b           0.233      0.412        0.302
##  2 a          32       13.4     5.75      7.1 a           0.360      0.210        0.263
##  3 a          17       20.3     4.35      2.3 b           0.227      0.499        0.252
##  4 a          30       22.3     7.55      4   b           0.294      0.482        0.221
##  5 a          30       20.5     8.5       2   a           0.477      0.326        0.193
##  6 a          27       18.5    10.2       2   a           0.668      0.181        0.148
##  7 a          25       12.1     5.95     16.8 c           0.167      0.208        0.314
##  8 a          30       12       6.3      14.5 c           0.230      0.197        0.303
##  9 a          28       10.1     5.45      0.9 a           0.481      0.0752       0.137
## 10 a          24       14.7     3.75      2   a           0.323      0.247        0.239
## # … with 35 more rows, and 4 more variables: posterior.d &lt;dbl&gt;, x.LD1 &lt;dbl&gt;, x.LD2 &lt;dbl&gt;,
## #   x.LD3 &lt;dbl&gt;</code></pre>
<p>If you want to avoid base R altogether, though, and go straight to
<code>bind_cols</code>, you have to be more careful about the types of
things. <code>bind_cols</code> <em>only</em> works with vectors and data
frames, not matrices, so that is what it is up to you to make sure you
have. That means pulling out the pieces, turning them from matrices
into data frames, and then gluing everything back together:</p>
<div class="sourceCode" id="cb3393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3393-1"><a href="discriminant-analysis.html#cb3393-1"></a>post &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(urine.pred<span class="op">$</span>posterior)</span>
<span id="cb3393-2"><a href="discriminant-analysis.html#cb3393-2"></a>ld &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(urine.pred<span class="op">$</span>x)</span>
<span id="cb3393-3"><a href="discriminant-analysis.html#cb3393-3"></a>ddd &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(urine, <span class="dt">class =</span> urine.pred<span class="op">$</span>class, ld, post)</span>
<span id="cb3393-4"><a href="discriminant-analysis.html#cb3393-4"></a>ddd</span></code></pre></div>
<pre><code>## # A tibble: 45 x 13
##    obesity     x creatinine chloride chlorine class    LD1     LD2     LD3     a      b
##    &lt;chr&gt;   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1 a          24       17.6     5.15      7.5 b      0.393 -0.329  -0.0704 0.233 0.412 
##  2 a          32       13.4     5.75      7.1 a     -0.482  0.655   0.177  0.360 0.210 
##  3 a          17       20.3     4.35      2.3 b      0.975 -0.372  -0.985  0.227 0.499 
##  4 a          30       22.3     7.55      4   b      2.19   0.207   0.136  0.294 0.482 
##  5 a          30       20.5     8.5       2   a      2.02   1.12    0.244  0.477 0.326 
##  6 a          27       18.5    10.2       2   a      1.95   2.09    0.831  0.668 0.181 
##  7 a          25       12.1     5.95     16.8 c     -0.962 -0.365   1.39   0.167 0.208 
##  8 a          30       12       6.3      14.5 c     -0.853  0.0890  1.23   0.230 0.197 
##  9 a          28       10.1     5.45      0.9 a     -1.23   1.95   -0.543  0.481 0.0752
## 10 a          24       14.7     3.75      2   a     -0.530  0.406  -1.06   0.323 0.247 
## # … with 35 more rows, and 2 more variables: c &lt;dbl&gt;, d &lt;dbl&gt;</code></pre>
<p>That’s a lot of work, but you might say that it’s worth it because you
are now absolutely sure what kind of thing everything is. I also had
to be slightly careful with the vector of <code>class</code> values; in
<code>ddd</code> it has to have a name, so I have to make sure I give it
one.
<label for="tufte-mn-219" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-219" class="margin-toggle"><span class="marginnote">If you run into an error like <em>Argument 2 must have names</em> here, that means that the second thing, <em>class</em>, needs to have a name and doesn’t have one.</span>
Any of these ways (in general) is good. The last way is a more
careful approach, since you are making sure things are of the right
type rather than relying on R to convert them for you, but I don’t
mind which way you go.
Now make the plot, making sure that you are using columns with the right names. I’m using my first data frame, with the two-part names:</p>
<div class="sourceCode" id="cb3395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3395-1"><a href="discriminant-analysis.html#cb3395-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> x.LD1, <span class="dt">y =</span> x.LD2, <span class="dt">colour =</span> obesity)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1700-1.png" width="672"  /></p>
<ol start="5" style="list-style-type: lower-alpha">
<li><a name="part:plot">*</a> Looking at your plot, discuss how (if at all) the
discriminants separate the obesity groups. (Where does each
obesity group fall on the plot?)</li>
</ol>
<p>Solution</p>
<p>My immediate reaction was
“they don’t much”. If you look a bit more closely, the
<code>b</code> group, in green, is on the right (high
<code>LD1</code>) and the <code>d</code> group (purple) is on the
left (low <code>LD1</code>). The <code>a</code> group, red, is
mostly at the top (high <code>LD2</code>) but the <code>c</code>
group, blue, really is all over the place.</p>
<p>The way to tackle interpreting a plot like this is to look
for each group individually and see if that group is only
or mainly found on a certain part of the plot.</p>
<p>This can be rationalized by looking at
the “coefficients of linear discriminants” on the output. <code>LD1</code> is
low if creatinine and chloride are low (it has nothing
much to do with <code>chlorine</code> since that coefficient
is near zero). Group <code>d</code> is lowest on both
creatinine and chloride, so that will be lowest on
<code>LD1</code>. <code>LD2</code> is high if <code>chloride</code>
is high, or <code>creatinine</code> and <code>chlorine</code> are
low. Out of the groups <code>a, b, c</code>, <code>a</code> has
the highest mean on chloride and lowest means on the other
two variables, so this should be highest on <code>LD2</code>
and (usually) is.
Looking at the means is only part of the story; if the
individuals within a group are very variable, as they are
here (especially group <code>c</code>), then that group will
appear all over the plot. The table of means only says how
the <em>average</em> individual within a group stacks up.</p>
<div class="sourceCode" id="cb3396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3396-1"><a href="discriminant-analysis.html#cb3396-1"></a><span class="kw">ggbiplot</span>(urine<span class="fl">.1</span>, <span class="dt">groups =</span> urine<span class="op">$</span>obesity)</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1701-1.png" width="672"  /></p>
<p>This shows (in a way that is perhaps easier to see) how the linear
discriminants are related to the original variables, and thus how the
groups differ in terms of the original variables.
<label for="tufte-mn-220" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-220" class="margin-toggle"><span class="marginnote">This was why we were doing discriminant analysis in the first place.</span>
Most of the B’s are high creatinine and high chloride (on the right); most of the D’s are low on both (on the left). LD2 has a bit of <code>chloride</code>, but not much of anything else.
Extra: the way we used to do this was with “base graphics”, which involved plotting the <code>lda</code> output itself:</p>
<div class="sourceCode" id="cb3397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3397-1"><a href="discriminant-analysis.html#cb3397-1"></a><span class="kw">plot</span>(urine<span class="fl">.1</span>)</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1702-1.png" width="672"  /></p>
<p>which is a plot of each discriminant score against each other
one. You can plot just the first two, like this:</p>
<div class="sourceCode" id="cb3398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3398-1"><a href="discriminant-analysis.html#cb3398-1"></a><span class="kw">plot</span>(urine<span class="fl">.1</span>, <span class="dt">dimen =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1703-1.png" width="672"  /></p>
<p>This is easier than using <code>ggplot</code>, but (i) less flexible and
(ii) you have to figure out how it works rather than doing things the
standard <code>ggplot</code> way. So I went with constructing a data frame
from the predictions, and then
<code>ggplot</code>ting that. It’s a matter of taste which way is better.</p>
<ol start="6" style="list-style-type: lower-alpha">
<li><a name="part:table">*</a> Obtain a table showing observed and predicted obesity
groups. Comment on the accuracy of the predictions.</li>
</ol>
<p>Solution</p>
<p>Make a table, one way or another:</p>
<div class="sourceCode" id="cb3399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3399-1"><a href="discriminant-analysis.html#cb3399-1"></a>tab &lt;-<span class="st"> </span><span class="kw">with</span>(d, <span class="kw">table</span>(obesity, class))</span>
<span id="cb3399-2"><a href="discriminant-analysis.html#cb3399-2"></a>tab</span></code></pre></div>
<pre><code>##        class
## obesity a b c d
##       a 7 3 2 0
##       b 2 9 2 1
##       c 3 4 1 3
##       d 2 0 1 5</code></pre>
<p><code>class</code> is always the <em>predicted</em> group in these. You can
also name things in <code>table</code>.
Or, if you prefer (equally good), the <code>tidyverse</code> way of
counting all the combinations of true <code>obesity</code> and predicted
<code>class</code>, which can be done all in one go, or in
two steps by saving the data frame first. I’m saving my results for
later:</p>
<div class="sourceCode" id="cb3401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3401-1"><a href="discriminant-analysis.html#cb3401-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(obesity, class) -&gt;<span class="st"> </span>tab</span>
<span id="cb3401-2"><a href="discriminant-analysis.html#cb3401-2"></a>tab</span></code></pre></div>
<pre><code>##    obesity class n
## 1        a     a 7
## 2        a     b 3
## 3        a     c 2
## 4        b     a 2
## 5        b     b 9
## 6        b     c 2
## 7        b     d 1
## 8        c     a 3
## 9        c     b 4
## 10       c     c 1
## 11       c     d 3
## 12       d     a 2
## 13       d     c 1
## 14       d     d 5</code></pre>
<p>or if you prefer to make it look more like a table of frequencies:</p>
<div class="sourceCode" id="cb3403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3403-1"><a href="discriminant-analysis.html#cb3403-1"></a>tab <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pivot_wider</span>(<span class="dt">names_from=</span>class, <span class="dt">values_from=</span>n, <span class="dt">values_fill =</span> <span class="kw">list</span>(<span class="dt">n=</span><span class="dv">0</span>))</span></code></pre></div>
<pre><code>## # A tibble: 4 x 5
##   obesity     a     b     c     d
##   &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1 a           7     3     2     0
## 2 b           2     9     2     1
## 3 c           3     4     1     3
## 4 d           2     0     1     5</code></pre>
<p>The thing on the end fills in zero frequencies as such (they would
otherwise be <code>NA</code>, which they are not: we know they are zero).
My immediate reaction to this is “it’s terrible”! But at least some
of the men have their obesity group correctly predicted: 7 of the
<span class="math inline">\(7+3+2+0=12\)</span>
men that are actually in group <code>a</code> are predicted to be in
<code>a</code>; 9 of the 14 actual <code>b</code>’s are predicted to be
<code>b</code>’s; 5 of the 8 actual <code>d</code>’s are predicted to be
<code>d</code>’s. These are not so awful. But only 1 of the 11
<code>c</code>’s is correctly predicted to be a <code>c</code>!</p>
<p>As for what I want to see: I am looking for some kind of statement
about how good you think the predictions are (the word “terrible” is
fine for this) with some kind of support for your statement. For
example, “the predictions are not that good, but at least group B is predicted with some accuracy (9 out of 14).”</p>
<p>I think looking at how well the individual groups were predicted is
the most incisive way of getting at this, because the <code>c</code> men
are the hardest to get right and the others are easier, but you could
also think about an overall misclassification rate. This comes most
easily from the “tidy” table:</p>
<div class="sourceCode" id="cb3405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3405-1"><a href="discriminant-analysis.html#cb3405-1"></a>tab <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(<span class="dt">correct =</span> (obesity <span class="op">==</span><span class="st"> </span>class), <span class="dt">wt =</span> n)</span></code></pre></div>
<pre><code>##   correct  n
## 1   FALSE 23
## 2    TRUE 22</code></pre>
<p>You can count anything, not just columns that already exist. This one
is a kind of combined mutate-and-count to create the (logical) column
called <code>correct</code>.</p>
<p>It’s a shortcut for this:</p>
<div class="sourceCode" id="cb3407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3407-1"><a href="discriminant-analysis.html#cb3407-1"></a>tab <span class="op">%&gt;%</span></span>
<span id="cb3407-2"><a href="discriminant-analysis.html#cb3407-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">is_correct =</span> (obesity <span class="op">==</span><span class="st"> </span>class)) <span class="op">%&gt;%</span></span>
<span id="cb3407-3"><a href="discriminant-analysis.html#cb3407-3"></a><span class="st">  </span><span class="kw">count</span>(is_correct, <span class="dt">wt =</span> n)</span></code></pre></div>
<pre><code>##   is_correct  n
## 1      FALSE 23
## 2       TRUE 22</code></pre>
<p>If I don’t put the <code>wt</code>, <code>count</code> counts the number of
<em>rows</em> for which the true and predicted obesity group is the
same. But that’s not what I want here: I want the number of
<em>observations</em> totalled up, which is what the <code>wt=</code>
does. It says “use the things in the given column as weights”, which
means to total them up rather than count up the number of rows.</p>
<p>This says that 22 men were classified correctly and 23 were gotten
wrong. We can find the proportions correct and wrong:</p>
<div class="sourceCode" id="cb3409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3409-1"><a href="discriminant-analysis.html#cb3409-1"></a>tab <span class="op">%&gt;%</span></span>
<span id="cb3409-2"><a href="discriminant-analysis.html#cb3409-2"></a><span class="st">  </span><span class="kw">count</span>(<span class="dt">correct =</span> (obesity <span class="op">==</span><span class="st"> </span>class), <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3409-3"><a href="discriminant-analysis.html#cb3409-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</span></code></pre></div>
<pre><code>##   correct  n proportion
## 1   FALSE 23  0.5111111
## 2    TRUE 22  0.4888889</code></pre>
<p>and we see that 51% of men had their obesity group predicted
wrongly. This is the overall misclassification rate, which is a simple
summary of how good a job the discriminant analysis did.</p>
<p>There is a subtlety here. <code>n</code> has changed its meaning in the
middle of this calculation! In <code>tab</code>, <code>n</code> is counting
the number of obesity observed and predicted combinations, but now it
is counting the number of men classified correctly and
incorrectly. The <code>wt=n</code> uses the first <code>n</code>, but the
<code>mutate</code> line uses the <em>new</em> <code>n</code>, the result of the
<code>count</code> line here. (I think <code>count</code> used to use
<code>nn</code> for the result of the second <code>count</code>, so that you
could tell them apart, but it no longer seems to do so.)</p>
<p>I said above that the obesity groups were not equally easy to
predict. A small modification of the above will get the
misclassification rates by (true) obesity group. This is done by
putting an appropriate <code>group_by</code> in at the front, before we
do any summarizing:</p>
<div class="sourceCode" id="cb3411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3411-1"><a href="discriminant-analysis.html#cb3411-1"></a>tab <span class="op">%&gt;%</span></span>
<span id="cb3411-2"><a href="discriminant-analysis.html#cb3411-2"></a><span class="st">  </span><span class="kw">group_by</span>(obesity) <span class="op">%&gt;%</span></span>
<span id="cb3411-3"><a href="discriminant-analysis.html#cb3411-3"></a><span class="st">  </span><span class="kw">count</span>(<span class="dt">correct =</span> (obesity <span class="op">==</span><span class="st"> </span>class), <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3411-4"><a href="discriminant-analysis.html#cb3411-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</span></code></pre></div>
<pre><code>## # A tibble: 8 x 4
## # Groups:   obesity [4]
##   obesity correct     n proportion
##   &lt;chr&gt;   &lt;lgl&gt;   &lt;int&gt;      &lt;dbl&gt;
## 1 a       FALSE       5     0.417 
## 2 a       TRUE        7     0.583 
## 3 b       FALSE       5     0.357 
## 4 b       TRUE        9     0.643 
## 5 c       FALSE      10     0.909 
## 6 c       TRUE        1     0.0909
## 7 d       FALSE       3     0.375 
## 8 d       TRUE        5     0.625</code></pre>
<p>This gives the proportion wrong and correct for each (true) obesity
group. I’m going to do the one more cosmetic thing to make it easier to
read, a kind of “untidying”:</p>
<div class="sourceCode" id="cb3413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3413-1"><a href="discriminant-analysis.html#cb3413-1"></a>tab <span class="op">%&gt;%</span></span>
<span id="cb3413-2"><a href="discriminant-analysis.html#cb3413-2"></a><span class="st">  </span><span class="kw">group_by</span>(obesity) <span class="op">%&gt;%</span></span>
<span id="cb3413-3"><a href="discriminant-analysis.html#cb3413-3"></a><span class="st">  </span><span class="kw">count</span>(<span class="dt">correct =</span> (obesity <span class="op">==</span><span class="st"> </span>class), <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3413-4"><a href="discriminant-analysis.html#cb3413-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span></span>
<span id="cb3413-5"><a href="discriminant-analysis.html#cb3413-5"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>n) <span class="op">%&gt;%</span></span>
<span id="cb3413-6"><a href="discriminant-analysis.html#cb3413-6"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from=</span>correct, <span class="dt">values_from=</span>proportion)</span></code></pre></div>
<pre><code>## # A tibble: 4 x 3
## # Groups:   obesity [4]
##   obesity `FALSE` `TRUE`
##   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;
## 1 a         0.417 0.583 
## 2 b         0.357 0.643 
## 3 c         0.909 0.0909
## 4 d         0.375 0.625</code></pre>
<p>Looking down the <code>TRUE</code> column, groups A, B and D were gotten
about 60% correct (and 40% wrong), but group C is much worse. The
overall misclassification rate is made bigger by the fact that C is so
hard to predict.</p>
<p>Find out for yourself what happens if I fail to remove the <code>n</code>
column before doing the <code>pivot_wider</code>.</p>
<p>A slightly more elegant look is obtained this way, by making nicer
values than TRUE and FALSE:</p>
<div class="sourceCode" id="cb3415"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3415-1"><a href="discriminant-analysis.html#cb3415-1"></a>tab <span class="op">%&gt;%</span></span>
<span id="cb3415-2"><a href="discriminant-analysis.html#cb3415-2"></a><span class="st">  </span><span class="kw">group_by</span>(obesity) <span class="op">%&gt;%</span></span>
<span id="cb3415-3"><a href="discriminant-analysis.html#cb3415-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prediction_stat =</span> <span class="kw">ifelse</span>(obesity <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3415-4"><a href="discriminant-analysis.html#cb3415-4"></a><span class="st">  </span><span class="kw">count</span>(prediction_stat, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3415-5"><a href="discriminant-analysis.html#cb3415-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span></span>
<span id="cb3415-6"><a href="discriminant-analysis.html#cb3415-6"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>n) <span class="op">%&gt;%</span></span>
<span id="cb3415-7"><a href="discriminant-analysis.html#cb3415-7"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from=</span>prediction_stat, <span class="dt">values_from=</span>proportion)</span></code></pre></div>
<pre><code>## # A tibble: 4 x 3
## # Groups:   obesity [4]
##   obesity correct wrong
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;
## 1 a        0.583  0.417
## 2 b        0.643  0.357
## 3 c        0.0909 0.909
## 4 d        0.625  0.375</code></pre>
<ol start="7" style="list-style-type: lower-alpha">
<li>Do your conclusions from (<a href="#part:plot">here</a>) and
(<a href="#part:table">here</a>) appear to be consistent?</li>
</ol>
<p>Solution</p>
<p>On the plot of (<a href="#part:plot">here</a>), we said that there was a
lot of scatter, but that groups <code>a</code>, <code>b</code> and
<code>d</code> tended to be found at the top, right and left
respectively of the plot. That suggests that these three
groups should be somewhat predictable. The <code>c</code>’s, on
the other hand, were all over the place on the plot, and
were mostly predicted wrong.</p>
<p>The idea is that the stories you pull from the plot and the
predictions should be more or less consistent. There are
several ways you might say that: another approach is to say
that the observations are all over the place on the plot,
and the predictions are all bad. This is not as insightful
as my comments above, but if that’s what the plot told you,
that’s what the predictions would seem to be saying as
well. (Or even, the predictions are not so bad compared to
the apparently random pattern on the plot, if that’s what
you saw. There are different ways to say something more or
less sensible.)</p>
</div>
<div id="understanding-a-manova" class="section level2" number="26.3">
<h2><span class="header-section-number">26.3</span> Understanding a MANOVA</h2>
<p>One use of discriminant analysis is to
understand the results of a MANOVA. This question is a followup to a
previous MANOVA that we did, the one with two variables <code>y1</code>
and <code>y2</code> and three groups <code>a</code> through <code>c</code>. The
data were in <a href="http://www.utsc.utoronto.ca/~butler/d29/simple-manova.txt">link</a>.</p>
<ol style="list-style-type: lower-alpha">
<li>Read the data in again and run the MANOVA that you did
before.</li>
</ol>
<p>Solution</p>
<p>This is an exact repeat of what you did before:</p>
<div class="sourceCode" id="cb3417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3417-1"><a href="discriminant-analysis.html#cb3417-1"></a>my_url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/nxskok/datafiles/master/simple-manova.txt&quot;</span></span>
<span id="cb3417-2"><a href="discriminant-analysis.html#cb3417-2"></a>simple &lt;-<span class="st"> </span><span class="kw">read_delim</span>(my_url, <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## 
## ── Column specification ──────────────────────────────────────────────────────────────────
## cols(
##   group = col_character(),
##   y1 = col_double(),
##   y2 = col_double()
## )</code></pre>
<div class="sourceCode" id="cb3419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3419-1"><a href="discriminant-analysis.html#cb3419-1"></a>simple</span></code></pre></div>
<pre><code>## # A tibble: 12 x 3
##    group    y1    y2
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 a         2     3
##  2 a         3     4
##  3 a         5     4
##  4 a         2     5
##  5 b         4     8
##  6 b         5     6
##  7 b         5     7
##  8 c         7     6
##  9 c         8     7
## 10 c        10     8
## 11 c         9     5
## 12 c         7     6</code></pre>
<div class="sourceCode" id="cb3421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3421-1"><a href="discriminant-analysis.html#cb3421-1"></a>response &lt;-<span class="st"> </span><span class="kw">with</span>(simple, <span class="kw">cbind</span>(y1, y2))</span>
<span id="cb3421-2"><a href="discriminant-analysis.html#cb3421-2"></a>simple<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">manova</span>(response <span class="op">~</span><span class="st"> </span>group, <span class="dt">data =</span> simple)</span>
<span id="cb3421-3"><a href="discriminant-analysis.html#cb3421-3"></a><span class="kw">summary</span>(simple<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>##           Df Pillai approx F num Df den Df    Pr(&gt;F)    
## group      2 1.3534   9.4196      4     18 0.0002735 ***
## Residuals  9                                            
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>This P-value is small, so there is some way in which some of the
groups differ on some of the variables.
<label for="tufte-mn-221" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-221" class="margin-toggle"><span class="marginnote">That sounds like the ultimate in evasiveness!</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Run a discriminant analysis “predicting” group from the
two response variables. Display the output.</li>
</ol>
<p>Solution</p>
<p>This:</p>
<div class="sourceCode" id="cb3423"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3423-1"><a href="discriminant-analysis.html#cb3423-1"></a>simple<span class="fl">.4</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(group <span class="op">~</span><span class="st"> </span>y1 <span class="op">+</span><span class="st"> </span>y2, <span class="dt">data =</span> simple)</span>
<span id="cb3423-2"><a href="discriminant-analysis.html#cb3423-2"></a>simple<span class="fl">.4</span></span></code></pre></div>
<pre><code>## Call:
## lda(group ~ y1 + y2, data = simple)
## 
## Prior probabilities of groups:
##         a         b         c 
## 0.3333333 0.2500000 0.4166667 
## 
## Group means:
##         y1  y2
## a 3.000000 4.0
## b 4.666667 7.0
## c 8.200000 6.4
## 
## Coefficients of linear discriminants:
##          LD1        LD2
## y1 0.7193766  0.4060972
## y2 0.3611104 -0.9319337
## 
## Proportion of trace:
##    LD1    LD2 
## 0.8331 0.1669</code></pre>
<p>Note that this is the other way around from MANOVA: here, we are
“predicting the group” from the response variables, in the same
manner as one of the flavours of logistic regression:
“what makes the groups different, in terms of those response variables?”.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li><a name="part:output">*</a> In the output from the discriminant analysis,
why are there exactly two linear discriminants <code>LD1</code> and
<code>LD2</code>?</li>
</ol>
<p>Solution</p>
<p>There are two linear discriminants because there are 3 groups and two
variables, so there are the smaller of <span class="math inline">\(3-1\)</span> and 2 discriminants.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li><a name="part:svd">*</a> From the output, how would you say that the
first linear discriminant <code>LD1</code> compares in importance to the
second one <code>LD2</code>: much more important, more important, equally
important, less important, much less important? Explain briefly.</li>
</ol>
<p>Solution</p>
<p>Look at the <code>Proportion of trace</code> at the bottom of the output.
The first number is much bigger than the second, so the first linear
discriminant is much more important than the second. (I care about
your reason; you can say it’s “more important” rather than
“much more important” and I’m good with that.)</p>
<ol start="5" style="list-style-type: lower-alpha">
<li>Obtain a plot of the
discriminant scores.</li>
</ol>
<p>Solution</p>
<p>This was the old-fashioned way:</p>
<div class="sourceCode" id="cb3425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3425-1"><a href="discriminant-analysis.html#cb3425-1"></a><span class="kw">plot</span>(simple<span class="fl">.4</span>)</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1715-1.png" width="672"  /></p>
<p>It needs cajoling to produce colours, but we can do better. The first
thing is to obtain the predictions:</p>
<div class="sourceCode" id="cb3426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3426-1"><a href="discriminant-analysis.html#cb3426-1"></a>simple.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(simple<span class="fl">.4</span>)</span></code></pre></div>
<p>Then we make a data frame out of the discriminant scores and the true
groups, using <code>cbind</code>:</p>
<div class="sourceCode" id="cb3427"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3427-1"><a href="discriminant-analysis.html#cb3427-1"></a>d &lt;-<span class="st"> </span><span class="kw">cbind</span>(simple, simple.pred)</span>
<span id="cb3427-2"><a href="discriminant-analysis.html#cb3427-2"></a><span class="kw">head</span>(d)</span></code></pre></div>
<pre><code>##   group y1 y2 class posterior.a  posterior.b  posterior.c      x.LD1      x.LD2
## 1     a  2  3     a 0.999836110 0.0001636933 1.964310e-07 -3.5708196  1.1076359
## 2     a  3  4     a 0.994129686 0.0058400248 3.028912e-05 -2.4903326  0.5817994
## 3     a  5  4     a 0.953416498 0.0267238544 1.985965e-02 -1.0515795  1.3939939
## 4     a  2  5     a 0.957685668 0.0423077129 6.618865e-06 -2.8485988 -0.7562315
## 5     b  4  8     b 0.001068057 0.9978789644 1.052978e-03 -0.3265145 -2.7398380
## 6     b  5  6     b 0.107572389 0.8136017106 7.882590e-02 -0.3293587 -0.4698735</code></pre>
<p>or like this, for fun:
<label for="tufte-mn-222" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-222" class="margin-toggle"><span class="marginnote">For suitable definitions of fun.</span></p>
<div class="sourceCode" id="cb3429"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3429-1"><a href="discriminant-analysis.html#cb3429-1"></a>ld &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(simple.pred<span class="op">$</span>x)</span>
<span id="cb3429-2"><a href="discriminant-analysis.html#cb3429-2"></a>post &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(simple.pred<span class="op">$</span>posterior)</span>
<span id="cb3429-3"><a href="discriminant-analysis.html#cb3429-3"></a>dd &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(simple, <span class="dt">class =</span> simple.pred<span class="op">$</span>class, ld, post)</span>
<span id="cb3429-4"><a href="discriminant-analysis.html#cb3429-4"></a>dd</span></code></pre></div>
<pre><code>## # A tibble: 12 x 9
##    group    y1    y2 class     LD1    LD2             a        b           c
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;
##  1 a         2     3 a     -3.57    1.11  1.00          0.000164 0.000000196
##  2 a         3     4 a     -2.49    0.582 0.994         0.00584  0.0000303  
##  3 a         5     4 a     -1.05    1.39  0.953         0.0267   0.0199     
##  4 a         2     5 a     -2.85   -0.756 0.958         0.0423   0.00000662 
##  5 b         4     8 b     -0.327  -2.74  0.00107       0.998    0.00105    
##  6 b         5     6 b     -0.329  -0.470 0.108         0.814    0.0788     
##  7 b         5     7 b      0.0318 -1.40  0.00772       0.959    0.0335     
##  8 c         7     6 c      1.11    0.342 0.00186       0.0671   0.931      
##  9 c         8     7 c      2.19   -0.184 0.0000127     0.0164   0.984      
## 10 c        10     8 c      3.99   -0.303 0.00000000317 0.000322 1.00       
## 11 c         9     5 c      2.19    2.09  0.0000173     0.000181 1.00       
## 12 c         7     6 c      1.11    0.342 0.00186       0.0671   0.931</code></pre>
<p>After that, we plot the first one against the second one, colouring by
true groups:</p>
<div class="sourceCode" id="cb3431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3431-1"><a href="discriminant-analysis.html#cb3431-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> x.LD1, <span class="dt">y =</span> x.LD2, <span class="dt">colour =</span> group)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1719-1.png" width="672"  /></p>
<p>I wanted to compare this plot with the original plot of <code>y1</code>
vs. <code>y2</code>, coloured by groups:</p>
<div class="sourceCode" id="cb3432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3432-1"><a href="discriminant-analysis.html#cb3432-1"></a><span class="kw">ggplot</span>(simple, <span class="kw">aes</span>(<span class="dt">x =</span> y1, <span class="dt">y =</span> y2, <span class="dt">colour =</span> group)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1720-1.png" width="672"  /></p>
<p>The difference between this plot and the one of <code>LD1</code> vs.<br />
<code>LD2</code> is that things have been rotated a bit so that most of
the separation of groups is done by <code>LD1</code>. This is reflected in
the fact that <code>LD1</code> is quite a bit more important than
<code>LD2</code>: the latter doesn’t help much in separating the groups.</p>
<p>With that in mind, we could also plot just <code>LD1</code>, presumably
against groups via boxplot:</p>
<div class="sourceCode" id="cb3433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3433-1"><a href="discriminant-analysis.html#cb3433-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> group, <span class="dt">y =</span> x.LD1)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_boxplot</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1721-1.png" width="672"  /></p>
<p>This shows that LD1 does a pretty fine job of separating the groups,
and <code>LD2</code> doesn’t really have much to add to the picture.</p>
<ol start="6" style="list-style-type: lower-alpha">
<li>Describe briefly how <code>LD1</code> and/or <code>LD2</code>
separate the groups. Does your picture confirm the relative importance
of <code>LD1</code> and <code>LD2</code> that you found back in part (<a href="#part:svd">here</a>)? Explain briefly.</li>
</ol>
<p>Solution</p>
<p><code>LD1</code> separates the groups left to right: group <code>a</code> is
low on <code>LD1</code>, <code>b</code> is in the middle and <code>c</code> is
high on <code>LD1</code>. (There is no intermingling of the groups on
<code>LD1</code>, so it separates the groups perfectly.)</p>
<p>As for <code>LD2</code>, all it does (possibly) is to distinguish
<code>b</code> (low) from <code>a</code> and <code>c</code> (high). Or you can,
just as reasonably, take the view that it doesn’t really separate
any of the groups.</p>
<p>Back in part (<a href="#part:svd">here</a>), you said (I hope) that <code>LD1</code>
was (very) important compared to <code>LD2</code>. This shows up here in
that <code>LD1</code> does a very good job of distinguishing the groups,
while <code>LD2</code> does a poor to non-existent job of separating any
groups. (If you didn’t
say that before, here is an invitation to reconsider what you
<em>did</em> say there.)</p>
<ol start="7" style="list-style-type: lower-alpha">
<li>What makes group <code>a</code> have a low score on <code>LD1</code>?
There are two steps that you need to make: consider the means of group
<code>a</code> on variables <code>y1</code> and <code>y2</code> and how they
compare to the other groups, and consider how
<code>y1</code> and <code>y2</code> play into the score on <code>LD1</code>.</li>
</ol>
<p>Solution</p>
<p>The information you need is in the big output.</p>
<p>The means of <code>y1</code> and <code>y2</code> for group <code>a</code> are 3
and 4 respectively, which are the lowest of all the groups. That’s
the first thing.</p>
<p>The second thing is the coefficients of
<code>LD1</code> in terms of <code>y1</code> and <code>y2</code>, which are both
<em>positive</em>. That means, for any observation, if its <code>y1</code>
and <code>y2</code> values are <em>large</em>, that observation’s score on
<code>LD1</code> will be large as well. Conversely, if its values are
<em>small</em>, as the ones in group <code>a</code> are, its score on
<code>LD1</code> will be small.</p>
<p>You need these two things.</p>
<p>This explains why the group <code>a</code> observations are on the left
of the plot. It also explains why the group <code>c</code> observations
are on the right: they are <em>large</em> on both <code>y1</code> and
<code>y2</code>, and so large on <code>LD1</code>.</p>
<p>What about <code>LD2</code>? This is a little more confusing (and thus I
didn’t ask you about that). Its “coefficients of linear discriminant”
are positive on <code>y1</code> and negative on
<code>y2</code>, with the latter being bigger in size. Group <code>b</code>
is about average on <code>y1</code> and distinctly <em>high</em> on
<code>y2</code>; the second of these coupled with the negative
coefficient on <code>y2</code> means that the <code>LD2</code> score for
observations in group <code>b</code> will be <em>negative</em>.</p>
<p>For <code>LD2</code>, group <code>a</code> has a low mean on both variables
and group <code>c</code> has a high mean, so for both groups there is a
kind of cancelling-out happening, and neither group <code>a</code> nor
group <code>c</code> will be especially remarkable on <code>LD2</code>.</p>
<ol start="8" style="list-style-type: lower-alpha">
<li>Obtain predictions for the group memberships of each
observation, and make a table of the actual group memberships against
the predicted ones. How many of the observations were wrongly classified?</li>
</ol>
<p>Solution</p>
<p>Use the
<code>simple.pred</code> that you got earlier. This is the
<code>table</code> way:</p>
<div class="sourceCode" id="cb3434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3434-1"><a href="discriminant-analysis.html#cb3434-1"></a><span class="kw">with</span>(d, <span class="kw">table</span>(<span class="dt">obs =</span> group, <span class="dt">pred =</span> class))</span></code></pre></div>
<pre><code>##    pred
## obs a b c
##   a 4 0 0
##   b 0 3 0
##   c 0 0 5</code></pre>
<p>Every single one of the 12 observations has been classified into its
correct group. (There is nothing off the diagonal of this table.)
The alternative to <code>table</code> is the <code>tidyverse</code> way:</p>
<div class="sourceCode" id="cb3436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3436-1"><a href="discriminant-analysis.html#cb3436-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(group, class)</span></code></pre></div>
<pre><code>##   group class n
## 1     a     a 4
## 2     b     b 3
## 3     c     c 5</code></pre>
<p>or</p>
<div class="sourceCode" id="cb3438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3438-1"><a href="discriminant-analysis.html#cb3438-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3438-2"><a href="discriminant-analysis.html#cb3438-2"></a><span class="st">  </span><span class="kw">count</span>(group, class) <span class="op">%&gt;%</span></span>
<span id="cb3438-3"><a href="discriminant-analysis.html#cb3438-3"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from=</span>class, <span class="dt">values_from=</span>n, <span class="dt">values_fill =</span> <span class="kw">list</span>(<span class="dt">n=</span><span class="dv">0</span>))</span></code></pre></div>
<pre><code>## # A tibble: 3 x 4
##   group     a     b     c
##   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1 a         4     0     0
## 2 b         0     3     0
## 3 c         0     0     5</code></pre>
<p>if you want something that looks like a frequency table.
All the <code>a</code>s got classified as <code>a</code>, and so on.
That’s the end of what I asked you to do, but as ever I wanted to
press on. The next question to ask after getting the predicted groups
is “what are the posterior probabilities of being in each group for each observation”:
that is, not just which group do I think it
belongs in, but how sure am I about that call? The posterior
probabilities in my <code>d</code> start with <code>posterior</code>. These
have a ton of decimal places which I like to round off first before I
display them, eg. to 3 decimals here:</p>
<div class="sourceCode" id="cb3440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3440-1"><a href="discriminant-analysis.html#cb3440-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3440-2"><a href="discriminant-analysis.html#cb3440-2"></a><span class="st">  </span><span class="kw">select</span>(y1, y2, group, class, <span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3440-3"><a href="discriminant-analysis.html#cb3440-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="kw">across</span>(<span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>), <span class="op">~</span><span class="st"> </span><span class="kw">round</span>(., <span class="dv">3</span>)))</span></code></pre></div>
<pre><code>##    y1 y2 group class posterior.a posterior.b posterior.c
## 1   2  3     a     a       1.000       0.000       0.000
## 2   3  4     a     a       0.994       0.006       0.000
## 3   5  4     a     a       0.953       0.027       0.020
## 4   2  5     a     a       0.958       0.042       0.000
## 5   4  8     b     b       0.001       0.998       0.001
## 6   5  6     b     b       0.108       0.814       0.079
## 7   5  7     b     b       0.008       0.959       0.034
## 8   7  6     c     c       0.002       0.067       0.931
## 9   8  7     c     c       0.000       0.016       0.984
## 10 10  8     c     c       0.000       0.000       1.000
## 11  9  5     c     c       0.000       0.000       1.000
## 12  7  6     c     c       0.002       0.067       0.931</code></pre>
<p>You see that the posterior probability of an observation being in the
group it actually <em>was</em> in is close to 1 all the way down. The
only one with any doubt at all is observation #6, which is actually
in group <code>b</code>, but has “only” probability 0.814 of being a
<code>b</code> based on its <code>y1</code> and <code>y2</code> values. What else
could it be? Well, it’s about equally split between being <code>a</code>
and <code>c</code>. Let me see if I can display this observation on the
plot in a different way. First I need to make a new column picking out
observation 6, and then I use this new variable as the <code>size</code>
of the point I plot:</p>
<div class="sourceCode" id="cb3442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3442-1"><a href="discriminant-analysis.html#cb3442-1"></a>simple <span class="op">%&gt;%</span></span>
<span id="cb3442-2"><a href="discriminant-analysis.html#cb3442-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">is6 =</span> (<span class="kw">row_number</span>() <span class="op">==</span><span class="st"> </span><span class="dv">6</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3442-3"><a href="discriminant-analysis.html#cb3442-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> y1, <span class="dt">y =</span> y2, <span class="dt">colour =</span> group, <span class="dt">size =</span> is6)) <span class="op">+</span></span>
<span id="cb3442-4"><a href="discriminant-analysis.html#cb3442-4"></a><span class="st">  </span><span class="kw">geom_point</span>()</span></code></pre></div>
<pre><code>## Warning: Using size for a discrete variable is not advised.</code></pre>
<p><img src="pasias_files/figure-html/unnamed-chunk-1726-1.png" width="672"  /></p>
<p>That makes it stand out.
As the legend indicates, observation #6 is plotted as a big circle,
with the rest being plotted as small circles as usual. Since observation #6
is in group <code>b</code>, it appears as a big green circle. What makes it
least like a <code>b</code>? Well, it has the smallest <code>y2</code> value
of any of the <code>b</code>’s (which makes it most like an <code>a</code> of
any of the <code>b</code>’s), and it has the largest <code>y1</code> value (which makes it
most like a <code>c</code> of any of the <code>b</code>’s). But still, it’s nearer the
greens than anything else, so it’s still more like a <code>b</code> than
it is like any of the other groups.</p>
</div>
<div id="what-distinguishes-people-who-do-different-jobs" class="section level2" number="26.4">
<h2><span class="header-section-number">26.4</span> What distinguishes people who do different jobs?</h2>
<p>244
<label for="tufte-mn-223" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-223" class="margin-toggle"><span class="marginnote">Grammatically, I am supposed to write this as <em>two hundred and forty-four</em> in words, since I am not supposed to start a sentence with a number. But, I say, deal with it. Or, I suppose, <em>there are 244 people who work…</em>.</span> people work at a
certain company.
They each have one of three jobs: customer service, mechanic,
dispatcher. In the data set, these are labelled 1, 2 and 3
respectively. In addition, they each are rated on scales called
<code>outdoor</code>, <code>social</code> and <code>conservative</code>. Do people
with different jobs tend to have different scores on these scales, or,
to put it another way, if you knew a person’s scores on
<code>outdoor</code>, <code>social</code> and <code>conservative</code>, could you
say something about what kind of job they were likely to hold? The
data are in <a href="http://www.utsc.utoronto.ca/~butler/d29/jobs.txt">link</a>.</p>
<ol style="list-style-type: lower-alpha">
<li>Read in the data and display some of it.</li>
</ol>
<p>Solution</p>
<p>The usual. This one is aligned columns.
I’m using a “temporary” name for my read-in data
frame, since I’m going to create the proper one in a moment.</p>
<div class="sourceCode" id="cb3444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3444-1"><a href="discriminant-analysis.html#cb3444-1"></a>my_url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/nxskok/datafiles/master/jobs.txt&quot;</span></span>
<span id="cb3444-2"><a href="discriminant-analysis.html#cb3444-2"></a>jobs0 &lt;-<span class="st"> </span><span class="kw">read_table</span>(my_url)</span></code></pre></div>
<pre><code>## 
## ── Column specification ──────────────────────────────────────────────────────────────────
## cols(
##   outdoor = col_double(),
##   social = col_double(),
##   conservative = col_double(),
##   job = col_double(),
##   id = col_double()
## )</code></pre>
<div class="sourceCode" id="cb3446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3446-1"><a href="discriminant-analysis.html#cb3446-1"></a>jobs0</span></code></pre></div>
<pre><code>## # A tibble: 244 x 5
##    outdoor social conservative   job    id
##      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1      10     22            5     1     1
##  2      14     17            6     1     2
##  3      19     33            7     1     3
##  4      14     29           12     1     4
##  5      14     25            7     1     5
##  6      20     25           12     1     6
##  7       6     18            4     1     7
##  8      13     27            7     1     8
##  9      18     31            9     1     9
## 10      16     35           13     1    10
## # … with 234 more rows</code></pre>
<p>We got all that was promised, plus a label <code>id</code> for each
employee, which we will from here on ignore.
<label for="tufte-mn-224" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-224" class="margin-toggle"><span class="marginnote">Until much later.</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Note the types of each of the variables, and create any new
variables that you need to.</li>
</ol>
<p>Solution</p>
<p>These are all <code>int</code> or whole numbers. But, the job ought
to be a <code>factor</code>: the labels 1, 2 and 3 have no meaning
as such, they just label the three different jobs. (I gave you a
hint of this above.) So we need to turn <code>job</code> into a
factor.
I think the best way to do that is via <code>mutate</code>, and then
we save the new data frame into one called <code>jobs</code> that we
actually use for the analysis below:</p>
<div class="sourceCode" id="cb3448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3448-1"><a href="discriminant-analysis.html#cb3448-1"></a>job_labels &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;custserv&quot;</span>, <span class="st">&quot;mechanic&quot;</span>, <span class="st">&quot;dispatcher&quot;</span>)</span>
<span id="cb3448-2"><a href="discriminant-analysis.html#cb3448-2"></a>jobs &lt;-<span class="st"> </span>jobs0 <span class="op">%&gt;%</span></span>
<span id="cb3448-3"><a href="discriminant-analysis.html#cb3448-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">job =</span> <span class="kw">factor</span>(job, <span class="dt">labels =</span> job_labels))</span></code></pre></div>
<p>I lived on the edge and saved my factor <code>job</code> into a variable
with the same name as the numeric one. I should check that I now have
the right thing:</p>
<div class="sourceCode" id="cb3449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3449-1"><a href="discriminant-analysis.html#cb3449-1"></a>jobs</span></code></pre></div>
<pre><code>## # A tibble: 244 x 5
##    outdoor social conservative job         id
##      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;
##  1      10     22            5 custserv     1
##  2      14     17            6 custserv     2
##  3      19     33            7 custserv     3
##  4      14     29           12 custserv     4
##  5      14     25            7 custserv     5
##  6      20     25           12 custserv     6
##  7       6     18            4 custserv     7
##  8      13     27            7 custserv     8
##  9      18     31            9 custserv     9
## 10      16     35           13 custserv    10
## # … with 234 more rows</code></pre>
<p>I like this better because you see the actual factor levels rather
than the underlying numeric values by which they are stored.</p>
<p>All is good here. If you forget the <code>labels</code> thing, you’ll get
a factor, but its levels will be 1, 2, and 3, and you will have to
remember which jobs they go with. I’m a fan of giving factors named
levels, so that you can remember what stands for what.
<label for="tufte-mn-225" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-225" class="margin-toggle"><span class="marginnote">When you’re <em>recording</em> the data, you may find it convenient to use short codes to represent the possibly long factor levels, but in that case you should also use a <em>codebook</em> so that you know what the codes represent. When I read the data into R, I would create a factor with named levels, like I did here, if I don’t already have one.</span></p>
<p>Extra: another way of doing this is to make a lookup table, that is, a little table that shows which job goes with which number:</p>
<div class="sourceCode" id="cb3451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3451-1"><a href="discriminant-analysis.html#cb3451-1"></a>lookup_tab &lt;-<span class="st"> </span><span class="kw">tribble</span>(</span>
<span id="cb3451-2"><a href="discriminant-analysis.html#cb3451-2"></a>  <span class="op">~</span>job, <span class="op">~</span>jobname,</span>
<span id="cb3451-3"><a href="discriminant-analysis.html#cb3451-3"></a>  <span class="dv">1</span>, <span class="st">&quot;custserv&quot;</span>,</span>
<span id="cb3451-4"><a href="discriminant-analysis.html#cb3451-4"></a>  <span class="dv">2</span>, <span class="st">&quot;mechanic&quot;</span>,</span>
<span id="cb3451-5"><a href="discriminant-analysis.html#cb3451-5"></a>  <span class="dv">3</span>, <span class="st">&quot;dispatcher&quot;</span></span>
<span id="cb3451-6"><a href="discriminant-analysis.html#cb3451-6"></a>)</span>
<span id="cb3451-7"><a href="discriminant-analysis.html#cb3451-7"></a>lookup_tab</span></code></pre></div>
<pre><code>## # A tibble: 3 x 2
##     job jobname   
##   &lt;dbl&gt; &lt;chr&gt;     
## 1     1 custserv  
## 2     2 mechanic  
## 3     3 dispatcher</code></pre>
<p>I carefully put the numbers in a column called <code>job</code> because I want to match these with the column called <code>job</code> in <code>jobs0</code>:</p>
<div class="sourceCode" id="cb3453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3453-1"><a href="discriminant-analysis.html#cb3453-1"></a>jobs0 <span class="op">%&gt;%</span></span>
<span id="cb3453-2"><a href="discriminant-analysis.html#cb3453-2"></a><span class="st">  </span><span class="kw">left_join</span>(lookup_tab) <span class="op">%&gt;%</span></span>
<span id="cb3453-3"><a href="discriminant-analysis.html#cb3453-3"></a><span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">20</span>)</span></code></pre></div>
<pre><code>## Joining, by = &quot;job&quot;</code></pre>
<pre><code>## # A tibble: 20 x 6
##    outdoor social conservative   job    id jobname   
##      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     
##  1      15     10           13     3    31 dispatcher
##  2       1     30            6     1    23 custserv  
##  3      13     15           18     3    21 dispatcher
##  4      15     13           13     3    65 dispatcher
##  5      12     25            8     1    42 custserv  
##  6      16     22            2     1    59 custserv  
##  7      25     16           12     3    46 dispatcher
##  8      13     22           12     2     9 mechanic  
##  9      10      8           16     3    25 dispatcher
## 10      20     20            9     2    45 mechanic  
## 11      16     28            6     1    41 custserv  
## 12      15     17            8     2    77 mechanic  
## 13      23     20           16     2    63 mechanic  
## 14      12     26            9     1    27 custserv  
## 15      10     24           12     1    18 custserv  
## 16      18     11           19     3    47 dispatcher
## 17      21     15           10     2     2 mechanic  
## 18      21     29           12     2    81 mechanic  
## 19      15     23           10     1    19 custserv  
## 20      13     20           10     2    22 mechanic</code></pre>
<p>You see that each row has the <em>name</em> of the job that employee has, in the column <code>jobname</code>, because the job <code>id</code> was looked up in our lookup table. (I displayed some random rows so you could see that it worked.)</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Run a multivariate analysis of variance to convince yourself
that there are some differences in scale scores among the jobs.</li>
</ol>
<p>Solution</p>
<p>You know how to do this, right? This one is the easy way:</p>
<div class="sourceCode" id="cb3456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3456-1"><a href="discriminant-analysis.html#cb3456-1"></a>response &lt;-<span class="st"> </span><span class="kw">with</span>(jobs, <span class="kw">cbind</span>(social, outdoor, conservative))</span>
<span id="cb3456-2"><a href="discriminant-analysis.html#cb3456-2"></a>response<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">manova</span>(response <span class="op">~</span><span class="st"> </span>job, <span class="dt">data =</span> jobs)</span>
<span id="cb3456-3"><a href="discriminant-analysis.html#cb3456-3"></a><span class="kw">summary</span>(response<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>##            Df  Pillai approx F num Df den Df    Pr(&gt;F)    
## job         2 0.76207   49.248      6    480 &lt; 2.2e-16 ***
## Residuals 241                                             
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Or you can use <code>Manova</code>. That is mostly for practice here,
since there is no reason to make things difficult for yourself:</p>
<div class="sourceCode" id="cb3458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3458-1"><a href="discriminant-analysis.html#cb3458-1"></a><span class="kw">library</span>(car)</span>
<span id="cb3458-2"><a href="discriminant-analysis.html#cb3458-2"></a>response<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(response <span class="op">~</span><span class="st"> </span>job, <span class="dt">data =</span> jobs)</span>
<span id="cb3458-3"><a href="discriminant-analysis.html#cb3458-3"></a><span class="kw">Manova</span>(response<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## 
## Type II MANOVA Tests: Pillai test statistic
##     Df test stat approx F num Df den Df    Pr(&gt;F)    
## job  2   0.76207   49.248      6    480 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Oh yes, there are differences (on some or all of the variables, for
some or all of the groups). So we need something like discriminant
analysis to understand the differences.</p>
<p>This, and the <code>lda</code> below, actually works perfectly well if you use the
original (integer) job, but then you have to remember which job number
is which.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>Run a discriminant analysis and display the output.</li>
</ol>
<p>Solution</p>
<p>Now <code>job</code>
is the “response”:</p>
<div class="sourceCode" id="cb3460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3460-1"><a href="discriminant-analysis.html#cb3460-1"></a>job<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(job <span class="op">~</span><span class="st"> </span>social <span class="op">+</span><span class="st"> </span>outdoor <span class="op">+</span><span class="st"> </span>conservative, <span class="dt">data =</span> jobs)</span>
<span id="cb3460-2"><a href="discriminant-analysis.html#cb3460-2"></a>job<span class="fl">.1</span></span></code></pre></div>
<pre><code>## Call:
## lda(job ~ social + outdoor + conservative, data = jobs)
## 
## Prior probabilities of groups:
##   custserv   mechanic dispatcher 
##  0.3483607  0.3811475  0.2704918 
## 
## Group means:
##              social  outdoor conservative
## custserv   24.22353 12.51765     9.023529
## mechanic   21.13978 18.53763    10.139785
## dispatcher 15.45455 15.57576    13.242424
## 
## Coefficients of linear discriminants:
##                      LD1         LD2
## social       -0.19427415 -0.04978105
## outdoor       0.09198065 -0.22501431
## conservative  0.15499199  0.08734288
## 
## Proportion of trace:
##    LD1    LD2 
## 0.7712 0.2288</code></pre>
<ol start="5" style="list-style-type: lower-alpha">
<li>Which is the more important, <code>LD1</code> or <code>LD2</code>? How
much more important? Justify your answer briefly.</li>
</ol>
<p>Solution</p>
<p>Look at the “proportion of trace” at the bottom. The value for
<code>LD1</code> is quite a bit higher, so <code>LD1</code> is quite a
bit more important when it comes to separating the groups.
<code>LD2</code> is, as I said, less important, but is not
completely worthless, so it will be worth taking a look at it.</p>
<ol start="6" style="list-style-type: lower-alpha">
<li>Describe what values for an individual on the scales will make
each of <code>LD1</code> and <code>LD2</code> high.</li>
</ol>
<p>Solution</p>
<p>This is a two-parter: decide whether each scale makes a
positive, negative or zero contribution to the linear
discriminant (looking at the “coefficients of linear discriminants”),
and then translate that into what would make
each <code>LD</code> high. Let’s start with <code>LD1</code>:</p>
<p>Its coefficients on the three scales are respectively negative
(<span class="math inline">\(-0.19\)</span>), zero (0.09; my call) and positive (0.15). Where you draw the
line is up to you: if you want to say that <code>outdoor</code>’s
contribution is positive, go ahead. This means that <code>LD1</code>
will be high if <code>social</code> is <em>low</em> and if
<code>conservative</code> is <em>high</em>. (If you thought that
<code>outdoor</code>’s coefficient was positive rather than zero, if
<code>outdoor</code> is high as well.)</p>
<p>Now for <code>LD2</code>: I’m going to call <code>outdoor</code>’s
coefficient of <span class="math inline">\(-0.22\)</span> negative and the other two zero, so that
<code>LD2</code> is high if <code>outdoor</code> is <em>low</em>. Again,
if you made a different judgement call, adapt your answer accordingly.</p>
<ol start="7" style="list-style-type: lower-alpha">
<li>The first group of employees, customer service, have the
highest mean on <code>social</code> and the lowest mean on both of the
other two scales. Would you expect the customer service employees to
score high or low on <code>LD1</code>? What about <code>LD2</code>?</li>
</ol>
<p>Solution</p>
<p>In the light of what we said in the previous part, the customer
service employees, who are high on <code>social</code> and low on
<code>conservative</code>, should be <em>low</em> (negative) on
<code>LD1</code>, since both of these means are pointing that way.
As I called it, the only thing that matters to <code>LD2</code> is
<code>outdoor</code>, which is <em>low</em> for the customer service
employees, and thus <code>LD2</code> for them will be <em>high</em>
(negative coefficient).</p>
<ol start="8" style="list-style-type: lower-alpha">
<li>Plot your discriminant scores (which you will have to obtain
first), and see if you were right about the customer service
employees in terms of <code>LD1</code> and <code>LD2</code>. The job names
are rather long, and there are a lot of individuals, so it is
probably best to plot the scores as coloured circles with a legend
saying which colour goes with which job (rather than labelling each
individual with the job they have).</li>
</ol>
<p>Solution</p>
<p>Predictions first, then make a data frame combining the predictions with the original data:</p>
<div class="sourceCode" id="cb3462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3462-1"><a href="discriminant-analysis.html#cb3462-1"></a>p &lt;-<span class="st"> </span><span class="kw">predict</span>(job<span class="fl">.1</span>)</span>
<span id="cb3462-2"><a href="discriminant-analysis.html#cb3462-2"></a>d &lt;-<span class="st"> </span><span class="kw">cbind</span>(jobs, p)</span>
<span id="cb3462-3"><a href="discriminant-analysis.html#cb3462-3"></a><span class="kw">head</span>(d)</span></code></pre></div>
<pre><code>##   outdoor social conservative      job id    class posterior.custserv posterior.mechanic
## 1      10     22            5 custserv  1 custserv          0.9037622         0.08894785
## 2      14     17            6 custserv  2 mechanic          0.3677743         0.48897890
## 3      19     33            7 custserv  3 custserv          0.7302117         0.26946971
## 4      14     29           12 custserv  4 custserv          0.8100756         0.18217319
## 5      14     25            7 custserv  5 custserv          0.7677607         0.22505382
## 6      20     25           12 custserv  6 mechanic          0.1682521         0.78482488
##   posterior.dispatcher      x.LD1       x.LD2
## 1         0.0072899882 -1.6423155  0.71477348
## 2         0.1432467601 -0.1480302  0.15096436
## 3         0.0003186265 -2.6415213 -1.68326115
## 4         0.0077512155 -1.5493681  0.07764901
## 5         0.0071854904 -1.5472314 -0.15994117
## 6         0.0469230463 -0.2203876 -1.07331266</code></pre>
<p>Following my suggestion, plot these the standard way with
<code>colour</code> distinguishing the jobs:</p>
<div class="sourceCode" id="cb3464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3464-1"><a href="discriminant-analysis.html#cb3464-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> x.LD1, <span class="dt">y =</span> x.LD2, <span class="dt">colour =</span> job)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1734-1.png" width="672"  /></p>
<p>I was mostly right about the customer service people: small
<code>LD1</code> definitely, large <code>LD2</code> kinda. I wasn’t more right
because the group means don’t tell the whole story: evidently, the
customer service people vary quite a bit on <code>outdoor</code>, so the
red dots are all over the left side of the plot.</p>
<p>There is quite a bit of intermingling of the three employee groups on
the plot, but the point of the MANOVA is that the groups are (way)
more separated than you’d expect by chance, that is if the employees
were just randomly scattered across the plot.</p>
<p>To think back to that <code>trace</code> thing: here, it seems that
<code>LD1</code> mainly separates customer service (left) from dispatchers
(right); the mechanics are all over the place on <code>LD1</code>, but
they tend to be low on <code>LD2</code>. So <code>LD2</code> <em>does</em> have
something to say.</p>
<ol style="list-style-type: lower-roman">
<li><a name="part:predjob">*</a> Obtain predicted job allocations for each individual (based on
their scores on the three scales), and tabulate the true jobs
against the predicted jobs. How would you describe the quality of
the classification? Is that in line with what the plot would suggest?</li>
</ol>
<p>Solution</p>
<p>Use the predictions that you got before and saved in <code>d</code>:</p>
<div class="sourceCode" id="cb3465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3465-1"><a href="discriminant-analysis.html#cb3465-1"></a><span class="kw">with</span>(d, <span class="kw">table</span>(<span class="dt">obs =</span> job, <span class="dt">pred =</span> class))</span></code></pre></div>
<pre><code>##             pred
## obs          custserv mechanic dispatcher
##   custserv         68       13          4
##   mechanic         16       67         10
##   dispatcher        3       13         50</code></pre>
<p>Or, the <code>tidyverse</code> way:</p>
<div class="sourceCode" id="cb3467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3467-1"><a href="discriminant-analysis.html#cb3467-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(job, class)</span></code></pre></div>
<pre><code>##          job      class  n
## 1   custserv   custserv 68
## 2   custserv   mechanic 13
## 3   custserv dispatcher  4
## 4   mechanic   custserv 16
## 5   mechanic   mechanic 67
## 6   mechanic dispatcher 10
## 7 dispatcher   custserv  3
## 8 dispatcher   mechanic 13
## 9 dispatcher dispatcher 50</code></pre>
<p>or:</p>
<div class="sourceCode" id="cb3469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3469-1"><a href="discriminant-analysis.html#cb3469-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3469-2"><a href="discriminant-analysis.html#cb3469-2"></a><span class="st">  </span><span class="kw">count</span>(job, class) <span class="op">%&gt;%</span></span>
<span id="cb3469-3"><a href="discriminant-analysis.html#cb3469-3"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from=</span>class, <span class="dt">values_from=</span>n, <span class="dt">values_fill =</span> <span class="kw">list</span>(<span class="dt">n=</span><span class="dv">0</span>))</span></code></pre></div>
<pre><code>## # A tibble: 3 x 4
##   job        custserv mechanic dispatcher
##   &lt;fct&gt;         &lt;int&gt;    &lt;int&gt;      &lt;int&gt;
## 1 custserv         68       13          4
## 2 mechanic         16       67         10
## 3 dispatcher        3       13         50</code></pre>
<p>I didn’t really need the <code>values_fill</code> since there are no missing
frequencies, but I’ve gotten used to putting it in.
There are a lot of misclassifications, but there are a lot of people,
so a large fraction of people actually got classified correctly. The
biggest frequencies are of people who got classified correctly. I
think this is about what I was expecting, looking at the plot: the
people top left are obviously customer service, the ones top right are
in dispatch, and most of the ones at the bottom are mechanics. So
there will be some errors, but the majority of people should be gotten
right. The easiest pairing to get confused is customer service and
mechanics, which you might guess from the plot: those customer service
people with a middling <code>LD1</code> score and a low <code>LD2</code> score
(that is, high on <code>outdoor</code>) could easily be confused with the
mechanics. The easiest pairing to distinguish is customer service and
dispatchers: on the plot, left and right, that is, low and high
respectively on <code>LD1</code>.</p>
<p>What fraction of people actually got misclassified? You could just
pull out the numbers and add them up, but you know me: I’m too lazy to
do that.</p>
<p>We can work out the total number and fraction who got
misclassified. There are different ways you might do this, but the
<code>tidyverse</code> way provides the easiest starting point. For
example, we can make a new column that indicates whether a group is
the correct or wrong classification:</p>
<div class="sourceCode" id="cb3471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3471-1"><a href="discriminant-analysis.html#cb3471-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3471-2"><a href="discriminant-analysis.html#cb3471-2"></a><span class="st">  </span><span class="kw">count</span>(job, class) <span class="op">%&gt;%</span></span>
<span id="cb3471-3"><a href="discriminant-analysis.html#cb3471-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">job_stat =</span> <span class="kw">ifelse</span>(job <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>))</span></code></pre></div>
<pre><code>##          job      class  n job_stat
## 1   custserv   custserv 68  correct
## 2   custserv   mechanic 13    wrong
## 3   custserv dispatcher  4    wrong
## 4   mechanic   custserv 16    wrong
## 5   mechanic   mechanic 67  correct
## 6   mechanic dispatcher 10    wrong
## 7 dispatcher   custserv  3    wrong
## 8 dispatcher   mechanic 13    wrong
## 9 dispatcher dispatcher 50  correct</code></pre>
<p>From there, we count up the correct and wrong ones, recognizing that
we want to total up the <em>frequencies</em> in <code>n</code>, not just
count the number of rows:</p>
<div class="sourceCode" id="cb3473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3473-1"><a href="discriminant-analysis.html#cb3473-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3473-2"><a href="discriminant-analysis.html#cb3473-2"></a><span class="st">  </span><span class="kw">count</span>(job, class) <span class="op">%&gt;%</span></span>
<span id="cb3473-3"><a href="discriminant-analysis.html#cb3473-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">job_stat =</span> <span class="kw">ifelse</span>(job <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3473-4"><a href="discriminant-analysis.html#cb3473-4"></a><span class="st">  </span><span class="kw">count</span>(job_stat, <span class="dt">wt =</span> n)</span></code></pre></div>
<pre><code>##   job_stat   n
## 1  correct 185
## 2    wrong  59</code></pre>
<p>and turn these into proportions:</p>
<div class="sourceCode" id="cb3475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3475-1"><a href="discriminant-analysis.html#cb3475-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3475-2"><a href="discriminant-analysis.html#cb3475-2"></a><span class="st">  </span><span class="kw">count</span>(job, class) <span class="op">%&gt;%</span></span>
<span id="cb3475-3"><a href="discriminant-analysis.html#cb3475-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">job_stat =</span> <span class="kw">ifelse</span>(job <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3475-4"><a href="discriminant-analysis.html#cb3475-4"></a><span class="st">  </span><span class="kw">count</span>(job_stat, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3475-5"><a href="discriminant-analysis.html#cb3475-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</span></code></pre></div>
<pre><code>##   job_stat   n proportion
## 1  correct 185  0.7581967
## 2    wrong  59  0.2418033</code></pre>
<p>There is a <code>count</code> followed by another <code>count</code> of the first lot of counts, so the second count column has taken over the name <code>n</code>.</p>
<p>24% of all the employees got classified into the wrong job, based on
their scores on <code>outdoor</code>, <code>social</code> and
<code>conservative</code>.</p>
<p>This is actually not bad, from one point of view: if you just guessed
which job each person did, without looking at their scores on the
scales at all, you would get <span class="math inline">\({1\over 3}=33\%\)</span> of them right, just by
luck, and <span class="math inline">\({2\over3}=67\%\)</span> of them wrong. From 67% to 24% error is a
big improvement, and <em>that</em> is what the MANOVA is reacting to.</p>
<p>To figure out whether some of the groups were harder to classify than
others, squeeze a <code>group_by</code> in early to do the counts and
proportions for each (true) job:</p>
<div class="sourceCode" id="cb3477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3477-1"><a href="discriminant-analysis.html#cb3477-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3477-2"><a href="discriminant-analysis.html#cb3477-2"></a><span class="st">  </span><span class="kw">count</span>(job, class) <span class="op">%&gt;%</span></span>
<span id="cb3477-3"><a href="discriminant-analysis.html#cb3477-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">job_stat =</span> <span class="kw">ifelse</span>(job <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3477-4"><a href="discriminant-analysis.html#cb3477-4"></a><span class="st">  </span><span class="kw">group_by</span>(job) <span class="op">%&gt;%</span></span>
<span id="cb3477-5"><a href="discriminant-analysis.html#cb3477-5"></a><span class="st">  </span><span class="kw">count</span>(job_stat, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3477-6"><a href="discriminant-analysis.html#cb3477-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</span></code></pre></div>
<pre><code>## # A tibble: 6 x 4
## # Groups:   job [3]
##   job        job_stat     n proportion
##   &lt;fct&gt;      &lt;chr&gt;    &lt;int&gt;      &lt;dbl&gt;
## 1 custserv   correct     68      0.8  
## 2 custserv   wrong       17      0.2  
## 3 mechanic   correct     67      0.720
## 4 mechanic   wrong       26      0.280
## 5 dispatcher correct     50      0.758
## 6 dispatcher wrong       16      0.242</code></pre>
<p>or even split out the correct and wrong ones into their own columns:</p>
<div class="sourceCode" id="cb3479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3479-1"><a href="discriminant-analysis.html#cb3479-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3479-2"><a href="discriminant-analysis.html#cb3479-2"></a><span class="st">  </span><span class="kw">count</span>(job, class) <span class="op">%&gt;%</span></span>
<span id="cb3479-3"><a href="discriminant-analysis.html#cb3479-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">job_stat =</span> <span class="kw">ifelse</span>(job <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3479-4"><a href="discriminant-analysis.html#cb3479-4"></a><span class="st">  </span><span class="kw">group_by</span>(job) <span class="op">%&gt;%</span></span>
<span id="cb3479-5"><a href="discriminant-analysis.html#cb3479-5"></a><span class="st">  </span><span class="kw">count</span>(job_stat, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3479-6"><a href="discriminant-analysis.html#cb3479-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span></span>
<span id="cb3479-7"><a href="discriminant-analysis.html#cb3479-7"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>n) <span class="op">%&gt;%</span></span>
<span id="cb3479-8"><a href="discriminant-analysis.html#cb3479-8"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from=</span>job_stat, <span class="dt">values_from=</span>proportion)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 3
## # Groups:   job [3]
##   job        correct wrong
##   &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt;
## 1 custserv     0.8   0.2  
## 2 mechanic     0.720 0.280
## 3 dispatcher   0.758 0.242</code></pre>
<p>The mechanics were hardest to get right and easiest to get wrong,
though there isn’t much in it. I think the reason is that the
mechanics were sort of “in the middle” in that a mechanic could be
mistaken for either a dispatcher or a customer service representative,
but but customer service and dispatchers were more or less distinct
from each other.</p>
<p>It’s up to you whether you prefer to do this kind of thing by learning
enough about <code>table</code> to get it to work, or whether you want to
use tidy-data mechanisms to do it in a larger number of smaller
steps. I immediately thought of <code>table</code> because I knew about
it, but the tidy-data way is more consistent with the way we have been
doing things.</p>
<ol start="10" style="list-style-type: lower-alpha">
<li>Consider an employee with these scores: 20 on
<code>outdoor</code>, 17 on <code>social</code> and 8 on <code>conservative</code> What job do you think
they do, and how certain are you about that? Use <code>predict</code>,
first making a data frame out of the values to predict for.</li>
</ol>
<p>Solution</p>
<p>This is in fact exactly the same idea as the data frame that I
generally called <code>new</code> when doing predictions for other
models. I think the
clearest way to make one of these is with <code>tribble</code>:</p>
<div class="sourceCode" id="cb3481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3481-1"><a href="discriminant-analysis.html#cb3481-1"></a>new &lt;-<span class="st"> </span><span class="kw">tribble</span>(</span>
<span id="cb3481-2"><a href="discriminant-analysis.html#cb3481-2"></a>  <span class="op">~</span>outdoor, <span class="op">~</span>social, <span class="op">~</span>conservative,</span>
<span id="cb3481-3"><a href="discriminant-analysis.html#cb3481-3"></a>  <span class="dv">20</span>, <span class="dv">17</span>, <span class="dv">8</span></span>
<span id="cb3481-4"><a href="discriminant-analysis.html#cb3481-4"></a>)</span>
<span id="cb3481-5"><a href="discriminant-analysis.html#cb3481-5"></a>new</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   outdoor social conservative
##     &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;
## 1      20     17            8</code></pre>
<p>There’s no need for <code>crossing</code> here because I’m not doing
combinations of things. (I might have done that here, to get a sense
for example of “what effect does a higher score on <code>outdoor</code> have on the likelihood of a person doing each job?”. But I didn’t.)</p>
<p>Then feed this into <code>predict</code> as the <em>second</em> thing:</p>
<div class="sourceCode" id="cb3483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3483-1"><a href="discriminant-analysis.html#cb3483-1"></a>pp1 &lt;-<span class="st"> </span><span class="kw">predict</span>(job<span class="fl">.1</span>, new)</span></code></pre></div>
<p>Our predictions are these:</p>
<div class="sourceCode" id="cb3484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3484-1"><a href="discriminant-analysis.html#cb3484-1"></a><span class="kw">cbind</span>(new, pp1)</span></code></pre></div>
<pre><code>##   outdoor social conservative    class posterior.custserv posterior.mechanic
## 1      20     17            8 mechanic         0.05114665          0.7800624
##   posterior.dispatcher     x.LD1     x.LD2
## 1            0.1687909 0.7138376 -1.024436</code></pre>
<p>The <code>class</code> thing gives our predicted job, and the
<code>posterior</code> probabilities say how sure we are about that.
So we reckon there’s a 78% chance that this person is a mechanic;
they might be a dispatcher but they are unlikely to be in customer
service. Our best guess is that they are a mechanic.
<label for="tufte-mn-226" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-226" class="margin-toggle"><span class="marginnote">I discovered that I used <em>pp</em> twice, and I want to use the first one again later, so I had to rename this one.</span></p>
<p>Does this pass the sanity-check test? First figure out where our new
employee stands compared to the others:</p>
<div class="sourceCode" id="cb3486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3486-1"><a href="discriminant-analysis.html#cb3486-1"></a><span class="kw">summary</span>(jobs)</span></code></pre></div>
<pre><code>##     outdoor          social       conservative           job           id       
##  Min.   : 0.00   Min.   : 7.00   Min.   : 0.00   custserv  :85   Min.   : 1.00  
##  1st Qu.:13.00   1st Qu.:17.00   1st Qu.: 8.00   mechanic  :93   1st Qu.:21.00  
##  Median :16.00   Median :21.00   Median :11.00   dispatcher:66   Median :41.00  
##  Mean   :15.64   Mean   :20.68   Mean   :10.59                   Mean   :41.95  
##  3rd Qu.:19.00   3rd Qu.:25.00   3rd Qu.:13.00                   3rd Qu.:61.25  
##  Max.   :28.00   Max.   :35.00   Max.   :20.00                   Max.   :93.00</code></pre>
<p>Their score on <code>outdoor</code> is above average, but their scores on
the other two scales are below average (right on the 1st quartile in
each case).</p>
<p>Go back to the table of means
from the discriminant analysis output. The mechanics have the highest
average for <code>outdoor</code>, they’re in the middle on <code>social</code>
and they are lowish on <code>conservative</code>. Our new employee is at
least somewhat like that.</p>
<p>Or, we can figure out where our new employee sits on the
plot. The output from <code>predict</code> gives the predicted
<code>LD1</code> and <code>LD2</code>, which are 0.71 and <span class="math inline">\(-1.02\)</span>
respectively. This employee would sit to the right of and below the
middle of the plot: in the greens, but with a few blues nearby: most
likely a mechanic, possibly a dispatcher, but likely not customer
service, as the posterior probabilities suggest.</p>
<p>Extra: I can use the same mechanism to predict for a combination of
values. This would allow for the variability of each of the original
variables to differ, and enable us to assess the effect of, say, a
change in <code>conservative</code> over its “typical range”, which we
found out above with <code>summary(jobs)</code>. I’ll take the quartiles,
in my usual fashion:</p>
<div class="sourceCode" id="cb3488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3488-1"><a href="discriminant-analysis.html#cb3488-1"></a>outdoors &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">13</span>, <span class="dv">19</span>)</span>
<span id="cb3488-2"><a href="discriminant-analysis.html#cb3488-2"></a>socials &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">17</span>, <span class="dv">25</span>)</span>
<span id="cb3488-3"><a href="discriminant-analysis.html#cb3488-3"></a>conservatives &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">8</span>, <span class="dv">13</span>)</span></code></pre></div>
<p>The IQRs are not that different, which says that what we get here will
not be that different from the ``coefficients of linear
discriminants’’ above:</p>
<div class="sourceCode" id="cb3489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3489-1"><a href="discriminant-analysis.html#cb3489-1"></a>new &lt;-<span class="st"> </span><span class="kw">crossing</span>(</span>
<span id="cb3489-2"><a href="discriminant-analysis.html#cb3489-2"></a>  <span class="dt">outdoor =</span> outdoors, <span class="dt">social =</span> socials,</span>
<span id="cb3489-3"><a href="discriminant-analysis.html#cb3489-3"></a>  <span class="dt">conservative =</span> conservatives</span>
<span id="cb3489-4"><a href="discriminant-analysis.html#cb3489-4"></a>)</span>
<span id="cb3489-5"><a href="discriminant-analysis.html#cb3489-5"></a>pp2 &lt;-<span class="st"> </span><span class="kw">predict</span>(job<span class="fl">.1</span>, new)</span>
<span id="cb3489-6"><a href="discriminant-analysis.html#cb3489-6"></a>px &lt;-<span class="st"> </span><span class="kw">round</span>(pp2<span class="op">$</span>x, <span class="dv">2</span>)</span>
<span id="cb3489-7"><a href="discriminant-analysis.html#cb3489-7"></a><span class="kw">cbind</span>(new, pp2<span class="op">$</span>class, px)</span></code></pre></div>
<pre><code>##   outdoor social conservative  pp2$class   LD1   LD2
## 1      13     17            8   mechanic  0.07  0.55
## 2      13     17           13 dispatcher  0.84  0.99
## 3      13     25            8   custserv -1.48  0.15
## 4      13     25           13   custserv -0.71  0.59
## 5      19     17            8   mechanic  0.62 -0.80
## 6      19     17           13 dispatcher  1.40 -0.36
## 7      19     25            8   mechanic -0.93 -1.20
## 8      19     25           13   mechanic -0.16 -0.76</code></pre>
<p>The highest (most positive) LD1 score goes with high outdoor, low
social, high conservative (and being a dispatcher). It is often
interesting to look at the <em>second</em>-highest one as well: here
that is <em>low</em> outdoor, and the same low social and high
conservative as before. That means that <code>outdoor</code> has nothing
much to do with <code>LD1</code> score. Being low <code>social</code> is
strongly associated with <code>LD1</code> being positive, so that’s the
important part of <code>LD1</code>.</p>
<p>What about <code>LD2</code>? The most positive LD2 are these:</p>
<pre><code>
LD2    outdoor  social  conservative
====================================
0.99   low      low     high
0.59   low      high    high
0.55   low      low     low
</code></pre>
<p>These most consistently go with <code>outdoor</code> being low.</p>
<p>Is that consistent with the “coefficients of linear discriminants”?</p>
<div class="sourceCode" id="cb3492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3492-1"><a href="discriminant-analysis.html#cb3492-1"></a>job<span class="fl">.1</span><span class="op">$</span>scaling</span></code></pre></div>
<pre><code>##                      LD1         LD2
## social       -0.19427415 -0.04978105
## outdoor       0.09198065 -0.22501431
## conservative  0.15499199  0.08734288</code></pre>
<p>Very much so: <code>outdoor</code> has nothing much to do with
<code>LD1</code> and everything to do with <code>LD2</code>.</p>
<ol start="11" style="list-style-type: lower-alpha">
<li>Since I am not making you hand this one in, I’m going to keep
going. Re-run the analysis to incorporate cross-validation, and make
a table of the predicted group memberships. Is it much different
from the previous one you had? Why would that be?</li>
</ol>
<p>Solution</p>
<p>Stick a <code>CV=T</code> in the <code>lda</code>:</p>
<div class="sourceCode" id="cb3494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3494-1"><a href="discriminant-analysis.html#cb3494-1"></a>job<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(job <span class="op">~</span><span class="st"> </span>social <span class="op">+</span><span class="st"> </span>outdoor <span class="op">+</span><span class="st"> </span>conservative, <span class="dt">data =</span> jobs, <span class="dt">CV =</span> T)</span>
<span id="cb3494-2"><a href="discriminant-analysis.html#cb3494-2"></a><span class="kw">glimpse</span>(job<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>## List of 5
##  $ class    : Factor w/ 3 levels &quot;custserv&quot;,&quot;mechanic&quot;,..: 1 2 1 1 1 2 1 1 1 1 ...
##  $ posterior: num [1:244, 1:3] 0.902 0.352 0.71 0.805 0.766 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:244] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:3] &quot;custserv&quot; &quot;mechanic&quot; &quot;dispatcher&quot;
##  $ terms    :Classes &#39;terms&#39;, &#39;formula&#39;  language job ~ social + outdoor + conservative
##   .. ..- attr(*, &quot;variables&quot;)= language list(job, social, outdoor, conservative)
##   .. ..- attr(*, &quot;factors&quot;)= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ...
##   .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..- attr(*, &quot;term.labels&quot;)= chr [1:3] &quot;social&quot; &quot;outdoor&quot; &quot;conservative&quot;
##   .. ..- attr(*, &quot;order&quot;)= int [1:3] 1 1 1
##   .. ..- attr(*, &quot;intercept&quot;)= int 1
##   .. ..- attr(*, &quot;response&quot;)= int 1
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; 
##   .. ..- attr(*, &quot;predvars&quot;)= language list(job, social, outdoor, conservative)
##   .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:4] &quot;factor&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot;
##   .. .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;job&quot; &quot;social&quot; &quot;outdoor&quot; &quot;conservative&quot;
##  $ call     : language lda(formula = job ~ social + outdoor + conservative, data = jobs, CV = T)
##  $ xlevels  : Named list()</code></pre>
<p>This directly contains a <code>class</code> (no need for a
<code>predict</code>), so we make a data frame, with a different name
since I shortly want to compare this one with the previous one:</p>
<div class="sourceCode" id="cb3496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3496-1"><a href="discriminant-analysis.html#cb3496-1"></a>dcv &lt;-<span class="st"> </span><span class="kw">cbind</span>(jobs, <span class="dt">class =</span> job<span class="fl">.3</span><span class="op">$</span>class, <span class="dt">posterior =</span> job<span class="fl">.3</span><span class="op">$</span>posterior)</span>
<span id="cb3496-2"><a href="discriminant-analysis.html#cb3496-2"></a><span class="kw">head</span>(dcv)</span></code></pre></div>
<pre><code>##   outdoor social conservative      job id    class posterior.custserv posterior.mechanic
## 1      10     22            5 custserv  1 custserv          0.9015959         0.09090173
## 2      14     17            6 custserv  2 mechanic          0.3521921         0.49980444
## 3      19     33            7 custserv  3 custserv          0.7101838         0.28951755
## 4      14     29           12 custserv  4 custserv          0.8054563         0.18657994
## 5      14     25            7 custserv  5 custserv          0.7655123         0.22717194
## 6      20     25           12 custserv  6 mechanic          0.1579450         0.79420994
##   posterior.dispatcher
## 1         0.0075023669
## 2         0.1480034406
## 3         0.0002986778
## 4         0.0079637680
## 5         0.0073158061
## 6         0.0478450256</code></pre>
<p>This is a bit fiddlier than before because <code>job.3</code> contains some things of different lengths and we can’t just <code>cbind</code> them all together.</p>
<p>Then go straight to the <code>table</code>:</p>
<div class="sourceCode" id="cb3498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3498-1"><a href="discriminant-analysis.html#cb3498-1"></a><span class="kw">with</span>(dcv, <span class="kw">table</span>(job, class))</span></code></pre></div>
<pre><code>##             class
## job          custserv mechanic dispatcher
##   custserv         67       14          4
##   mechanic         16       67         10
##   dispatcher        3       14         49</code></pre>
<p>This is almost exactly the same as we had in part
(<a href="#part:predjob">here</a>): the cross-validation has made almost no
difference. The reason for that is that here, we have lots of data
(you can predict for one mechanic, say, and there are still lots of
others to say that the mechanics are “over there”. This is in sharp
contrast to the example in class with the bellydancers, where if you
try to predict for one of the extreme ones, the notion of
“where are the bellydancers” changes substantially.
Here, I suspect that the few
people whose predictions changed were ones where the posterior
probabilities were almost equal for two jobs, and the cross-validation
was just enough to tip the balance. You can check this, but there are
a lot of posterior probabilities to look at!</p>
<p>This is another way of saying that with small data sets, your
conclusions are more “fragile” or less likely to be
generalizable. With a larger data set like this one, cross-validation,
which is the right thing to do, makes almost no difference.
<label for="tufte-mn-227" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-227" class="margin-toggle"><span class="marginnote">So we should do it, when assessing how good the classification is.</span></p>
<p>All right, I suppose I do want to investigate the individuals whose
predicted jobs changed, and look at their posterior probabilities. I
think I have the machinery to do that.</p>
<p>Let’s start by gluing together the dataframes with the predictions from the regular <code>lda</code> (in <code>d</code>) and the ones from the cross-validation (in <code>dcv</code>). I think I can do that like this:</p>
<div class="sourceCode" id="cb3500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3500-1"><a href="discriminant-analysis.html#cb3500-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">left_join</span>(dcv, <span class="dt">by=</span><span class="kw">c</span>(<span class="st">&quot;id&quot;</span>, <span class="st">&quot;job&quot;</span>))</span></code></pre></div>
<pre><code>##    outdoor.x social.x conservative.x      job id    class.x posterior.custserv.x
## 1         10       22              5 custserv  1   custserv           0.90376216
## 2         14       17              6 custserv  2   mechanic           0.36777434
## 3         19       33              7 custserv  3   custserv           0.73021166
## 4         14       29             12 custserv  4   custserv           0.81007560
## 5         14       25              7 custserv  5   custserv           0.76776069
## 6         20       25             12 custserv  6   mechanic           0.16825208
## 7          6       18              4 custserv  7   custserv           0.94083284
## 8         13       27              7 custserv  8   custserv           0.87900857
## 9         18       31              9 custserv  9   custserv           0.67674638
## 10        16       35             13 custserv 10   custserv           0.86435636
## 11        17       25              8 custserv 11   custserv           0.49503885
## 12        10       29             11 custserv 12   custserv           0.95374457
## 13        17       25              7 custserv 13   custserv           0.52408230
## 14        10       22             13 custserv 14   custserv           0.68197947
## 15        10       31             13 custserv 15   custserv           0.96135426
## 16        18       25              5 custserv 16   mechanic           0.48875843
## 17         0       27             11 custserv 17   custserv           0.99745335
## 18        10       24             12 custserv 18   custserv           0.83707070
## 19        15       23             10 custserv 19   custserv           0.50040125
## 20         8       29             14 custserv 20   custserv           0.96490542
## 21         6       27             11 custserv 21   custserv           0.98156768
## 22        10       17              8 custserv 22   custserv           0.56977475
## 23         1       30              6 custserv 23   custserv           0.99919823
## 24        14       29              7 custserv 24   custserv           0.88377741
## 25        13       21             11 custserv 25   custserv           0.49380438
## 26        21       31             11 custserv 26   mechanic           0.35640965
## 27        12       26              9 custserv 27   custserv           0.86931260
## 28        12       22              9 custserv 28   custserv           0.72159120
## 29         5       25              7 custserv 29   custserv           0.98833648
## 30        10       24              5 custserv 30   custserv           0.93590416
## 31         3       20             14 custserv 31   custserv           0.80187583
## 32         6       25             12 custserv 32   custserv           0.96187981
## 33        11       27             10 custserv 33   custserv           0.91292302
## 34        13       21             14 custserv 34 dispatcher           0.32205156
## 35        11       23              5 custserv 35   custserv           0.89099176
## 36         8       18              8 custserv 36   custserv           0.77349841
## 37         5       17              9 custserv 37   custserv           0.79289686
## 38        11       22             11 custserv 38   custserv           0.71523020
## 39        14       22             11 custserv 39   custserv           0.48308467
## 40        22       22              6 custserv 40   mechanic           0.09523627
## 41        16       28              6 custserv 41   custserv           0.76798327
## 42        12       25              8 custserv 42   custserv           0.85827566
## 43        12       25              7 custserv 43   custserv           0.87271314
## 44        15       21              4 custserv 44   custserv           0.58325498
## 45        11       28              8 custserv 45   custserv           0.94323136
## 46        11       20              9 custserv 46   custserv           0.67486570
## 47        15       19              9 custserv 47   mechanic           0.29551041
## 48        15       24              7 custserv 48   custserv           0.64962195
## 49        15       21             10 custserv 49   mechanic           0.37742144
## 50        17       26              7 custserv 50   custserv           0.57551700
## 51        12       28             13 custserv 51   custserv           0.86035047
## 52         7       28             12 custserv 52   custserv           0.97641554
## 53        14       12              6 custserv 53 dispatcher           0.10135781
## 54        22       24              6 custserv 54   mechanic           0.13730091
## 55        22       27             12 custserv 55   mechanic           0.13029357
##    posterior.mechanic.x posterior.dispatcher.x       x.LD1       x.LD2 outdoor.y social.y
## 1          0.0889478485           7.289988e-03 -1.64231553  0.71477348        10       22
## 2          0.4889789008           1.432468e-01 -0.14803023  0.15096436        14       17
## 3          0.2694697105           3.186265e-04 -2.64152132 -1.68326115        19       33
## 4          0.1821731894           7.751215e-03 -1.54936806  0.07764901        14       29
## 5          0.2250538225           7.185490e-03 -1.54723140 -0.15994117        14       25
## 6          0.7848248752           4.692305e-02 -0.22038758 -1.07331266        20       25
## 7          0.0424620706           1.670509e-02 -1.38813353  1.72661206         6       18
## 8          0.1186423050           2.349121e-03 -2.02776034 -0.03448896        13       27
## 9          0.3217022453           1.551373e-03 -2.03496971 -1.18399898        18       31
## 10         0.1347795344           8.641095e-04 -2.37605965 -0.58372304        16       35
## 11         0.4914751264           1.348603e-02 -1.11629747 -0.74764123        17       25
## 12         0.0437303474           2.525084e-03 -2.07228264  0.89036339        10       29
## 13         0.4665293631           9.388337e-03 -1.27128946 -0.83498411        17       25
## 14         0.1606625929           1.573579e-01 -0.40237965  1.41351651        10       22
## 15         0.0365588154           2.086923e-03 -2.15084695  0.96548704        10       31
## 16         0.5065409138           4.700656e-03 -1.48929278 -1.23468418        18       25
## 17         0.0016911727           8.554731e-04 -2.60354083  3.24006862         0       27
## 18         0.1179003525           4.502895e-02 -0.94591992  1.22661153        10       24
## 19         0.4419175871           5.768116e-02 -0.60172651 -0.02336475        15       23
## 20         0.0292665325           5.828045e-03 -1.79126798  1.60242065         8       29
## 21         0.0153480620           3.084262e-03 -2.05165694  1.88998275         6       27
## 22         0.2142654531           2.159598e-01 -0.20596885  1.22570737        10       17
## 23         0.0007741549           2.761694e-05 -3.86934255  2.42899676         1       30
## 24         0.1151828840           1.039703e-03 -2.32432799 -0.35906538        14       29
## 25         0.3478209184           1.583747e-01 -0.24214753  0.61356886        13       21
## 26         0.6399736784           3.616675e-03 -1.44904379 -1.68435616        21       31
## 27         0.1234199965           7.267402e-03 -1.61548287  0.41499216        12       26
## 28         0.2304179226           4.799088e-02 -0.83838629  0.61411637        12       22
## 29         0.0103444748           1.319044e-03 -2.37505724  1.86518765         5       25
## 30         0.0614193101           2.676529e-03 -2.03086382  0.61521138        10       24
## 31         0.0236590252           1.744651e-01 -0.50270391  3.17552168         3       20
## 32         0.0251561754           1.296402e-02 -1.50811666  2.07688773         6       25
## 33         0.0815110137           5.565968e-03 -1.74674568  0.67756830        11       27
## 34         0.3146838699           3.632646e-01  0.22282843  0.87559750        13       21
## 35         0.1036949094           5.313327e-03 -1.74460903  0.43997811        11       23
## 36         0.1132639919           1.132376e-01 -0.58420429  1.62595495         8       18
## 37         0.0522175881           1.548856e-01 -0.51088010  2.43812182         5       17
## 38         0.1961688726           8.860092e-02 -0.62038297  1.01381644        11       22
## 39         0.4023708150           1.145445e-01 -0.34444102  0.33877350        14       22
## 40         0.8890830359           1.568069e-02 -0.38355577 -1.89805540        22       22
## 41         0.2304785704           1.538158e-03 -2.10108453 -0.84665583        16       28
## 42         0.1338004111           7.923928e-03 -1.57620071  0.37743033        12       25
## 43         0.1219886414           5.298216e-03 -1.73119270  0.29008746        12       25
## 44         0.4014139446           1.533108e-02 -1.14313013 -0.44785991        15       21
## 45         0.0552880094           1.480633e-03 -2.25100380  0.45310149        11       28
## 46         0.2231741012           1.019602e-01 -0.54181865  0.93869278        11       20
## 47         0.5262939623           1.781956e-01  0.02037809  0.08841658        15       19
## 48         0.3377002656           1.267779e-02 -1.26097661 -0.33517443        15       24
## 49         0.4998701545           1.227084e-01 -0.21317822  0.07619736        15       21
## 50         0.4183442282           6.138769e-03 -1.46556360 -0.88476516        17       26
## 51         0.1260109570           1.363857e-02 -1.38406322  0.66480157        12       28
## 52         0.0201350730           3.449384e-03 -1.99895845  1.70253026         7       28
## 53         0.3711805789           5.274616e-01  0.82334050  0.39986962        14       12
## 54         0.8546840791           8.015011e-03 -0.77210406 -1.99761750        22       24
## 55         0.8498459777           1.986045e-02 -0.42497458 -1.62290339        22       27
##    conservative.y    class.y posterior.custserv.y posterior.mechanic.y
## 1               5   custserv           0.90159590         0.0909017323
## 2               6   mechanic           0.35219212         0.4998044379
## 3               7   custserv           0.71018377         0.2895175491
## 4              12   custserv           0.80545629         0.1865799427
## 5               7   custserv           0.76551226         0.2271719387
## 6              12   mechanic           0.15794504         0.7942099381
## 7               4   custserv           0.93792890         0.0439602743
## 8               7   custserv           0.87730096         0.1203168496
## 9               9   custserv           0.66305314         0.3353941278
## 10             13   custserv           0.85374335         0.1454071153
## 11              8   mechanic           0.49005230         0.4961867815
## 12             11   custserv           0.95297866         0.0444497636
## 13              7   custserv           0.51774825         0.4726677462
## 14             13   custserv           0.67414769         0.1639151041
## 15             13   custserv           0.96045000         0.0374216844
## 16              5   mechanic           0.47476429         0.5204646333
## 17             11   custserv           0.99766495         0.0014939107
## 18             12   custserv           0.83407817         0.1197743609
## 19             10   custserv           0.49840060         0.4433833185
## 20             14   custserv           0.96405062         0.0298520020
## 21             11   custserv           0.98144464         0.0153892528
## 22              8   custserv           0.55644297         0.2193844588
## 23              6   custserv           0.99933596         0.0006425343
## 24              7   custserv           0.88123187         0.1177243692
## 25             11   custserv           0.49004754         0.3507825820
## 26             11   mechanic           0.32846956         0.6679472291
## 27              9   custserv           0.86814860         0.1244616639
## 28              9   custserv           0.71972491         0.2318062554
## 29              7   custserv           0.98839615         0.0102735851
## 30              5   custserv           0.93471965         0.0625547690
## 31             14   custserv           0.77424635         0.0235896872
## 32             12   custserv           0.96091913         0.0254749476
## 33             10   custserv           0.91182090         0.0825052772
## 34             14 dispatcher           0.31104587         0.3194771955
## 35              5   custserv           0.88878974         0.1057697409
## 36              8   custserv           0.76456675         0.1163652476
## 37              9   custserv           0.77587842         0.0534832962
## 38             11   custserv           0.71200418         0.1983147027
## 39             11   custserv           0.48010222         0.4047357863
## 40              6   mechanic           0.08328070         0.9010277467
## 41              6   custserv           0.76162434         0.2368262506
## 42              8   custserv           0.85709634         0.1348488652
## 43              7   custserv           0.87133964         0.1232692610
## 44              4   custserv           0.57063277         0.4135417207
## 45              8   custserv           0.94246281         0.0560415630
## 46              9   custserv           0.67059301         0.2259802818
## 47              9   mechanic           0.28926354         0.5309880384
## 48              7   custserv           0.64651797         0.3405640467
## 49             10   mechanic           0.37396611         0.5026088101
## 50              7   custserv           0.56871864         0.4250221357
## 51             13   custserv           0.85678048         0.1291169423
## 52             12   custserv           0.97614179         0.0203090285
## 53              6 dispatcher           0.08223677         0.3667692252
## 54              6   mechanic           0.12227679         0.8697585447
## 55             12   mechanic           0.11601556         0.8639522011
##    posterior.dispatcher.y
## 1            7.502367e-03
## 2            1.480034e-01
## 3            2.986778e-04
## 4            7.963768e-03
## 5            7.315806e-03
## 6            4.784503e-02
## 7            1.811083e-02
## 8            2.382195e-03
## 9            1.552731e-03
## 10           8.495300e-04
## 11           1.376092e-02
## 12           2.571578e-03
## 13           9.584003e-03
## 14           1.619372e-01
## 15           2.128320e-03
## 16           4.771076e-03
## 17           8.411382e-04
## 18           4.614747e-02
## 19           5.821608e-02
## 20           6.097378e-03
## 21           3.166103e-03
## 22           2.241726e-01
## 23           2.150907e-05
## 24           1.043763e-03
## 25           1.591699e-01
## 26           3.583212e-03
## 27           7.389737e-03
## 28           4.846884e-02
## 29           1.330263e-03
## 30           2.725584e-03
## 31           2.021640e-01
## 32           1.360592e-02
## 33           5.673827e-03
## 34           3.694769e-01
## 35           5.440523e-03
## 36           1.190680e-01
## 37           1.706383e-01
## 38           8.968111e-02
## 39           1.151620e-01
## 40           1.569155e-02
## 41           1.549406e-03
## 42           8.054796e-03
## 43           5.391098e-03
## 44           1.582551e-02
## 45           1.495624e-03
## 46           1.034267e-01
## 47           1.797484e-01
## 48           1.291799e-02
## 49           1.234251e-01
## 50           6.259229e-03
## 51           1.410258e-02
## 52           3.549184e-03
## 53           5.509940e-01
## 54           7.964666e-03
## 55           2.003224e-02
##  [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 189 rows ]</code></pre>
<p>There’s already subtlety. The people are numbered separately within each actual <code>job</code>, so the thing that uniquely identifies each person (what database people call a “key”) is the combination of the actual job they do <em>plus</em> their <code>id</code> within that job. You might also think of using <code>bind_cols</code>, except that this adds a number to all the column names which is a pain to deal with. I don’t really need to look up the people in the second dataframe, since I know where they are (in the corresponding rows), but doing so seems to make everything else easier.</p>
<p>The columns with an <code>x</code> on the end of their names came from <code>d</code>, that is, the predictions without cross-validation, and the ones with a <code>y</code> came from cross-validation. Let’s see if we can keep only the columns we need so that it’s a bit less unwieldy:</p>
<div class="sourceCode" id="cb3502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3502-1"><a href="discriminant-analysis.html#cb3502-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">left_join</span>(dcv, <span class="dt">by=</span><span class="kw">c</span>(<span class="st">&quot;id&quot;</span>, <span class="st">&quot;job&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3502-2"><a href="discriminant-analysis.html#cb3502-2"></a><span class="st">  </span><span class="kw">select</span>(<span class="dt">outdoor =</span> outdoor.x,</span>
<span id="cb3502-3"><a href="discriminant-analysis.html#cb3502-3"></a>         <span class="dt">social =</span> social.x,</span>
<span id="cb3502-4"><a href="discriminant-analysis.html#cb3502-4"></a>         <span class="dt">conservative =</span> conservative.x,</span>
<span id="cb3502-5"><a href="discriminant-analysis.html#cb3502-5"></a>         job, id, <span class="kw">starts_with</span>(<span class="st">&quot;class&quot;</span>),</span>
<span id="cb3502-6"><a href="discriminant-analysis.html#cb3502-6"></a>         <span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>)) -&gt;<span class="st"> </span>all</span>
<span id="cb3502-7"><a href="discriminant-analysis.html#cb3502-7"></a>all</span></code></pre></div>
<pre><code>##    outdoor social conservative      job id    class.x    class.y posterior.custserv.x
## 1       10     22            5 custserv  1   custserv   custserv           0.90376216
## 2       14     17            6 custserv  2   mechanic   mechanic           0.36777434
## 3       19     33            7 custserv  3   custserv   custserv           0.73021166
## 4       14     29           12 custserv  4   custserv   custserv           0.81007560
## 5       14     25            7 custserv  5   custserv   custserv           0.76776069
## 6       20     25           12 custserv  6   mechanic   mechanic           0.16825208
## 7        6     18            4 custserv  7   custserv   custserv           0.94083284
## 8       13     27            7 custserv  8   custserv   custserv           0.87900857
## 9       18     31            9 custserv  9   custserv   custserv           0.67674638
## 10      16     35           13 custserv 10   custserv   custserv           0.86435636
## 11      17     25            8 custserv 11   custserv   mechanic           0.49503885
## 12      10     29           11 custserv 12   custserv   custserv           0.95374457
## 13      17     25            7 custserv 13   custserv   custserv           0.52408230
## 14      10     22           13 custserv 14   custserv   custserv           0.68197947
## 15      10     31           13 custserv 15   custserv   custserv           0.96135426
## 16      18     25            5 custserv 16   mechanic   mechanic           0.48875843
## 17       0     27           11 custserv 17   custserv   custserv           0.99745335
## 18      10     24           12 custserv 18   custserv   custserv           0.83707070
## 19      15     23           10 custserv 19   custserv   custserv           0.50040125
## 20       8     29           14 custserv 20   custserv   custserv           0.96490542
## 21       6     27           11 custserv 21   custserv   custserv           0.98156768
## 22      10     17            8 custserv 22   custserv   custserv           0.56977475
## 23       1     30            6 custserv 23   custserv   custserv           0.99919823
## 24      14     29            7 custserv 24   custserv   custserv           0.88377741
## 25      13     21           11 custserv 25   custserv   custserv           0.49380438
## 26      21     31           11 custserv 26   mechanic   mechanic           0.35640965
## 27      12     26            9 custserv 27   custserv   custserv           0.86931260
## 28      12     22            9 custserv 28   custserv   custserv           0.72159120
## 29       5     25            7 custserv 29   custserv   custserv           0.98833648
## 30      10     24            5 custserv 30   custserv   custserv           0.93590416
## 31       3     20           14 custserv 31   custserv   custserv           0.80187583
## 32       6     25           12 custserv 32   custserv   custserv           0.96187981
## 33      11     27           10 custserv 33   custserv   custserv           0.91292302
## 34      13     21           14 custserv 34 dispatcher dispatcher           0.32205156
## 35      11     23            5 custserv 35   custserv   custserv           0.89099176
## 36       8     18            8 custserv 36   custserv   custserv           0.77349841
## 37       5     17            9 custserv 37   custserv   custserv           0.79289686
## 38      11     22           11 custserv 38   custserv   custserv           0.71523020
## 39      14     22           11 custserv 39   custserv   custserv           0.48308467
## 40      22     22            6 custserv 40   mechanic   mechanic           0.09523627
## 41      16     28            6 custserv 41   custserv   custserv           0.76798327
## 42      12     25            8 custserv 42   custserv   custserv           0.85827566
## 43      12     25            7 custserv 43   custserv   custserv           0.87271314
## 44      15     21            4 custserv 44   custserv   custserv           0.58325498
## 45      11     28            8 custserv 45   custserv   custserv           0.94323136
## 46      11     20            9 custserv 46   custserv   custserv           0.67486570
## 47      15     19            9 custserv 47   mechanic   mechanic           0.29551041
## 48      15     24            7 custserv 48   custserv   custserv           0.64962195
## 49      15     21           10 custserv 49   mechanic   mechanic           0.37742144
## 50      17     26            7 custserv 50   custserv   custserv           0.57551700
## 51      12     28           13 custserv 51   custserv   custserv           0.86035047
## 52       7     28           12 custserv 52   custserv   custserv           0.97641554
## 53      14     12            6 custserv 53 dispatcher dispatcher           0.10135781
## 54      22     24            6 custserv 54   mechanic   mechanic           0.13730091
## 55      22     27           12 custserv 55   mechanic   mechanic           0.13029357
## 56      18     30            9 custserv 56   custserv   custserv           0.63051858
## 57      16     18            5 custserv 57   mechanic   mechanic           0.30520076
## 58      12     23            4 custserv 58   custserv   custserv           0.86506409
## 59      16     22            2 custserv 59   custserv   custserv           0.60143367
## 60      15     26            9 custserv 60   custserv   custserv           0.69101257
## 61       7     13            7 custserv 61 dispatcher dispatcher           0.43751617
## 62       6     18            6 custserv 62   custserv   custserv           0.91141056
## 63       9     24            6 custserv 63   custserv   custserv           0.94872657
## 64       9     20           12 custserv 64   custserv   custserv           0.63961913
## 65      20     28            8 custserv 65   mechanic   mechanic           0.37698626
## 66       5     22           15 custserv 66   custserv   custserv           0.81509298
## 67      14     26           17 custserv 67   custserv   custserv           0.48046467
## 68       8     28           12 custserv 68   custserv   custserv           0.96688547
## 69      14     22            9 custserv 69   custserv   custserv           0.56428261
## 70      15     26            4 custserv 70   custserv   custserv           0.79882562
## 71      15     25           10 custserv 71   custserv   custserv           0.61359773
## 72      14     27            6 custserv 72   custserv   custserv           0.84930250
## 73      15     25           11 custserv 73   custserv   custserv           0.58176861
## 74      11     26            9 custserv 74   custserv   custserv           0.90516323
## 75      10     28            5 custserv 75   custserv   custserv           0.97130979
## 76       7     22           10 custserv 76   custserv   custserv           0.91726515
##    posterior.mechanic.x posterior.dispatcher.x posterior.custserv.y posterior.mechanic.y
## 1          0.0889478485           7.289988e-03           0.90159590         0.0909017323
## 2          0.4889789008           1.432468e-01           0.35219212         0.4998044379
## 3          0.2694697105           3.186265e-04           0.71018377         0.2895175491
## 4          0.1821731894           7.751215e-03           0.80545629         0.1865799427
## 5          0.2250538225           7.185490e-03           0.76551226         0.2271719387
## 6          0.7848248752           4.692305e-02           0.15794504         0.7942099381
## 7          0.0424620706           1.670509e-02           0.93792890         0.0439602743
## 8          0.1186423050           2.349121e-03           0.87730096         0.1203168496
## 9          0.3217022453           1.551373e-03           0.66305314         0.3353941278
## 10         0.1347795344           8.641095e-04           0.85374335         0.1454071153
## 11         0.4914751264           1.348603e-02           0.49005230         0.4961867815
## 12         0.0437303474           2.525084e-03           0.95297866         0.0444497636
## 13         0.4665293631           9.388337e-03           0.51774825         0.4726677462
## 14         0.1606625929           1.573579e-01           0.67414769         0.1639151041
## 15         0.0365588154           2.086923e-03           0.96045000         0.0374216844
## 16         0.5065409138           4.700656e-03           0.47476429         0.5204646333
## 17         0.0016911727           8.554731e-04           0.99766495         0.0014939107
## 18         0.1179003525           4.502895e-02           0.83407817         0.1197743609
## 19         0.4419175871           5.768116e-02           0.49840060         0.4433833185
## 20         0.0292665325           5.828045e-03           0.96405062         0.0298520020
## 21         0.0153480620           3.084262e-03           0.98144464         0.0153892528
## 22         0.2142654531           2.159598e-01           0.55644297         0.2193844588
## 23         0.0007741549           2.761694e-05           0.99933596         0.0006425343
## 24         0.1151828840           1.039703e-03           0.88123187         0.1177243692
## 25         0.3478209184           1.583747e-01           0.49004754         0.3507825820
## 26         0.6399736784           3.616675e-03           0.32846956         0.6679472291
## 27         0.1234199965           7.267402e-03           0.86814860         0.1244616639
## 28         0.2304179226           4.799088e-02           0.71972491         0.2318062554
## 29         0.0103444748           1.319044e-03           0.98839615         0.0102735851
## 30         0.0614193101           2.676529e-03           0.93471965         0.0625547690
## 31         0.0236590252           1.744651e-01           0.77424635         0.0235896872
## 32         0.0251561754           1.296402e-02           0.96091913         0.0254749476
## 33         0.0815110137           5.565968e-03           0.91182090         0.0825052772
## 34         0.3146838699           3.632646e-01           0.31104587         0.3194771955
## 35         0.1036949094           5.313327e-03           0.88878974         0.1057697409
## 36         0.1132639919           1.132376e-01           0.76456675         0.1163652476
## 37         0.0522175881           1.548856e-01           0.77587842         0.0534832962
## 38         0.1961688726           8.860092e-02           0.71200418         0.1983147027
## 39         0.4023708150           1.145445e-01           0.48010222         0.4047357863
## 40         0.8890830359           1.568069e-02           0.08328070         0.9010277467
## 41         0.2304785704           1.538158e-03           0.76162434         0.2368262506
## 42         0.1338004111           7.923928e-03           0.85709634         0.1348488652
## 43         0.1219886414           5.298216e-03           0.87133964         0.1232692610
## 44         0.4014139446           1.533108e-02           0.57063277         0.4135417207
## 45         0.0552880094           1.480633e-03           0.94246281         0.0560415630
## 46         0.2231741012           1.019602e-01           0.67059301         0.2259802818
## 47         0.5262939623           1.781956e-01           0.28926354         0.5309880384
## 48         0.3377002656           1.267779e-02           0.64651797         0.3405640467
## 49         0.4998701545           1.227084e-01           0.37396611         0.5026088101
## 50         0.4183442282           6.138769e-03           0.56871864         0.4250221357
## 51         0.1260109570           1.363857e-02           0.85678048         0.1291169423
## 52         0.0201350730           3.449384e-03           0.97614179         0.0203090285
## 53         0.3711805789           5.274616e-01           0.08223677         0.3667692252
## 54         0.8546840791           8.015011e-03           0.12227679         0.8697585447
## 55         0.8498459777           1.986045e-02           0.11601556         0.8639522011
## 56         0.3670539517           2.427472e-03           0.61794765         0.3796015651
## 57         0.6230462861           7.175295e-02           0.28851972         0.6373977298
## 58         0.1307240754           4.211839e-03           0.86145035         0.1342358317
## 59         0.3935126078           5.053717e-03           0.58006042         0.4147716187
## 60         0.2979301609           1.105727e-02           0.68870979         0.3000364384
## 61         0.1092588927           4.532249e-01           0.39156559         0.1091245644
## 62         0.0511644494           3.742499e-02           0.90676249         0.0527878840
## 63         0.0479502714           3.323158e-03           0.94794627         0.0486636791
## 64         0.1399212131           2.204597e-01           0.62967504         0.1427717631
## 65         0.6188639027           4.149835e-03           0.36034706         0.6354747314
## 66         0.0375046016           1.474024e-01           0.79580474         0.0381876729
## 67         0.3424095294           1.771258e-01           0.45358332         0.3564338501
## 68         0.0288735299           4.240998e-03           0.96635531         0.0292786181
## 69         0.3778629731           5.785442e-02           0.56241916         0.3792364918
## 70         0.1996028063           1.571578e-03           0.79216340         0.2062528007
## 71         0.3613257474           2.507652e-02           0.61152087         0.3630313609
## 72         0.1488443717           1.853127e-03           0.84641360         0.1517112655
## 73         0.3820746444           3.615675e-02           0.57878617         0.3845196437
## 74         0.0887421887           6.094582e-03           0.90417782         0.0896198215
## 75         0.0283410443           3.491677e-04           0.97092997         0.0287283607
## 76         0.0512945333           3.144031e-02           0.91512373         0.0521945697
##    posterior.dispatcher.y
## 1            7.502367e-03
## 2            1.480034e-01
## 3            2.986778e-04
## 4            7.963768e-03
## 5            7.315806e-03
## 6            4.784503e-02
## 7            1.811083e-02
## 8            2.382195e-03
## 9            1.552731e-03
## 10           8.495300e-04
## 11           1.376092e-02
## 12           2.571578e-03
## 13           9.584003e-03
## 14           1.619372e-01
## 15           2.128320e-03
## 16           4.771076e-03
## 17           8.411382e-04
## 18           4.614747e-02
## 19           5.821608e-02
## 20           6.097378e-03
## 21           3.166103e-03
## 22           2.241726e-01
## 23           2.150907e-05
## 24           1.043763e-03
## 25           1.591699e-01
## 26           3.583212e-03
## 27           7.389737e-03
## 28           4.846884e-02
## 29           1.330263e-03
## 30           2.725584e-03
## 31           2.021640e-01
## 32           1.360592e-02
## 33           5.673827e-03
## 34           3.694769e-01
## 35           5.440523e-03
## 36           1.190680e-01
## 37           1.706383e-01
## 38           8.968111e-02
## 39           1.151620e-01
## 40           1.569155e-02
## 41           1.549406e-03
## 42           8.054796e-03
## 43           5.391098e-03
## 44           1.582551e-02
## 45           1.495624e-03
## 46           1.034267e-01
## 47           1.797484e-01
## 48           1.291799e-02
## 49           1.234251e-01
## 50           6.259229e-03
## 51           1.410258e-02
## 52           3.549184e-03
## 53           5.509940e-01
## 54           7.964666e-03
## 55           2.003224e-02
## 56           2.450782e-03
## 57           7.408255e-02
## 58           4.313814e-03
## 59           5.167962e-03
## 60           1.125377e-02
## 61           4.993098e-01
## 62           4.044963e-02
## 63           3.390047e-03
## 64           2.275532e-01
## 65           4.178210e-03
## 66           1.660076e-01
## 67           1.899828e-01
## 68           4.366069e-03
## 69           5.834435e-02
## 70           1.583798e-03
## 71           2.544777e-02
## 72           1.875131e-03
## 73           3.669419e-02
## 74           6.202361e-03
## 75           3.416697e-04
## 76           3.268170e-02
##  [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 168 rows ]</code></pre>
<p>That’s not too bad. We could shorten some variable names and reduce some decimal places, but that’ll do for now.</p>
<p>How many individuals were predicted differently? We have columns called <code>class.x</code> (predicted group membership from original LDA) and <code>class.y</code> (from cross-validation), and so:</p>
<div class="sourceCode" id="cb3504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3504-1"><a href="discriminant-analysis.html#cb3504-1"></a>all <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(class.x <span class="op">!=</span><span class="st"> </span>class.y)</span></code></pre></div>
<pre><code>##   outdoor social conservative        job id    class.x  class.y posterior.custserv.x
## 1      17     25            8   custserv 11   custserv mechanic           0.49503885
## 2      18     20           15 dispatcher 42 dispatcher mechanic           0.05555677
##   posterior.mechanic.x posterior.dispatcher.x posterior.custserv.y posterior.mechanic.y
## 1            0.4914751             0.01348603           0.49005230            0.4961868
## 2            0.4721765             0.47226675           0.05681314            0.4789033
##   posterior.dispatcher.y
## 1             0.01376092
## 2             0.46428353</code></pre>
<p>There are exactly <em>two</em> individuals that were predicted differently.
Under cross-validation, they both got called mechanics.
How do their posterior probabilities compare? These are all in columns beginning with <code>posterior</code>. We could scrutinize the output above, or
try to make things simpler.
Let’s round them to three decimals, and then display only some of the columns:</p>
<div class="sourceCode" id="cb3506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3506-1"><a href="discriminant-analysis.html#cb3506-1"></a>all <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(class.x <span class="op">!=</span><span class="st"> </span>class.y) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3506-2"><a href="discriminant-analysis.html#cb3506-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="kw">across</span>(<span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>), <span class="op">~</span><span class="st"> </span><span class="kw">round</span>(., <span class="dv">3</span>))) <span class="op">%&gt;%</span></span>
<span id="cb3506-3"><a href="discriminant-analysis.html#cb3506-3"></a><span class="st">  </span><span class="kw">select</span>(id, job, <span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>))</span></code></pre></div>
<pre><code>##   id        job posterior.custserv.x posterior.mechanic.x posterior.dispatcher.x
## 1 11   custserv                0.495                0.491                  0.013
## 2 42 dispatcher                0.056                0.472                  0.472
##   posterior.custserv.y posterior.mechanic.y posterior.dispatcher.y
## 1                0.490                0.496                  0.014
## 2                0.057                0.479                  0.464</code></pre>
<p>And then, because I can, let’s re-format that to make it easier to read, <code>x</code> being regular LDA and <code>y</code> being cross-validation:</p>
<div class="sourceCode" id="cb3508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3508-1"><a href="discriminant-analysis.html#cb3508-1"></a>all <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(class.x <span class="op">!=</span><span class="st"> </span>class.y) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3508-2"><a href="discriminant-analysis.html#cb3508-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="kw">across</span>(<span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>), <span class="op">~</span><span class="st"> </span><span class="kw">round</span>(., <span class="dv">3</span>))) <span class="op">%&gt;%</span></span>
<span id="cb3508-3"><a href="discriminant-analysis.html#cb3508-3"></a><span class="st">  </span><span class="kw">select</span>(id, job, <span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3508-4"><a href="discriminant-analysis.html#cb3508-4"></a><span class="st">  </span><span class="kw">pivot_longer</span>(<span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>), <span class="dt">names_to =</span> <span class="kw">c</span>(<span class="st">&quot;post_job&quot;</span>, <span class="st">&quot;method&quot;</span>), <span class="dt">names_pattern =</span> <span class="st">&quot;posterior</span><span class="ch">\\</span><span class="st">.(.*)</span><span class="ch">\\</span><span class="st">.(.)&quot;</span>, <span class="dt">values_to =</span> <span class="st">&quot;prob&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3508-5"><a href="discriminant-analysis.html#cb3508-5"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from =</span> method, <span class="dt">values_from =</span> prob)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 5
##      id job        post_job       x     y
##   &lt;dbl&gt; &lt;fct&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;
## 1    11 custserv   custserv   0.495 0.49 
## 2    11 custserv   mechanic   0.491 0.496
## 3    11 custserv   dispatcher 0.013 0.014
## 4    42 dispatcher custserv   0.056 0.057
## 5    42 dispatcher mechanic   0.472 0.479
## 6    42 dispatcher dispatcher 0.472 0.464</code></pre>
<p>As I suspected, the posterior probabilities in each case are almost
identical, but different ones happen to be slightly higher in the two
cases. For the first individual (actually in customer service),
cross-validation just tweaked the posterior probabilities enough to
call that individual a mechanic, and for the second one, actually a
dispatcher, the first analysis was almost too close to call, and
things under cross-validation got nudged onto the mechanic side again.</p>
<p>All right, what <em>about</em> those people who got misclassified (say,
by the LDA rather than the cross-validation, since it seems not to
make much difference)?</p>
<p>Let’s count them first:</p>
<div class="sourceCode" id="cb3510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3510-1"><a href="discriminant-analysis.html#cb3510-1"></a>all <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">is_correct =</span> <span class="kw">ifelse</span>(job <span class="op">==</span><span class="st"> </span>class.x, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) -&gt;<span class="st"> </span>all.mis</span>
<span id="cb3510-2"><a href="discriminant-analysis.html#cb3510-2"></a>all.mis <span class="op">%&gt;%</span></span>
<span id="cb3510-3"><a href="discriminant-analysis.html#cb3510-3"></a><span class="st">  </span><span class="kw">count</span>(is_correct <span class="op">==</span><span class="st"> &quot;wrong&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb3510-4"><a href="discriminant-analysis.html#cb3510-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</span></code></pre></div>
<pre><code>##   is_correct == &quot;wrong&quot;   n proportion
## 1                 FALSE 185  0.7581967
## 2                  TRUE  59  0.2418033</code></pre>
<p>24% of them.
There are a lot of them, so we’ll pick a random sample to look at,
rounding the posterior probabilities to 3 decimals first and reducing
the number of columns to look at:</p>
<div class="sourceCode" id="cb3512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3512-1"><a href="discriminant-analysis.html#cb3512-1"></a><span class="kw">set.seed</span>(<span class="dv">457299</span>)</span>
<span id="cb3512-2"><a href="discriminant-analysis.html#cb3512-2"></a>all.mis <span class="op">%&gt;%</span></span>
<span id="cb3512-3"><a href="discriminant-analysis.html#cb3512-3"></a><span class="st">  </span><span class="kw">filter</span>(is_correct <span class="op">==</span><span class="st"> &quot;wrong&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb3512-4"><a href="discriminant-analysis.html#cb3512-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="kw">across</span>(<span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>), <span class="op">~</span><span class="st"> </span><span class="kw">round</span>(., <span class="dv">3</span>))) <span class="op">%&gt;%</span></span>
<span id="cb3512-5"><a href="discriminant-analysis.html#cb3512-5"></a><span class="st">  </span><span class="kw">select</span>(</span>
<span id="cb3512-6"><a href="discriminant-analysis.html#cb3512-6"></a>    id, job, class.x, outdoor, social, conservative,</span>
<span id="cb3512-7"><a href="discriminant-analysis.html#cb3512-7"></a>    <span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>)</span>
<span id="cb3512-8"><a href="discriminant-analysis.html#cb3512-8"></a>  ) <span class="op">%&gt;%</span></span>
<span id="cb3512-9"><a href="discriminant-analysis.html#cb3512-9"></a><span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">15</span>)</span></code></pre></div>
<pre><code>##    id        job    class.x outdoor social conservative posterior.custserv.x
## 1   6   mechanic dispatcher      24      9           17                0.000
## 2  65   mechanic dispatcher      13     16           11                0.138
## 3  61   custserv dispatcher       7     13            7                0.438
## 4  47   custserv   mechanic      15     19            9                0.296
## 5  22   mechanic   custserv      13     20           10                0.470
## 6  53 dispatcher   mechanic      21     15            7                0.026
## 7   4   mechanic   custserv      15     29            8                0.825
## 8  34   mechanic   custserv      17     28           13                0.505
## 9  40   custserv   mechanic      22     22            6                0.095
## 10 46 dispatcher   mechanic      25     16           12                0.004
## 11 42   mechanic dispatcher      22     18           16                0.007
## 12 38   mechanic   custserv      11     25           12                0.829
## 13 86   mechanic   custserv      17     24            5                0.529
## 14 55   custserv   mechanic      22     27           12                0.130
## 15 15   mechanic   custserv      14     28            1                0.923
##    posterior.mechanic.x posterior.dispatcher.x posterior.custserv.y posterior.mechanic.y
## 1                 0.040                  0.960                0.000                0.026
## 2                 0.269                  0.593                0.140                0.259
## 3                 0.109                  0.453                0.392                0.109
## 4                 0.526                  0.178                0.289                0.531
## 5                 0.364                  0.166                0.474                0.357
## 6                 0.775                  0.198                0.027                0.794
## 7                 0.174                  0.002                0.838                0.161
## 8                 0.471                  0.024                0.518                0.458
## 9                 0.889                  0.016                0.083                0.901
## 10                0.675                  0.321                0.004                0.701
## 11                0.416                  0.577                0.006                0.397
## 12                0.138                  0.033                0.839                0.128
## 13                0.464                  0.007                0.542                0.452
## 14                0.850                  0.020                0.116                0.864
## 15                0.077                  0.000                0.943                0.057
##    posterior.dispatcher.y
## 1                   0.974
## 2                   0.601
## 3                   0.499
## 4                   0.180
## 5                   0.168
## 6                   0.179
## 7                   0.002
## 8                   0.024
## 9                   0.016
## 10                  0.295
## 11                  0.597
## 12                  0.033
## 13                  0.007
## 14                  0.020
## 15                  0.000</code></pre>
<p>I put the <code>set.seed</code> in so that this will come out the same
each time I do it, and so that the discussion below always makes sense.</p>
<p>Now we can look at the true and predicted jobs for these people, and
the posterior probabilities (which I rounded earlier).</p>
<ul>
<li>The first one, id 6, is badly wrong; this was actually a mechanic, but the posterior probabilities say that it is a near-certain dispatcher.</li>
<li>The second one, id 65, is a little better, but the posterior probability of actually being a mechanic is only 0.269; the probability of being a dispatcher is much higher at 0.593, so that’s what it gets classified as.</li>
<li>The third one, though, id #61, is a very close call: posterior
probability 0.438 of being in customer service (correct), 0.453 of
being a dispatcher, only slightly higher, but enough to make the
prediction wrong.</li>
</ul>
<p>The implication from looking at our sample of 15 people is that some
of them are “badly” misclassified (with a high posterior probability
of having a different job from the one they actually hold), but a lot
of them came out on the wrong end of a close call. This suggests that
a number of the correct classifications came out <em>right</em> almost
by chance as well, with (hypothesizing) two close posterior
probabilities of which their actual job came out slightly higher.</p>
<p>Further further analysis would look at the original variables
<code>social</code>, <code>outdoor</code> and <code>conservative</code> for the
misclassified people, and try to find out what was unusual about
them. But I think now would be an excellent place for me to stop.</p>
</div>
<div id="observing-children-with-adhd" class="section level2" number="26.5">
<h2><span class="header-section-number">26.5</span> Observing children with ADHD</h2>
<p>A number of children with ADHD were observed by their mother
or their father (only one parent observed each child). Each parent was
asked to rate occurrences of behaviours of four different types,
labelled <code>q1</code> through <code>q4</code> in the data set. Also
recorded was the identity of the parent doing the observation for each
child: 1 is father, 2 is mother.</p>
<p>Can we tell (without looking at the <code>parent</code> column) which
parent is doing the observation? Research suggests that rating the
degree of impairment in different categories depends on who is doing
the rating: for example, mothers may feel that a child has difficulty
sitting still, while fathers, who might do more observing of a child
at play, might think of such a child as simply being “active” or
“just being a kid”. The data are in
<a href="http://www.utsc.utoronto.ca/~butler/d29/adhd-parents.txt">link</a>.</p>
<ol style="list-style-type: lower-alpha">
<li>Read in the data and confirm that you have four ratings and
a column labelling the parent who made each observation.</li>
</ol>
<p>Solution</p>
<p>As ever:</p>
<div class="sourceCode" id="cb3514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3514-1"><a href="discriminant-analysis.html#cb3514-1"></a>my_url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/nxskok/datafiles/master/adhd-parents.txt&quot;</span></span>
<span id="cb3514-2"><a href="discriminant-analysis.html#cb3514-2"></a>adhd &lt;-<span class="st"> </span><span class="kw">read_delim</span>(my_url, <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## 
## ── Column specification ──────────────────────────────────────────────────────────────────
## cols(
##   parent = col_character(),
##   q1 = col_double(),
##   q2 = col_double(),
##   q3 = col_double(),
##   q4 = col_double()
## )</code></pre>
<div class="sourceCode" id="cb3516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3516-1"><a href="discriminant-analysis.html#cb3516-1"></a>adhd</span></code></pre></div>
<pre><code>## # A tibble: 29 x 5
##    parent    q1    q2    q3    q4
##    &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 father     2     1     3     1
##  2 mother     1     3     1     1
##  3 father     2     1     3     1
##  4 mother     3     2     3     3
##  5 mother     3     3     2     1
##  6 mother     1     3     3     1
##  7 mother     3     3     1     1
##  8 mother     2     3     1     1
##  9 mother     1     3     3     1
## 10 mother     3     3     3     3
## # … with 19 more rows</code></pre>
<p>Yes, exactly that.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Run a suitable discriminant analysis and display the output.</li>
</ol>
<p>Solution</p>
<p>This is as before:</p>
<div class="sourceCode" id="cb3518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3518-1"><a href="discriminant-analysis.html#cb3518-1"></a>adhd<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(parent <span class="op">~</span><span class="st"> </span>q1 <span class="op">+</span><span class="st"> </span>q2 <span class="op">+</span><span class="st"> </span>q3 <span class="op">+</span><span class="st"> </span>q4, <span class="dt">data =</span> adhd)</span>
<span id="cb3518-2"><a href="discriminant-analysis.html#cb3518-2"></a>adhd<span class="fl">.1</span></span></code></pre></div>
<pre><code>## Call:
## lda(parent ~ q1 + q2 + q3 + q4, data = adhd)
## 
## Prior probabilities of groups:
##    father    mother 
## 0.1724138 0.8275862 
## 
## Group means:
##           q1       q2       q3    q4
## father 1.800 1.000000 1.800000 1.800
## mother 2.375 2.791667 1.958333 1.625
## 
## Coefficients of linear discriminants:
##           LD1
## q1 -0.3223454
## q2  2.3219448
## q3  0.1411360
## q4  0.1884613</code></pre>
<ol start="3" style="list-style-type: lower-alpha">
<li>Which behaviour item or items seem to be most helpful at
distinguishing the parent making the observations? Explain briefly.</li>
</ol>
<p>Solution</p>
<p>Look at the Coefficients of Linear Discriminants. The coefficient
of <code>q2</code>, 2.32, is much larger in size than the others, so
it’s really <code>q2</code> that distinguishes mothers and fathers.
Note also that the group means for fathers and mothers are fairly
close on all the items except for <code>q2</code>, which are a long
way apart. So that’s another hint that it might be <code>q2</code>
that makes the difference. But that might be deceiving: one of the
other <code>q</code>s, even though the means are close for mothers and
fathers, might actually do a good job of distinguishing mothers
from fathers, because it has a small SD overall.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>Obtain the predictions from the <code>lda</code>, and make a
suitable plot of the discriminant scores, bearing in mind that you
only have one <code>LD</code>. Do you think there will be any
misclassifications? Explain briefly.</li>
</ol>
<p>Solution</p>
<p>The prediction is the obvious thing. I take a quick look at it
(using <code>glimpse</code>), but only because I feel like it:</p>
<div class="sourceCode" id="cb3520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3520-1"><a href="discriminant-analysis.html#cb3520-1"></a>adhd<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(adhd<span class="fl">.1</span>)</span>
<span id="cb3520-2"><a href="discriminant-analysis.html#cb3520-2"></a><span class="kw">glimpse</span>(adhd<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## List of 3
##  $ class    : Factor w/ 2 levels &quot;father&quot;,&quot;mother&quot;: 1 2 1 2 2 2 2 2 2 2 ...
##  $ posterior: num [1:29, 1:2] 9.98e-01 5.57e-06 9.98e-01 4.97e-02 4.10e-05 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:29] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:2] &quot;father&quot; &quot;mother&quot;
##  $ x        : num [1:29, 1] -3.327 1.357 -3.327 -0.95 0.854 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:29] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr &quot;LD1&quot;</code></pre>
<p>The discriminant scores are in the thing called <code>x</code> in
there. There is only <code>LD1</code> (only two groups, mothers and
fathers), so the right way to plot it is against the true groups, eg.
by a boxplot, first making a data frame, using <code>data.frame</code>,
containing what you need:</p>
<div class="sourceCode" id="cb3522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3522-1"><a href="discriminant-analysis.html#cb3522-1"></a>d &lt;-<span class="st"> </span><span class="kw">cbind</span>(adhd, adhd<span class="fl">.2</span>)</span>
<span id="cb3522-2"><a href="discriminant-analysis.html#cb3522-2"></a><span class="kw">head</span>(d)</span></code></pre></div>
<pre><code>##   parent q1 q2 q3 q4  class posterior.father posterior.mother        LD1
## 1 father  2  1  3  1 father     9.984540e-01      0.001545972 -3.3265660
## 2 mother  1  3  1  1 mother     5.573608e-06      0.999994426  1.3573971
## 3 father  2  1  3  1 father     9.984540e-01      0.001545972 -3.3265660
## 4 mother  3  2  3  3 mother     4.971864e-02      0.950281356 -0.9500439
## 5 mother  3  3  2  1 mother     4.102507e-05      0.999958975  0.8538422
## 6 mother  1  3  3  1 mother     1.820430e-06      0.999998180  1.6396690</code></pre>
<div class="sourceCode" id="cb3524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3524-1"><a href="discriminant-analysis.html#cb3524-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> parent, <span class="dt">y =</span> LD1)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_boxplot</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1762-1.png" width="672"  /></p>
<p>The fathers look to be a very compact group with <code>LD1</code> score
around <span class="math inline">\(-3\)</span>, so I don’t foresee any problems there. The mothers, on
the other hand, have outliers: there is one with <code>LD1</code> score
beyond <span class="math inline">\(-3\)</span> that will certainly be mistaken for a father. There are a
couple of other unusual <code>LD1</code> scores among the mothers, but a
rule like
“anything above <span class="math inline">\(-2\)</span> is called a mother, anything below is called a father”
will get these two right. So I expect that the one
very low mother will get misclassified, but that’s the only one.</p>
<ol start="5" style="list-style-type: lower-alpha">
<li>Obtain the predicted group memberships and make a table of
actual vs. predicted. Were there any misclassifications? Explain
briefly.</li>
</ol>
<p>Solution</p>
<p>Use the predictions from the previous part, and the observed
<code>parent</code> values from the original data frame. Then use
either <code>table</code> or <code>tidyverse</code> to summarize.</p>
<div class="sourceCode" id="cb3525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3525-1"><a href="discriminant-analysis.html#cb3525-1"></a><span class="kw">with</span>(d, <span class="kw">table</span>(<span class="dt">obs =</span> parent, <span class="dt">pred =</span> class))</span></code></pre></div>
<pre><code>##         pred
## obs      father mother
##   father      5      0
##   mother      1     23</code></pre>
<p>Or,</p>
<div class="sourceCode" id="cb3527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3527-1"><a href="discriminant-analysis.html#cb3527-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(parent, class)</span></code></pre></div>
<pre><code>##   parent  class  n
## 1 father father  5
## 2 mother father  1
## 3 mother mother 23</code></pre>
<p>or</p>
<div class="sourceCode" id="cb3529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3529-1"><a href="discriminant-analysis.html#cb3529-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3529-2"><a href="discriminant-analysis.html#cb3529-2"></a><span class="st">  </span><span class="kw">count</span>(parent, class) <span class="op">%&gt;%</span></span>
<span id="cb3529-3"><a href="discriminant-analysis.html#cb3529-3"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from=</span>class, <span class="dt">values_from=</span>n, <span class="dt">values_fill =</span> <span class="kw">list</span>(<span class="dt">n=</span><span class="dv">0</span>))</span></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   parent father mother
##   &lt;chr&gt;   &lt;int&gt;  &lt;int&gt;
## 1 father      5      0
## 2 mother      1     23</code></pre>
<p>One of the mothers got classified as a father (evidently that one with
a very negative <code>LD1</code> score), but everything else is correct.</p>
<p>This time, by “explain briefly” I mean something like
“tell me how you know there are or are not misclassifications”, or
“describe any misclassifications that occur” or something like that.</p>
<p>Extra: I was curious — what is it about that one mother that caused
her to get misclassified? (I didn’t ask you to think further about
this, but in case you are curious as well.)</p>
<p>First, which mother <em>was</em> it? Let’s begin by adding the predicted
classification to the data frame, and then we can query it by asking
to see only the rows where the actual parent and the predicted parent
were different. I’m also going to create a column <code>id</code> that
will give us the row of the <em>original</em> data frame:</p>
<div class="sourceCode" id="cb3531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3531-1"><a href="discriminant-analysis.html#cb3531-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3531-2"><a href="discriminant-analysis.html#cb3531-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">id =</span> <span class="kw">row_number</span>()) <span class="op">%&gt;%</span></span>
<span id="cb3531-3"><a href="discriminant-analysis.html#cb3531-3"></a><span class="st">  </span><span class="kw">filter</span>(parent <span class="op">!=</span><span class="st"> </span>class)</span></code></pre></div>
<pre><code>##    parent q1 q2 q3 q4  class posterior.father posterior.mother       LD1 id
## 17 mother  1  1  2  1 father        0.9968343      0.003165699 -3.145357 17</code></pre>
<p>It was the original row 17. So what was unusual about this? We know
from earlier
that behaviour <code>q2</code> was the one that generally distinguished
mothers from fathers, so maybe we should find the mean and SD of scores for
mothers and fathers on <code>q2</code>:</p>
<div class="sourceCode" id="cb3533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3533-1"><a href="discriminant-analysis.html#cb3533-1"></a>adhd <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(parent) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="dt">m2 =</span> <span class="kw">mean</span>(q2), <span class="dt">s2 =</span> <span class="kw">sd</span>(q2))</span></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   parent    m2    s2
##   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 father  1    0    
## 2 mother  2.79 0.509</code></pre>
<p>The fathers’ scores on <code>q2</code> were <em>all</em> 1, but the mothers’
scores on <code>q2</code> were on average much higher. So it’s not really
a surprise that this mother was mistaken for a father.</p>
<ol start="6" style="list-style-type: lower-alpha">
<li>Re-run the discriminant analysis using cross-validation,
and again obtain a table of actual and predicted parents. Is the
pattern of misclassification different from before? Hints: (i) Bear in mind
that there is no <code>predict</code> step this time, because the
cross-validation output includes predictions; (ii) use a different name
for the predictions this time because we are going to do a
comparison in a moment.</li>
</ol>
<p>Solution</p>
<p>So, this, with different name:</p>
<div class="sourceCode" id="cb3535"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3535-1"><a href="discriminant-analysis.html#cb3535-1"></a>adhd<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(parent <span class="op">~</span><span class="st"> </span>q1 <span class="op">+</span><span class="st"> </span>q2 <span class="op">+</span><span class="st"> </span>q3 <span class="op">+</span><span class="st"> </span>q4, <span class="dt">data =</span> adhd, <span class="dt">CV =</span> T)</span>
<span id="cb3535-2"><a href="discriminant-analysis.html#cb3535-2"></a>dd &lt;-<span class="st"> </span><span class="kw">cbind</span>(adhd, <span class="dt">class =</span> adhd<span class="fl">.3</span><span class="op">$</span>class, <span class="dt">posterior =</span> adhd<span class="fl">.3</span><span class="op">$</span>posterior)</span>
<span id="cb3535-3"><a href="discriminant-analysis.html#cb3535-3"></a><span class="kw">with</span>(dd, <span class="kw">table</span>(parent, class))</span></code></pre></div>
<pre><code>##         class
## parent   father mother
##   father      5      0
##   mother      1     23</code></pre>
<p>It’s exactly the same pattern of misclassification. (In fact, it’s
exactly the same mother being misclassified as a father.)</p>
<p>This one is the same <em>not</em> because of having lots of data. In
fact, as you see below, having a small data set makes quite a bit of
difference to the posterior probabilities (where they are not close to
1 or 0), but the decisions about whether the parents are a mother or a
father are clear-cut enough that none of <em>those</em> change. Even
though (some of) the posterior probabilities are noticeably changed,
which one is the bigger has not changed at all.</p>
<ol start="7" style="list-style-type: lower-alpha">
<li>Display the original data (that you read in from the data
file) side by side with two sets of posterior probabilities: the
ones that you obtained with <code>predict</code> before, and the ones
from the cross-validated analysis. Comment briefly on whether the
two sets of posterior probabilities are similar. Hints: (i) use
<code>data.frame</code> rather than <code>cbind</code>, for reasons that I
explain elsewhere; (ii) round the posterior probabilities to 3
decimals before you display them.
There are only 29 rows, so look at them all. I am going to add the
<code>LD1</code> scores to my output and sort by that, but you don’t
need to. (This is for something I am going to add later.)</li>
</ol>
<p>Solution</p>
<p>We have two data frames, <code>d</code> and
<code>dd</code>
<label for="tufte-mn-228" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-228" class="margin-toggle"><span class="marginnote">I have to learn to come up with better names.</span><br />
that respectively
contain everything from the (original) <code>lda</code> output and the
cross-validated output. Let’s glue them together, look at what we
have, and then pull out what we need:</p>
<div class="sourceCode" id="cb3537"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3537-1"><a href="discriminant-analysis.html#cb3537-1"></a>all &lt;-<span class="st"> </span><span class="kw">data.frame</span>(d, dd)</span>
<span id="cb3537-2"><a href="discriminant-analysis.html#cb3537-2"></a><span class="kw">head</span>(all)</span></code></pre></div>
<pre><code>##   parent q1 q2 q3 q4  class posterior.father posterior.mother        LD1 parent.1 q1.1
## 1 father  2  1  3  1 father     9.984540e-01      0.001545972 -3.3265660   father    2
## 2 mother  1  3  1  1 mother     5.573608e-06      0.999994426  1.3573971   mother    1
## 3 father  2  1  3  1 father     9.984540e-01      0.001545972 -3.3265660   father    2
## 4 mother  3  2  3  3 mother     4.971864e-02      0.950281356 -0.9500439   mother    3
## 5 mother  3  3  2  1 mother     4.102507e-05      0.999958975  0.8538422   mother    3
## 6 mother  1  3  3  1 mother     1.820430e-06      0.999998180  1.6396690   mother    1
##   q2.1 q3.1 q4.1 class.1 posterior.father.1 posterior.mother.1
## 1    1    3    1  father       9.958418e-01        0.004158233
## 2    3    1    1  mother       5.036602e-06        0.999994963
## 3    1    3    1  father       9.958418e-01        0.004158233
## 4    2    3    3  mother       2.359247e-01        0.764075258
## 5    3    2    1  mother       5.702541e-05        0.999942975
## 6    3    3    1  mother       8.430421e-07        0.999999157</code></pre>
<p>The ones with a 1 on the end are the cross-validated ones. We need the posterior probabilities, rounded, and they need to have shorter names:</p>
<div class="sourceCode" id="cb3539"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3539-1"><a href="discriminant-analysis.html#cb3539-1"></a>all <span class="op">%&gt;%</span></span>
<span id="cb3539-2"><a href="discriminant-analysis.html#cb3539-2"></a><span class="st">  </span><span class="kw">select</span>(parent, <span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>), LD1) <span class="op">%&gt;%</span></span>
<span id="cb3539-3"><a href="discriminant-analysis.html#cb3539-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="kw">across</span>(<span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>), <span class="op">~</span><span class="st"> </span><span class="kw">round</span>(., <span class="dv">3</span>))) <span class="op">%&gt;%</span></span>
<span id="cb3539-4"><a href="discriminant-analysis.html#cb3539-4"></a><span class="st">  </span><span class="kw">rename_with</span>(</span>
<span id="cb3539-5"><a href="discriminant-analysis.html#cb3539-5"></a>    <span class="op">~</span><span class="st"> </span><span class="kw">str_replace</span>(., <span class="st">&quot;posterior&quot;</span>, <span class="st">&quot;p&quot;</span>),</span>
<span id="cb3539-6"><a href="discriminant-analysis.html#cb3539-6"></a>    <span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>),</span>
<span id="cb3539-7"><a href="discriminant-analysis.html#cb3539-7"></a>  ) <span class="op">%&gt;%</span></span>
<span id="cb3539-8"><a href="discriminant-analysis.html#cb3539-8"></a><span class="st">  </span><span class="kw">arrange</span>(LD1)</span></code></pre></div>
<pre><code>##    parent p.father p.mother p.father.1 p.mother.1        LD1
## 19 father    0.999    0.001      0.999      0.001 -3.6088379
## 1  father    0.998    0.002      0.996      0.004 -3.3265660
## 3  father    0.998    0.002      0.996      0.004 -3.3265660
## 27 father    0.998    0.002      0.994      0.006 -3.2319152
## 17 mother    0.997    0.003      1.000      0.000 -3.1453565
## 16 father    0.992    0.008      0.958      0.042 -2.9095698
## 4  mother    0.050    0.950      0.236      0.764 -0.9500439
## 23 mother    0.043    0.957      0.107      0.893 -0.9099704
## 13 mother    0.015    0.985      0.030      0.970 -0.6349504
## 7  mother    0.000    1.000      0.000      1.000  0.7127063
## 20 mother    0.000    1.000      0.000      1.000  0.7127063
## 21 mother    0.000    1.000      0.000      1.000  0.7127063
## 5  mother    0.000    1.000      0.000      1.000  0.8538422
## 14 mother    0.000    1.000      0.000      1.000  0.8538422
## 12 mother    0.000    1.000      0.000      1.000  0.9011676
## 15 mother    0.000    1.000      0.000      1.000  0.9949782
## 18 mother    0.000    1.000      0.000      1.000  0.9949782
## 22 mother    0.000    1.000      0.000      1.000  0.9949782
## 28 mother    0.000    1.000      0.000      1.000  0.9949782
## 8  mother    0.000    1.000      0.000      1.000  1.0350517
## 29 mother    0.000    1.000      0.000      1.000  1.0350517
## 11 mother    0.000    1.000      0.000      1.000  1.2307649
## 24 mother    0.000    1.000      0.000      1.000  1.2307649
## 25 mother    0.000    1.000      0.000      1.000  1.2307649
## 2  mother    0.000    1.000      0.000      1.000  1.3573971
## 10 mother    0.000    1.000      0.000      1.000  1.3719009
## 26 mother    0.000    1.000      0.000      1.000  1.5458584
## 6  mother    0.000    1.000      0.000      1.000  1.6396690
## 9  mother    0.000    1.000      0.000      1.000  1.6396690</code></pre>
<p>The <code>rename</code> changes the names of the columns that start
with <code>posterior</code> to start with <code>p</code> instead (shorter). I
learned about this today (having wondered whether it existed or not),
and it took about three goes for me to get it right.
<label for="tufte-mn-229" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-229" class="margin-toggle"><span class="marginnote"><em>str-replace</em> is from <em>stringr</em>, and takes three inputs: a piece of text, the text to look for, and the text to replace it with. The piece of text in this case is one of the columns whose name starts with <em>posterior</em>; the dot represents <em>it</em> in the usual fashion.</span>
The first column is the actual parent; the other five columns are: the
posterior probabilities from before, for father and for mother (two
columns), and the posterior probabilities from cross-validation for
father and for mother (two more columns), and the LD1 scores from
before, sorted into order. You might have these the other way around
from me, but in any case you ought to make it clear which is which. I
included the <code>LD1</code> scores for my discussion below; you don’t
need to.
Are the two sets of posterior probabilities similar? Only kinda. The
ones at the top and bottom of the list are without doubt respectively
fathers at the top of the list (top 5 rows on my sorted output, except that
one of those is actually a mother), or mothers at the bottom, from row
10 down. But for rows 6 through 9, the posterior probabilities are not
that similar. The most dissimilar ones are in row 4, where the
regular <code>lda</code> gives a posterior probability of 0.050 that the
parent is a father, but under cross-validation that goes all the way
up to 0.236. I think this is one of those mothers that is a bit like a
father: her score on <code>q2</code> was only 2, compared to 3 for most of
the mothers. If you take out this mother, as cross-validation does,
there are noticeably fewer <code>q2=2</code> mothers left, so the
observation looks more like a father than it would otherwise.</p>
<ol start="8" style="list-style-type: lower-alpha">
<li>Row 17 of your (original) data frame above, row 5 of the
output in the previous part, is the mother that was
misclassified as a father. Why is it that the cross-validated
posterior probabilities are 1 and 0, while the previous posterior
probabilities are a bit less than 1 and a bit more than 0?</li>
</ol>
<p>Solution</p>
<p>In making the classification, the non-cross-validated procedure
uses all the data, so that parent #17 suggests that the mothers are
very variable on <code>q2</code>, so it is conceivable (though still
unlikely) that this parent actually is a mother.
Under cross-validation, however, parent #17 is
<em>omitted</em>. This mother is nothing like any of the other
mothers, or, to put it another way, the remaining mothers as a
group are very far away from this one, so #17 doesn’t look like a
mother <em>at all</em>.</p>
<ol style="list-style-type: lower-roman">
<li>Find the parents where the cross-validated posterior
probability of being a father is “non-trivial”: that is, not
close to zero and not close to 1. (You will have to make a judgement
about what “close to zero or 1” means for you.) What do these
parents have in common, all of them or most of them?</li>
</ol>
<p>Solution</p>
<p>Let’s add something to the output we had before: the original
scores on <code>q1</code> through <code>q4</code>:</p>
<div class="sourceCode" id="cb3541"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3541-1"><a href="discriminant-analysis.html#cb3541-1"></a>all <span class="op">%&gt;%</span></span>
<span id="cb3541-2"><a href="discriminant-analysis.html#cb3541-2"></a><span class="st">  </span><span class="kw">select</span>(q1<span class="op">:</span>q4, parent, <span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>), LD1) <span class="op">%&gt;%</span></span>
<span id="cb3541-3"><a href="discriminant-analysis.html#cb3541-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="kw">across</span>(<span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>), <span class="op">~</span><span class="st"> </span><span class="kw">round</span>(., <span class="dv">3</span>))) <span class="op">%&gt;%</span></span>
<span id="cb3541-4"><a href="discriminant-analysis.html#cb3541-4"></a><span class="st">  </span><span class="kw">rename_with</span>(</span>
<span id="cb3541-5"><a href="discriminant-analysis.html#cb3541-5"></a>    <span class="op">~</span><span class="st"> </span><span class="kw">str_replace</span>(., <span class="st">&quot;posterior&quot;</span>, <span class="st">&quot;p&quot;</span>),</span>
<span id="cb3541-6"><a href="discriminant-analysis.html#cb3541-6"></a>    <span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>),</span>
<span id="cb3541-7"><a href="discriminant-analysis.html#cb3541-7"></a>  ) <span class="op">%&gt;%</span></span>
<span id="cb3541-8"><a href="discriminant-analysis.html#cb3541-8"></a><span class="st">  </span><span class="kw">arrange</span>(LD1)</span></code></pre></div>
<pre><code>##    q1 q2 q3 q4 parent p.father p.mother p.father.1 p.mother.1        LD1
## 19  2  1  1  1 father    0.999    0.001      0.999      0.001 -3.6088379
## 1   2  1  3  1 father    0.998    0.002      0.996      0.004 -3.3265660
## 3   2  1  3  1 father    0.998    0.002      0.996      0.004 -3.3265660
## 27  2  1  1  3 father    0.998    0.002      0.994      0.006 -3.2319152
## 17  1  1  2  1 mother    0.997    0.003      1.000      0.000 -3.1453565
## 16  1  1  1  3 father    0.992    0.008      0.958      0.042 -2.9095698
## 4   3  2  3  3 mother    0.050    0.950      0.236      0.764 -0.9500439
## 23  2  2  1  3 mother    0.043    0.957      0.107      0.893 -0.9099704
## 13  1  2  2  2 mother    0.015    0.985      0.030      0.970 -0.6349504
## 7   3  3  1  1 mother    0.000    1.000      0.000      1.000  0.7127063
## 20  3  3  1  1 mother    0.000    1.000      0.000      1.000  0.7127063
## 21  3  3  1  1 mother    0.000    1.000      0.000      1.000  0.7127063
## 5   3  3  2  1 mother    0.000    1.000      0.000      1.000  0.8538422
## 14  3  3  2  1 mother    0.000    1.000      0.000      1.000  0.8538422
## 12  3  3  1  2 mother    0.000    1.000      0.000      1.000  0.9011676
## 15  3  3  3  1 mother    0.000    1.000      0.000      1.000  0.9949782
## 18  3  3  3  1 mother    0.000    1.000      0.000      1.000  0.9949782
## 22  3  3  3  1 mother    0.000    1.000      0.000      1.000  0.9949782
## 28  3  3  3  1 mother    0.000    1.000      0.000      1.000  0.9949782
## 8   2  3  1  1 mother    0.000    1.000      0.000      1.000  1.0350517
## 29  2  3  1  1 mother    0.000    1.000      0.000      1.000  1.0350517
## 11  3  3  2  3 mother    0.000    1.000      0.000      1.000  1.2307649
## 24  3  3  2  3 mother    0.000    1.000      0.000      1.000  1.2307649
## 25  3  3  2  3 mother    0.000    1.000      0.000      1.000  1.2307649
## 2   1  3  1  1 mother    0.000    1.000      0.000      1.000  1.3573971
## 10  3  3  3  3 mother    0.000    1.000      0.000      1.000  1.3719009
## 26  1  3  1  2 mother    0.000    1.000      0.000      1.000  1.5458584
## 6   1  3  3  1 mother    0.000    1.000      0.000      1.000  1.6396690
## 9   1  3  3  1 mother    0.000    1.000      0.000      1.000  1.6396690</code></pre>
<p>To my mind, the “non-trivial” posterior probabilities are in rows 5
through 9. (You might have drawn the line in a different place.) These
are the ones where there was some doubt, though maybe only a little,
about which parent actually gave the ratings. For three of these,
the parent (that was actually a mother) gave a rating of
2 on <code>q2</code>. These were the only 2’s on <code>q2</code>. The others
were easy to call: “mother” if 3 and “father” if 1, and you’d get
them all right except for that outlying mother.
The clue in looking at <code>q2</code> was that we found earlier that
<code>LD1</code> contained mostly <code>q2</code>, so that it was mainly
<code>q2</code> that separated the fathers and mothers. If you found
something else that the “non-trivial” rows had in common, that is
good too, but I think looking at <code>q2</code> is your quickest route to
an answer. (<code>q1=1</code> picks out some of these, but not all of
them.)
This is really the same kind of issue as we discussed when
comparing the posterior probabilities for <code>lda</code> and
cross-validation above: there were only a few parents with
<code>q2=2</code>, so the effect there is that under cross-validation,
there are even fewer when you take one of them out.</p>
</div>
<div id="growing-corn" class="section level2" number="26.6">
<h2><span class="header-section-number">26.6</span> Growing corn</h2>
<p>A new type of corn seed has been developed.
The people developing it want to know if the type of soil the seed
is planted in has an impact on how well the seed performs, and if so,
what kind of impact. Three
outcome measures were used: the yield of corn produced (from a fixed
amount of seed), the amount of water needed, and the amount of
herbicide needed. The data are in
<a href="http://www.utsc.utoronto.ca/~butler/d29/cornseed.csv">link</a>. 32 fields
were planted with the seed, 8 fields with each soil type.</p>
<ol style="list-style-type: lower-alpha">
<li>Read in the data and verify that you have 32 observations
with the correct variables.</li>
</ol>
<p>Solution</p>
<p>The usual:</p>
<div class="sourceCode" id="cb3543"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3543-1"><a href="discriminant-analysis.html#cb3543-1"></a>my_url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/nxskok/datafiles/master/cornseed.csv&quot;</span></span>
<span id="cb3543-2"><a href="discriminant-analysis.html#cb3543-2"></a>cornseed &lt;-<span class="st"> </span><span class="kw">read_csv</span>(my_url)</span></code></pre></div>
<pre><code>## 
## ── Column specification ──────────────────────────────────────────────────────────────────
## cols(
##   field = col_double(),
##   soil = col_character(),
##   yield = col_double(),
##   water = col_double(),
##   herbicide = col_double()
## )</code></pre>
<div class="sourceCode" id="cb3545"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3545-1"><a href="discriminant-analysis.html#cb3545-1"></a>cornseed</span></code></pre></div>
<pre><code>## # A tibble: 32 x 5
##    field soil  yield water herbicide
##    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
##  1     1 loam   76.7  29.5       7.5
##  2     2 loam   60.5  32.1       6.3
##  3     3 loam   96.1  40.7       4.2
##  4     4 loam   88.1  45.1       4.9
##  5     5 loam   50.2  34.1      11.7
##  6     6 loam   55    31.1       6.9
##  7     7 loam   65.4  21.6       4.3
##  8     8 loam   65.7  27.7       5.3
##  9     9 sandy  67.3  48.3       5.5
## 10    10 sandy  61.3  28.9       6.9
## # … with 22 more rows</code></pre>
<p>We have 32 rows; we have a categorical soil type, three
numerical columns containing the yield, water and herbicide values,
and we also have a label for each of the 32 fields (which is actually
a number, but we don’t have to worry about that, since we won’t be
using <code>field</code> for anything).</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Run a multivariate analysis of variance to see whether
the type of soil has any effect on any of the variables. What do you
conclude from it?</li>
</ol>
<p>Solution</p>
<p>The usual thing: create the response, use <code>manova</code> (or
<code>Manova</code> from <code>car</code> if you like, but it’s not necessary):</p>
<div class="sourceCode" id="cb3547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3547-1"><a href="discriminant-analysis.html#cb3547-1"></a>response &lt;-<span class="st"> </span><span class="kw">with</span>(cornseed, <span class="kw">cbind</span>(yield, water, herbicide))</span>
<span id="cb3547-2"><a href="discriminant-analysis.html#cb3547-2"></a>cornseed<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">manova</span>(response <span class="op">~</span><span class="st"> </span>soil, <span class="dt">data =</span> cornseed)</span>
<span id="cb3547-3"><a href="discriminant-analysis.html#cb3547-3"></a><span class="kw">summary</span>(cornseed<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>##           Df Pillai approx F num Df den Df  Pr(&gt;F)  
## soil       3 0.5345   2.0234      9     84 0.04641 *
## Residuals 28                                        
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>With a P-value (just) less than 0.05, soil type has some effect on the
response variables: that is, it affects one or more of the three
responses, or some combination of them. ANOVA conclusions are usually
vague, and MANOVA conclusions are vaguer than most. We will try to
improve on this. But with an only-just-significant P-value, we should
not be expecting miracles.</p>
<p>Here and below, <code>field</code> is neither a response variable nor an
explanatory variable; it is an experimental unit, so <code>field</code>
acts as an ID rather than anything else. So <code>field</code> should not
be part of any of the analyses; if it did appear, the only way it
could is as a factor, for example if this was somehow a repeated
measures analysis over the three response variables. In that case,
<code>lmer</code>, if you were going that way, would use <code>field</code> as
a random effect.</p>
<p>The variables to include are the
yield, water and herbicide as measured response variables, and soil
type, as the categorical explanatory variable. (For the discriminant
analysis, these get turned around: the grouping variable <code>soil</code>
acts like a response and the others act as explanatory.)</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Run a discriminant analysis on these data, “predicting”
soil type from the three response variables. Display the results.</li>
</ol>
<p>Solution</p>
<div class="sourceCode" id="cb3549"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3549-1"><a href="discriminant-analysis.html#cb3549-1"></a>cornseed<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(soil <span class="op">~</span><span class="st"> </span>yield <span class="op">+</span><span class="st"> </span>water <span class="op">+</span><span class="st"> </span>herbicide, <span class="dt">data =</span> cornseed)</span>
<span id="cb3549-2"><a href="discriminant-analysis.html#cb3549-2"></a>cornseed<span class="fl">.2</span></span></code></pre></div>
<pre><code>## Call:
## lda(soil ~ yield + water + herbicide, data = cornseed)
## 
## Prior probabilities of groups:
##  clay  loam salty sandy 
##  0.25  0.25  0.25  0.25 
## 
## Group means:
##         yield   water herbicide
## clay  58.8375 33.0875    4.0875
## loam  69.7125 32.7375    6.3875
## salty 55.3125 30.6375    3.8625
## sandy 62.5750 28.2000    4.3500
## 
## Coefficients of linear discriminants:
##                   LD1         LD2         LD3
## yield      0.08074845  0.02081174 -0.04822432
## water     -0.03759961 -0.09598577 -0.03231897
## herbicide  0.50654017 -0.06979662  0.27281743
## 
## Proportion of trace:
##    LD1    LD2    LD3 
## 0.9487 0.0456 0.0057</code></pre>
<p>No <code>field</code> in here, for reasons discussed above. (I’m not even
sure how you <em>can</em> run a discriminant analysis with a factor
variable on the right of the squiggle.) The fields were numbered by
soil type:</p>
<div class="sourceCode" id="cb3551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3551-1"><a href="discriminant-analysis.html#cb3551-1"></a>cornseed <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(field, soil)</span></code></pre></div>
<pre><code>## # A tibble: 32 x 2
##    field soil 
##    &lt;dbl&gt; &lt;chr&gt;
##  1     1 loam 
##  2     2 loam 
##  3     3 loam 
##  4     4 loam 
##  5     5 loam 
##  6     6 loam 
##  7     7 loam 
##  8     8 loam 
##  9     9 sandy
## 10    10 sandy
## # … with 22 more rows</code></pre>
<p>so evidently if you know the field number you can guess the field
type, but we didn’t care about that: we cared about whether you can
distinguish the fields by yield, water, herbicide or combination
thereof.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li><a name="part:corn-svd">*</a>
Which linear discriminants seem to be worth paying attention to?
Why did you get three linear discriminants? Explain briefly.</li>
</ol>
<p>Solution</p>
<p>Look for “proportion of trace” in the output.</p>
<p>The first one is <em>way</em> bigger than the others, which says that
the first linear discriminant is way more important (at separating the
groups) than either of the other two.</p>
<p>As to why we got three: there are 3 variables and 4 groups (soil
types), and the smaller of 3 and <span class="math inline">\(4-1\)</span> is 3.</p>
<ol start="5" style="list-style-type: lower-alpha">
<li>Which response variables do the important linear
discriminants depend on? Answer this by extracting something from
your discriminant analysis output.</li>
</ol>
<p>Solution</p>
<p>The table “coefficients of linear discriminants”.
We said earlier that the only important discriminant is
<code>LD1</code>. On that, the only notably non-zero coefficient is for
<code>herbicide</code>; the ones for <code>yield</code> and <code>water</code> are
close to zero. That is to say, the effects of the soil types play out
through herbicide and not either of the other two variables.</p>
<p>I didn’t ask you to, but you could check this by seeing how
<code>herbicide</code> differs according to soil type:</p>
<div class="sourceCode" id="cb3553"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3553-1"><a href="discriminant-analysis.html#cb3553-1"></a><span class="kw">ggplot</span>(cornseed, <span class="kw">aes</span>(<span class="dt">x =</span> soil, <span class="dt">y =</span> herbicide)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_boxplot</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1775-1.png" width="672"  /></p>
<p>The fields in <code>loam</code> soil needed more herbicide than the others.</p>
<p>Or by <code>water</code>:</p>
<div class="sourceCode" id="cb3554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3554-1"><a href="discriminant-analysis.html#cb3554-1"></a><span class="kw">ggplot</span>(cornseed, <span class="kw">aes</span>(<span class="dt">x =</span> soil, <span class="dt">y =</span> water)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_boxplot</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1776-1.png" width="672"  /></p>
<p>There isn’t much difference in the amount of water needed between any
of the fields, no matter what soil type.</p>
<p>This confirms that <code>water</code> is not distinguished by soil type,
while <code>herbicide</code> is (at least to some extent).</p>
<ol start="6" style="list-style-type: lower-alpha">
<li>Obtain predictions for the discriminant analysis. (You
don’t need to do anything with them yet.)</li>
</ol>
<p>Solution</p>
<p>Just this, therefore:</p>
<div class="sourceCode" id="cb3555"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3555-1"><a href="discriminant-analysis.html#cb3555-1"></a>cornseed.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(cornseed<span class="fl">.2</span>)</span></code></pre></div>
<ol start="7" style="list-style-type: lower-alpha">
<li>Plot the first two discriminant scores against each other,
coloured by soil type. You’ll have to start by making a data frame
containing what you need.</li>
</ol>
<p>Solution</p>
<p>I changed my mind from the past about how to do this. I make a big data frame out of the data and predictions (with <code>cbind</code>) and go from there:</p>
<div class="sourceCode" id="cb3556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3556-1"><a href="discriminant-analysis.html#cb3556-1"></a>d &lt;-<span class="st"> </span><span class="kw">cbind</span>(cornseed, cornseed.pred)</span>
<span id="cb3556-2"><a href="discriminant-analysis.html#cb3556-2"></a><span class="kw">head</span>(d)</span></code></pre></div>
<pre><code>##   field soil yield water herbicide class posterior.clay posterior.loam posterior.salty
## 1     1 loam  76.7  29.5       7.5  loam    0.008122562      0.9303136     0.003067182
## 2     2 loam  60.5  32.1       6.3  loam    0.195608997      0.3536733     0.134234919
## 3     3 loam  96.1  40.7       4.2  loam    0.029529543      0.8533003     0.008018680
## 4     4 loam  88.1  45.1       4.9  loam    0.069082256      0.7696194     0.020907569
## 5     5 loam  50.2  34.1      11.7  loam    0.010588934      0.9457005     0.005163698
## 6     6 loam  55.0  31.1       6.9  loam    0.208208691      0.3194421     0.159072574
##   posterior.sandy     x.LD1      x.LD2       x.LD3
## 1      0.05849665 2.7137304  0.2765450  0.09765792
## 2      0.31648283 0.6999983 -0.2264123  0.46748170
## 3      0.10915147 2.1875521 -0.1644190 -2.10016387
## 4      0.14039076 1.7307043 -0.8021079 -1.66560056
## 5      0.03854692 2.5284069 -1.0096466  2.37276838
## 6      0.31327660 0.5974055 -0.2867691  0.92872489</code></pre>
<p>Then we use this as input to <code>ggplot</code>:</p>
<div class="sourceCode" id="cb3558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3558-1"><a href="discriminant-analysis.html#cb3558-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> x.LD1, <span class="dt">y =</span> x.LD2, <span class="dt">colour =</span> soil)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1779-1.png" width="672"  /></p>
<ol start="8" style="list-style-type: lower-alpha">
<li>On your plot that you just made, explain briefly how <code>LD1</code>
distinguishes at least one of the soil types.</li>
</ol>
<p>Solution</p>
<p>Find a soil type that is typically high (or low or average) on
LD1. Any one or more of these will do: loam soils are typically high on LD1,
clay soils or salty soils are typically low on LD1; sandy soils are
typically average on LD1. (There are exceptions, but I’m looking for
“typically”.)</p>
<ol style="list-style-type: lower-roman">
<li>On your plot, does <code>LD2</code> appear to do anything to
separate the groups? Is this surprising given your earlier findings?
Explain briefly.</li>
</ol>
<p>Solution</p>
<p>All the soil groups appear go to about the full height of the plot:
that is to say, none of the groups appear to be especially at the
top or the bottom. That means that <code>LD2</code> does not separate
the groups at all. Back in part (<a href="#part:corn-svd">here</a>), we said that
the first linear discriminant is way more important than either of
the other two, and here we see what that means: <code>LD2</code> does nothing to
separate the groups. So it’s not a surprising finding at all.
I thought earlier about asking you to plot only the first linear
discriminant, and now we see why: only the first one separates the
groups. If you wanted to do that, you could make a boxplot of the
discriminant scores by <code>soil</code> group, thus:</p>
<div class="sourceCode" id="cb3559"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3559-1"><a href="discriminant-analysis.html#cb3559-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> soil, <span class="dt">y =</span> x.LD1)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_boxplot</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1780-1.png" width="672"  /></p>
<p>This says more or less the same thing as your plot of <code>LD1</code> and
<code>LD2</code>: <code>loam</code> has the highest <code>LD1</code> score,
<code>sandy</code> is about in the middle, and <code>clay</code> and
<code>salty</code> have typically negative <code>LD1</code> scores, similar to
each other, though there is one outlying <code>salty</code> that looks a
lot more like a <code>loam</code>.</p>
<ol start="10" style="list-style-type: lower-alpha">
<li>Make a table of actual and predicted <code>soil</code>
group. Which soil type was classified correctly the most often?</li>
</ol>
<p>Solution</p>
<div class="sourceCode" id="cb3560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3560-1"><a href="discriminant-analysis.html#cb3560-1"></a><span class="kw">with</span>(d, <span class="kw">table</span>(<span class="dt">obs =</span> soil, <span class="dt">pred =</span> class))</span></code></pre></div>
<pre><code>##        pred
## obs     clay loam salty sandy
##   clay     3    0     3     2
##   loam     0    6     0     2
##   salty    1    1     5     1
##   sandy    2    1     1     4</code></pre>
<p>Or, the <code>tidyverse</code> way, which is below.</p>
<p>There were 8 fields of each soil type. The soil type that has the most
of its fields classified correctly (based on the values of the
response variables) has the biggest number down the diagonal of the
table: looking at 3, 6, 5 and 4, we see that the <code>loam</code> soil
type had the most of its fields classified correctly, so this was the
most distinct from the others. (We also saw this on the plot of
<code>LD1</code> vs. <code>LD2</code>: the <code>loam</code> fields were all over
on the right.)</p>
<p>This was easier because we had the same number of fields of each
type. If we didn’t have that, the right way to go then would be to work out
<em>row</em> percentages:
“out of the fields that were actually sandy, what percent of them got classified as sandy”,
and so on.</p>
<p>This is not a perfect classification, though, which is about what you
would expect from the soil types being intermingled on the plot of
<code>LD1</code> vs. <code>LD2</code>. If you look at the table,
<code>salty</code> and <code>sandy</code> are fairly distinct also, but
<code>clay</code> is often confused with both of them. On the plot of
<code>LD1</code> and <code>LD2</code>, <code>salty</code> is generally to the left
of <code>sandy</code>, but <code>clay</code> is mixed up with them both.
The tidyverse way of doing this is equally good. This is the tidied-up way:</p>
<div class="sourceCode" id="cb3562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3562-1"><a href="discriminant-analysis.html#cb3562-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(soil, class) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3562-2"><a href="discriminant-analysis.html#cb3562-2"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from =</span> class, <span class="dt">values_from =</span> n, <span class="dt">values_fill =</span> <span class="dv">0</span>)</span></code></pre></div>
<pre><code>## # A tibble: 4 x 5
##   soil   clay salty sandy  loam
##   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1 clay      3     3     2     0
## 2 loam      0     0     2     6
## 3 salty     1     5     1     1
## 4 sandy     2     1     4     1</code></pre>
<p>Six out of eight <code>loam</code>s were correctly classified, which is
better than anything else.</p>
<p>Extra: we can calculate misclassification rates, first overall, which is easier:</p>
<div class="sourceCode" id="cb3564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3564-1"><a href="discriminant-analysis.html#cb3564-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3564-2"><a href="discriminant-analysis.html#cb3564-2"></a><span class="st">  </span><span class="kw">count</span>(soil, class) <span class="op">%&gt;%</span></span>
<span id="cb3564-3"><a href="discriminant-analysis.html#cb3564-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">soil_stat =</span> <span class="kw">ifelse</span>(soil <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3564-4"><a href="discriminant-analysis.html#cb3564-4"></a><span class="st">  </span><span class="kw">count</span>(soil_stat, <span class="dt">wt =</span> n)</span></code></pre></div>
<pre><code>##   soil_stat  n
## 1   correct 18
## 2     wrong 14</code></pre>
<div class="sourceCode" id="cb3566"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3566-1"><a href="discriminant-analysis.html#cb3566-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3566-2"><a href="discriminant-analysis.html#cb3566-2"></a><span class="st">  </span><span class="kw">count</span>(soil, class) <span class="op">%&gt;%</span></span>
<span id="cb3566-3"><a href="discriminant-analysis.html#cb3566-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">soil_stat =</span> <span class="kw">ifelse</span>(soil <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3566-4"><a href="discriminant-analysis.html#cb3566-4"></a><span class="st">  </span><span class="kw">count</span>(soil_stat, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3566-5"><a href="discriminant-analysis.html#cb3566-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prop =</span> nn <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(nn))</span></code></pre></div>
<pre><code>## Error: Problem with `mutate()` input `prop`.
## x object &#39;nn&#39; not found
## ℹ Input `prop` is `nn/sum(nn)`.</code></pre>
<p>Note the use of <code>wt</code> on the second <code>count</code> to count the
number of <em>observations</em> from the first <code>count</code>, not the
number of <em>rows</em>.</p>
<p>This shows that 44% of the soil types were misclassified, which
sounds awful, but is actually not so bad, considering. Bear in mind
that if you were just guessing, you’d get 75% of them wrong, so
getting 44% wrong is quite a bit better than that. The variables
(especially <code>herbicide</code>) are at least somewhat informative
about soil type; it’s better to know them than not to.</p>
<p>Or do it by actual soil type:</p>
<div class="sourceCode" id="cb3568"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3568-1"><a href="discriminant-analysis.html#cb3568-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3568-2"><a href="discriminant-analysis.html#cb3568-2"></a><span class="st">  </span><span class="kw">count</span>(soil, class) <span class="op">%&gt;%</span></span>
<span id="cb3568-3"><a href="discriminant-analysis.html#cb3568-3"></a><span class="st">  </span><span class="kw">group_by</span>(soil) <span class="op">%&gt;%</span></span>
<span id="cb3568-4"><a href="discriminant-analysis.html#cb3568-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">soil_stat =</span> <span class="kw">ifelse</span>(soil <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3568-5"><a href="discriminant-analysis.html#cb3568-5"></a><span class="st">  </span><span class="kw">count</span>(soil_stat, <span class="dt">wt =</span> n)</span></code></pre></div>
<pre><code>## # A tibble: 8 x 3
## # Groups:   soil [4]
##   soil  soil_stat     n
##   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;
## 1 clay  correct       3
## 2 clay  wrong         5
## 3 loam  correct       6
## 4 loam  wrong         2
## 5 salty correct       5
## 6 salty wrong         3
## 7 sandy correct       4
## 8 sandy wrong         4</code></pre>
<div class="sourceCode" id="cb3570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3570-1"><a href="discriminant-analysis.html#cb3570-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3570-2"><a href="discriminant-analysis.html#cb3570-2"></a><span class="st">  </span><span class="kw">count</span>(soil, class) <span class="op">%&gt;%</span></span>
<span id="cb3570-3"><a href="discriminant-analysis.html#cb3570-3"></a><span class="st">  </span><span class="kw">group_by</span>(soil) <span class="op">%&gt;%</span></span>
<span id="cb3570-4"><a href="discriminant-analysis.html#cb3570-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">soil_stat =</span> <span class="kw">ifelse</span>(soil <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3570-5"><a href="discriminant-analysis.html#cb3570-5"></a><span class="st">  </span><span class="kw">count</span>(soil_stat, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3570-6"><a href="discriminant-analysis.html#cb3570-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prop =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span></span>
<span id="cb3570-7"><a href="discriminant-analysis.html#cb3570-7"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>n) <span class="op">%&gt;%</span></span>
<span id="cb3570-8"><a href="discriminant-analysis.html#cb3570-8"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from=</span>soil_stat, <span class="dt">values_from=</span>prop)</span></code></pre></div>
<pre><code>## # A tibble: 4 x 3
## # Groups:   soil [4]
##   soil  correct wrong
##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;
## 1 clay    0.375 0.625
## 2 loam    0.75  0.25 
## 3 salty   0.625 0.375
## 4 sandy   0.5   0.5</code></pre>
<p>Loam soil was the easiest to get right, and clay was easiest to get
wrong. However, these proportions were each based on only eight
observations, so it’s probably wise <em>not</em> to say that loam is
<em>always</em> easiest to get right.</p>
<p>I didn’t have you look at posterior probabilities here.
<label for="tufte-mn-230" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-230" class="margin-toggle"><span class="marginnote">Rest assured that I will on the final exam!</span> With 32 fields, this is rather a lot
to list them all, but what we can do is to look at the ones that were
misclassified (the true soil type differs from the predicted soil
type). Before that, though, we need to make a data frame with the stuff in
it that we want to look at. And before <em>that</em>, I want to round
the posterior probabilities to a small number of decimals.</p>
<p>Then, we can fire away with this:</p>
<div class="sourceCode" id="cb3572"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3572-1"><a href="discriminant-analysis.html#cb3572-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3572-2"><a href="discriminant-analysis.html#cb3572-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="kw">across</span>(<span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>), <span class="op">~</span><span class="st"> </span><span class="kw">round</span>(., <span class="dv">3</span>))) <span class="op">%&gt;%</span></span>
<span id="cb3572-3"><a href="discriminant-analysis.html#cb3572-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">row =</span> <span class="kw">row_number</span>()) -&gt;<span class="st"> </span>dd</span>
<span id="cb3572-4"><a href="discriminant-analysis.html#cb3572-4"></a>dd <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(soil <span class="op">!=</span><span class="st"> </span>class)</span></code></pre></div>
<pre><code>##    field  soil yield water herbicide class posterior.clay posterior.loam posterior.salty
## 7      7  loam  65.4  21.6       4.3 sandy          0.174          0.214           0.147
## 8      8  loam  65.7  27.7       5.3 sandy          0.163          0.352           0.113
## 9      9 sandy  67.3  48.3       5.5  clay          0.384          0.206           0.195
## 10    10 sandy  61.3  28.9       6.9  loam          0.106          0.553           0.069
## 11    11 sandy  58.2  42.5       4.8  clay          0.436          0.043           0.339
## 13    13 sandy  66.9  23.9       1.1 salty          0.317          0.013           0.362
## 17    17 salty  62.8  25.9       2.9 sandy          0.308          0.038           0.315
## 20    20 salty  75.6  27.7       6.3  loam          0.026          0.819           0.012
## 24    24 salty  68.4  35.3       1.9  clay          0.403          0.018           0.351
## 25    25  clay  52.5  39.0       3.1 salty          0.414          0.004           0.484
## 28    28  clay  63.5  25.6       3.0 sandy          0.298          0.047           0.295
## 30    30  clay  61.5  16.8       1.9 sandy          0.255          0.020           0.338
## 31    31  clay  62.9  25.8       2.4 salty          0.320          0.024           0.346
## 32    32  clay  49.3  39.4       5.2 salty          0.416          0.019           0.418
##    posterior.sandy      x.LD1       x.LD2       x.LD3 row
## 7            0.465  0.4773813  1.02300901  0.02489683   7
## 8            0.373  0.7787883  0.37394272  0.08610126   8
## 9            0.215  0.2347419 -1.58402466 -0.60226491   9
## 10           0.272  1.1888399  0.05551354  0.69601339  10
## 11           0.182 -0.6365694 -1.16783644 -0.16694577  11
## 13           0.307 -1.1089037  1.05680853 -0.99478905  13
## 17           0.340 -0.6033993  0.65387493 -0.37063590  17
## 20           0.143  2.0847382  0.51018238 -0.11850210  20
## 24           0.227 -1.0111845 -0.06204891 -1.21730783  24
## 25           0.098 -1.8263552 -0.83185894 -0.24274038  25
## 28           0.360 -0.4849415  0.69025922 -0.36741549  28
## 30           0.388 -0.8727560  1.57008678 -0.28665910  30
## 31           0.310 -0.8448346  0.70045299 -0.50863515  31
## 32           0.146 -1.0360558 -1.08342373  0.47156646  32</code></pre>
<p>Most of the posterior probabilities are neither especially small nor
especially large, which adds to the impression that things are really
rather uncertain. For example, field 8 could have been either loam
(0.352) or sandy (0.373). There was one field that was actually salty
but looked like a loam one (with <code>LD1</code> score around 2); this is
field 20, that needed a lot of herbicide; it was rated to have an 82%
chance of being loam and only 1% chance of salty.</p>
<p>Let’s remind ourselves of why we were doing this: the MANOVA was
significant, so at least some of the fields were different on some of
the variables from some of the others. What we found by doing the
discriminant analysis was that only the first discriminant was of any
value in distinguishing the soil types by the variables we measured,
and <em>that</em> was mostly <code>herbicide</code>. So the major effect
that soil type had was on the amount of herbicide needed, with the
loam soils needing most.</p>
<p>I wanted to finish with one more thing, which was to look again at the
soils that were actually loam:</p>
<div class="sourceCode" id="cb3574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3574-1"><a href="discriminant-analysis.html#cb3574-1"></a>dd <span class="op">%&gt;%</span></span>
<span id="cb3574-2"><a href="discriminant-analysis.html#cb3574-2"></a><span class="st">  </span><span class="kw">filter</span>(soil <span class="op">==</span><span class="st"> &quot;loam&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb3574-3"><a href="discriminant-analysis.html#cb3574-3"></a><span class="st">  </span><span class="kw">select</span>(soil, yield, water, herbicide, class, <span class="kw">starts_with</span>(<span class="st">&quot;posterior&quot;</span>))</span></code></pre></div>
<pre><code>##   soil yield water herbicide class posterior.clay posterior.loam posterior.salty
## 1 loam  76.7  29.5       7.5  loam          0.008          0.930           0.003
## 2 loam  60.5  32.1       6.3  loam          0.196          0.354           0.134
## 3 loam  96.1  40.7       4.2  loam          0.030          0.853           0.008
## 4 loam  88.1  45.1       4.9  loam          0.069          0.770           0.021
## 5 loam  50.2  34.1      11.7  loam          0.011          0.946           0.005
## 6 loam  55.0  31.1       6.9  loam          0.208          0.319           0.159
## 7 loam  65.4  21.6       4.3 sandy          0.174          0.214           0.147
## 8 loam  65.7  27.7       5.3 sandy          0.163          0.352           0.113
##   posterior.sandy
## 1           0.058
## 2           0.316
## 3           0.109
## 4           0.140
## 5           0.039
## 6           0.313
## 7           0.465
## 8           0.373</code></pre>
<p>Fields 7 and 8 could have been pretty much any type of soil;
<code>sandy</code> came out with the highest posterior probability, so
that’s what they were predicted (wrongly) to be. Some of the fields,
1, 3 and 5, were clearly (and correctly) loam. For 1 and 5, you can clearly
see that this is because <code>herbicide</code> was high, but field 3 is
more of a mystery. For this field, <code>herbicide</code> is <em>not</em>
high, so one or more of the other variables must be pointing towards
<code>loam</code>.</p>
<p>We can obtain predicted
<code>LD1</code> scores for various combinations of “typical” values of
the response variables and see what has what effect on <code>LD1</code>:</p>
<div class="sourceCode" id="cb3576"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3576-1"><a href="discriminant-analysis.html#cb3576-1"></a><span class="kw">summary</span>(cornseed)</span></code></pre></div>
<pre><code>##      field           soil               yield           water         herbicide     
##  Min.   : 1.00   Length:32          Min.   :45.00   Min.   :14.50   Min.   : 1.100  
##  1st Qu.: 8.75   Class :character   1st Qu.:50.58   1st Qu.:25.75   1st Qu.: 3.075  
##  Median :16.50   Mode  :character   Median :61.40   Median :29.60   Median : 4.750  
##  Mean   :16.50                      Mean   :61.61   Mean   :31.17   Mean   : 4.672  
##  3rd Qu.:24.25                      3rd Qu.:67.00   3rd Qu.:36.83   3rd Qu.: 5.825  
##  Max.   :32.00                      Max.   :96.10   Max.   :54.20   Max.   :11.700</code></pre>
<p>The problem is that the variables have different spreads. Let’s do
some predictions (ie. calculations) of LD1 score for combinations of
quartiles of our response variables. I like quartiles because these
are “representative” values of the variables, typical of how far up
and down they go. This process is one you’ve seen before:</p>
<div class="sourceCode" id="cb3578"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3578-1"><a href="discriminant-analysis.html#cb3578-1"></a>yields &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">51</span>, <span class="dv">67</span>)</span>
<span id="cb3578-2"><a href="discriminant-analysis.html#cb3578-2"></a>waters &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">26</span>, <span class="dv">37</span>)</span>
<span id="cb3578-3"><a href="discriminant-analysis.html#cb3578-3"></a>herbicides &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">6</span>)</span>
<span id="cb3578-4"><a href="discriminant-analysis.html#cb3578-4"></a>new &lt;-<span class="st"> </span><span class="kw">crossing</span>(<span class="dt">yield =</span> yields, <span class="dt">water =</span> waters, <span class="dt">herbicide =</span> herbicides)</span>
<span id="cb3578-5"><a href="discriminant-analysis.html#cb3578-5"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(cornseed<span class="fl">.2</span>, new)</span>
<span id="cb3578-6"><a href="discriminant-analysis.html#cb3578-6"></a><span class="kw">cbind</span>(new, pred<span class="op">$</span>x) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(LD1))</span></code></pre></div>
<pre><code>##   yield water herbicide         LD1        LD2         LD3
## 6    67    26         6  1.30225880  0.5153162  0.26932408
## 8    67    37         6  0.88866305 -0.5405273 -0.08618456
## 2    51    26         6  0.01028356  0.1823283  1.04091323
## 5    67    26         3 -0.21736172  0.7247060 -0.54912820
## 4    51    37         6 -0.40331219 -0.8735152  0.68540458
## 7    67    37         3 -0.63095747 -0.3311374 -0.90463685
## 1    51    26         3 -1.50933696  0.3917181  0.22246094
## 3    51    37         3 -1.92293271 -0.6641254 -0.13304771</code></pre>
<p>I arranged the predicted LD1 scores in descending order, so the most
loam-like combinations are at the top. The top two combinations look
like loam; they both have high <code>herbicide</code>, as we figured
before. But they also have high <code>yield</code>. That might go some way
towards explaining why field 3, with its non-high <code>herbicide</code>,
was confidently predicted to be <code>loam</code>:</p>
<div class="sourceCode" id="cb3580"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3580-1"><a href="discriminant-analysis.html#cb3580-1"></a>cornseed <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(field <span class="op">==</span><span class="st"> </span><span class="dv">3</span>)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 5
##   field soil  yield water herbicide
##   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
## 1     3 loam   96.1  40.7       4.2</code></pre>
<p>This has a very high <code>yield</code>, and <em>that</em> is what is making
us (correctly) think it is <code>loam</code>.</p>
<p>I suddenly remembered that I hadn’t done a biplot of this one, which I
could, since it’s a discriminant analysis:</p>
<div class="sourceCode" id="cb3582"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3582-1"><a href="discriminant-analysis.html#cb3582-1"></a><span class="kw">ggbiplot</span>(cornseed<span class="fl">.2</span>, <span class="dt">groups =</span> cornseed<span class="op">$</span>soil)</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1792-1.png" width="672"  /></p>
<p>This shows the dominant influence of <code>herbicide</code> on LD1 score
(more herbicide is more positive), and that <code>water</code> has nothing
to say (in terms of distinguishing soil types) and <code>yield</code> has
not much to say, their arrows being short. That observation with a
non-high <code>herbicide</code> that was predicted to be  had
the highest <code>yield</code> of all, so even the small influence of
<code>yield</code> on <code>LD1</code> made a big difference here.</p>
</div>
<div id="understanding-athletes-height-weight-sport-and-gender" class="section level2" number="26.7">
<h2><span class="header-section-number">26.7</span> Understanding athletes’ height, weight, sport and gender</h2>
<p>On a previous assignment, we used MANOVA on the athletes
data to demonstrate that there was a significant relationship between
the combination of the athletes’ height and weight, with the sport they
play and the athlete’s gender. The problem with MANOVA is that it
doesn’t give any information about the <em>kind</em> of relationship. To
understand that, we need to do discriminant analysis, which is the
purpose of this question.</p>
<p>The data can be found at
<a href="https://raw.githubusercontent.com/nxskok/datafiles/master/ais.txt">link</a>.</p>
<ol style="list-style-type: lower-alpha">
<li>Once again, read in and display (some of) the data, bearing
in mind that the data values are separated by <em>tabs</em>. (This
ought to be a free two marks.)</li>
</ol>
<p>Solution</p>
<p>Nothing new here:</p>
<div class="sourceCode" id="cb3583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3583-1"><a href="discriminant-analysis.html#cb3583-1"></a>my_url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/nxskok/datafiles/master/ais.txt&quot;</span></span>
<span id="cb3583-2"><a href="discriminant-analysis.html#cb3583-2"></a>athletes &lt;-<span class="st"> </span><span class="kw">read_tsv</span>(my_url)</span></code></pre></div>
<pre><code>## 
## ── Column specification ──────────────────────────────────────────────────────────────────
## cols(
##   Sex = col_character(),
##   Sport = col_character(),
##   RCC = col_double(),
##   WCC = col_double(),
##   Hc = col_double(),
##   Hg = col_double(),
##   Ferr = col_double(),
##   BMI = col_double(),
##   SSF = col_double(),
##   `%Bfat` = col_double(),
##   LBM = col_double(),
##   Ht = col_double(),
##   Wt = col_double()
## )</code></pre>
<div class="sourceCode" id="cb3585"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3585-1"><a href="discriminant-analysis.html#cb3585-1"></a>athletes</span></code></pre></div>
<pre><code>## # A tibble: 202 x 13
##    Sex    Sport     RCC   WCC    Hc    Hg  Ferr   BMI   SSF `%Bfat`   LBM    Ht    Wt
##    &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 female Netball  4.56  13.3  42.2  13.6    20  19.2  49      11.3  53.1  177.  59.9
##  2 female Netball  4.15   6    38    12.7    59  21.2 110.     25.3  47.1  173.  63  
##  3 female Netball  4.16   7.6  37.5  12.3    22  21.4  89      19.4  53.4  176   66.3
##  4 female Netball  4.32   6.4  37.7  12.3    30  21.0  98.3    19.6  48.8  170.  60.7
##  5 female Netball  4.06   5.8  38.7  12.8    78  21.8 122.     23.1  56.0  183   72.9
##  6 female Netball  4.12   6.1  36.6  11.8    21  21.4  90.4    16.9  56.4  178.  67.9
##  7 female Netball  4.17   5    37.4  12.7   109  21.5 107.     21.3  53.1  177.  67.5
##  8 female Netball  3.8    6.6  36.5  12.4   102  24.4 157.     26.6  54.4  174.  74.1
##  9 female Netball  3.96   5.5  36.3  12.4    71  22.6 101.     17.9  56.0  174.  68.2
## 10 female Netball  4.44   9.7  41.4  14.1    64  22.8 126.     25.0  51.6  174.  68.8
## # … with 192 more rows</code></pre>
<ol start="2" style="list-style-type: lower-alpha">
<li>Use <code>unite</code> to make a new column in your data frame
which contains the sport-gender <em>combination</em>. Display it. (You
might like to display only a few columns so that it is clear that
you did the right thing.) Hint: you’ve seen <code>unite</code> in the
peanuts example in class.</li>
</ol>
<p>Solution</p>
<p>The columns to combine are called <code>Sport</code> and <code>Sex</code>,
with Capital Letters. The syntax for <code>unite</code> is that you
give the name of the new combo column first, and then the names of
the columns you want to combine, either by listing them or by
using a select-helper. They will be separated by an underscore by
default, which is usually easiest to handle.
<label for="tufte-mn-231" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-231" class="margin-toggle"><span class="marginnote">The opposite of <em>unite</em> is <em>separate</em>, which splits a combined column like my <em>combo</em> into separate columns; it too uses underscore as the default separator.</span>
In <code>unite</code>, you can
group the columns to “unite” with <code>c()</code>, as in class, or
not, as here. Either way is good.
<label for="tufte-mn-232" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-232" class="margin-toggle"><span class="marginnote">You used to have to group them, but you don’t any more. Hence my old code has them grouped, but my new code does not.</span>
We’ll be using height and weight in the
analysis to come, so I decided to display just those:</p>
<div class="sourceCode" id="cb3587"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3587-1"><a href="discriminant-analysis.html#cb3587-1"></a>athletes <span class="op">%&gt;%</span></span>
<span id="cb3587-2"><a href="discriminant-analysis.html#cb3587-2"></a><span class="st">  </span><span class="kw">unite</span>(combo, Sport, Sex) -&gt;<span class="st"> </span>athletesc</span>
<span id="cb3587-3"><a href="discriminant-analysis.html#cb3587-3"></a>athletesc <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(combo, Ht, Wt)</span></code></pre></div>
<pre><code>## # A tibble: 202 x 3
##    combo             Ht    Wt
##    &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;
##  1 Netball_female  177.  59.9
##  2 Netball_female  173.  63  
##  3 Netball_female  176   66.3
##  4 Netball_female  170.  60.7
##  5 Netball_female  183   72.9
##  6 Netball_female  178.  67.9
##  7 Netball_female  177.  67.5
##  8 Netball_female  174.  74.1
##  9 Netball_female  174.  68.2
## 10 Netball_female  174.  68.8
## # … with 192 more rows</code></pre>
<p>I gave the data frame a new name, since I might want to come back to
the original later. Also, displaying only those columns gives more
width for the display of my <code>combo</code>, so that I can be sure I
got it right.</p>
<p>Extra: there is another column, <code>SSF</code>, that begins with S, so the
select-helper thing is not so obviously helpful here. But the two
columns we want start with S followed by either e or p, so we could do this:</p>
<div class="sourceCode" id="cb3589"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3589-1"><a href="discriminant-analysis.html#cb3589-1"></a>athletes <span class="op">%&gt;%</span></span>
<span id="cb3589-2"><a href="discriminant-analysis.html#cb3589-2"></a><span class="st">  </span><span class="kw">unite</span>(combo, <span class="kw">matches</span>(<span class="st">&quot;^S(e|p)&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3589-3"><a href="discriminant-analysis.html#cb3589-3"></a><span class="st">  </span><span class="kw">select</span>(combo, Ht, Wt)</span></code></pre></div>
<pre><code>## # A tibble: 202 x 3
##    combo             Ht    Wt
##    &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;
##  1 female_Netball  177.  59.9
##  2 female_Netball  173.  63  
##  3 female_Netball  176   66.3
##  4 female_Netball  170.  60.7
##  5 female_Netball  183   72.9
##  6 female_Netball  178.  67.9
##  7 female_Netball  177.  67.5
##  8 female_Netball  174.  74.1
##  9 female_Netball  174.  68.2
## 10 female_Netball  174.  68.8
## # … with 192 more rows</code></pre>
<p>The <code>matches</code> takes a so-called regular expression. This one
says ``starting at the beginning of the column name, find an uppercase
S followed by either a lowercase e or a lowercase p’’. This picks out
the columns and only the columns we want. In the opposite order,
though (either order is fine).</p>
<p>I have a feeling we can also take advantage of the fact that the two
columns we want to <code>unite</code> are the only two text ones:</p>
<div class="sourceCode" id="cb3591"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3591-1"><a href="discriminant-analysis.html#cb3591-1"></a>athletes <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3591-2"><a href="discriminant-analysis.html#cb3591-2"></a><span class="st">  </span><span class="kw">unite</span>(combo, <span class="kw">where</span>(is.character))</span></code></pre></div>
<pre><code>## # A tibble: 202 x 12
##    combo            RCC   WCC    Hc    Hg  Ferr   BMI   SSF `%Bfat`   LBM    Ht    Wt
##    &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 female_Netball  4.56  13.3  42.2  13.6    20  19.2  49      11.3  53.1  177.  59.9
##  2 female_Netball  4.15   6    38    12.7    59  21.2 110.     25.3  47.1  173.  63  
##  3 female_Netball  4.16   7.6  37.5  12.3    22  21.4  89      19.4  53.4  176   66.3
##  4 female_Netball  4.32   6.4  37.7  12.3    30  21.0  98.3    19.6  48.8  170.  60.7
##  5 female_Netball  4.06   5.8  38.7  12.8    78  21.8 122.     23.1  56.0  183   72.9
##  6 female_Netball  4.12   6.1  36.6  11.8    21  21.4  90.4    16.9  56.4  178.  67.9
##  7 female_Netball  4.17   5    37.4  12.7   109  21.5 107.     21.3  53.1  177.  67.5
##  8 female_Netball  3.8    6.6  36.5  12.4   102  24.4 157.     26.6  54.4  174.  74.1
##  9 female_Netball  3.96   5.5  36.3  12.4    71  22.6 101.     17.9  56.0  174.  68.2
## 10 female_Netball  4.44   9.7  41.4  14.1    64  22.8 126.     25.0  51.6  174.  68.8
## # … with 192 more rows</code></pre>
<p>I wasn’t expecting that to work!</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Run a discriminant analysis “predicting” sport-gender
combo from height and weight. Display the results. (No comment
needed yet.)</li>
</ol>
<p>Solution</p>
<p>That would be this. I’m having my familiar trouble with names:</p>
<div class="sourceCode" id="cb3593"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3593-1"><a href="discriminant-analysis.html#cb3593-1"></a>combo<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(combo <span class="op">~</span><span class="st"> </span>Ht <span class="op">+</span><span class="st"> </span>Wt, <span class="dt">data =</span> athletesc)</span></code></pre></div>
<p>If you used a new name for the data frame with the sport-gender
combinations in it, use that new name here.</p>
<p>The output:</p>
<div class="sourceCode" id="cb3594"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3594-1"><a href="discriminant-analysis.html#cb3594-1"></a>combo<span class="fl">.1</span></span></code></pre></div>
<pre><code>## Call:
## lda(combo ~ Ht + Wt, data = athletesc)
## 
## Prior probabilities of groups:
##   BBall_female     BBall_male   Field_female     Field_male     Gym_female Netball_female 
##     0.06435644     0.05940594     0.03465347     0.05940594     0.01980198     0.11386139 
##     Row_female       Row_male    Swim_female      Swim_male   T400m_female     T400m_male 
##     0.10891089     0.07425743     0.04455446     0.06435644     0.05445545     0.08910891 
##  Tennis_female    Tennis_male  TSprnt_female    TSprnt_male     WPolo_male 
##     0.03465347     0.01980198     0.01980198     0.05445545     0.08415842 
## 
## Group means:
##                      Ht       Wt
## BBall_female   182.2692 71.33077
## BBall_male     195.5833 88.92500
## Field_female   172.5857 80.04286
## Field_male     185.2750 95.76250
## Gym_female     153.4250 43.62500
## Netball_female 176.0870 69.59348
## Row_female     178.8591 72.90000
## Row_male       187.5333 86.80667
## Swim_female    173.1778 65.73333
## Swim_male      185.6462 81.66154
## T400m_female   169.3364 57.23636
## T400m_male     179.1889 68.20833
## Tennis_female  168.5714 58.22857
## Tennis_male    183.9500 75.40000
## TSprnt_female  170.4750 59.72500
## TSprnt_male    178.5364 75.79091
## WPolo_male     188.2235 86.72941
## 
## Coefficients of linear discriminants:
##           LD1        LD2
## Ht 0.08898971 -0.1888615
## Wt 0.06825230  0.1305246
## 
## Proportion of trace:
##    LD1    LD2 
## 0.7877 0.2123</code></pre>
<p>I comment here that there are two linear discriminants because there
are two variables (height and weight) and actually 17 groups (not
quite <span class="math inline">\(2\times 10\)</span> because some sports are played by athletes of only
one gender). The smaller of 2 and <span class="math inline">\(17-1\)</span> is 2. (I often ask about
this, but am choosing not to here.)</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>What kind of height and weight would make an athlete have a
large (positive) score on <code>LD1</code>? Explain briefly.</li>
</ol>
<p>Solution</p>
<p>The Coefficients of Linear Discriminants for <code>LD1</code> are both
positive, so an athlete with a large positive score on
<code>LD1</code> has a large height and weight: that is to say, they
are tall and heavy.</p>
<ol start="5" style="list-style-type: lower-alpha">
<li>Make a guess at the sport-gender combination that has the
<em>highest</em> score on LD1. Why did you choose the combination you did?</li>
</ol>
<p>Solution</p>
<p>I could have made you guess the <em>smallest</em> score on LD1, but
that would have been too easy (female gymnasts).
For this one, you want a sport-gender combination that is typically
tall and heavy, and you can look in the table of Group Means to
help you find a candidate group.
I think the two best guesses are male basketball players (tallest
and nearly the heaviest) and male field athletes (heaviest and
among the group of athletes that are second-tallest behind the
male basketball players). I don’t so much mind what you guess, as
long as you make a sensible call about a group that is reasonably
tall and reasonably heavy (or, I suppose, that matches with what
you said in the previous part, whatever that was).</p>
<ol start="6" style="list-style-type: lower-alpha">
<li><a name="part:ld2"><em></a> What combination of height and weight would make an athlete have a
</em>small* (that is, very negative) score on LD2? Explain briefly.</li>
</ol>
<p>Solution</p>
<p>The italics in the question are something to do with questions
that have a link to them in Bookdown. I don’t know how to fix
that.
Going back to the Coefficients of Linear Discriminants, the
coefficient for Height is negative, and the one for Weight is
positive. What will make an athlete come out small (very
negative) on this is if they have a <em>large</em> height and a
<em>small</em> weight.
To clarify your thinking on this, think of
the heights and weights as being standardized, so that a big one
will be positive and a small one will be negative. To make
<code>LD2</code> very negative, you want a “plus” height to multiply
the minus sign, and a “minus” weight multiplying the plus sign.
Extra: what is happening here is that <code>LD1</code> gives the most
important way in which the groups differ, and <code>LD2</code> the
next-most important. There is generally a positive correlation
between height and weight (taller athletes are generally heavier),
so the most important “dimension” is the big-small one with tall
heavy athletes at one end and short light athletes at the other.
The <code>Proportion of trace</code> in the output says that
<code>LD1</code> is definitely more important, in terms of separating
the groups, than <code>LD2</code> is, but the latter still has
<em>some</em> value.</p>
<ol start="7" style="list-style-type: lower-alpha">
<li>Obtain predictions for the discriminant analysis, and use
these to make a plot of <code>LD1</code> score against <code>LD2</code>
score, with the individual athletes distinguished by what sport they play
and gender they are. (You can use colour to distinguish them, or you
can use shapes. If you want to go the latter way, there are clues in
my solutions to the MANOVA question about these athletes.)</li>
</ol>
<p>Solution</p>
<p>The prediction part is only one step:</p>
<div class="sourceCode" id="cb3596"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3596-1"><a href="discriminant-analysis.html#cb3596-1"></a>p &lt;-<span class="st"> </span><span class="kw">predict</span>(combo<span class="fl">.1</span>)</span></code></pre></div>
<p>One point for this.</p>
<p>This, in case you are wondering, is obtaining predicted group
membership and LD scores for the original data, that is, for our 202
athletes.</p>
<p>I prefer (no obligation) to take a look at what I have. My <code>p</code>
is actually a <code>list</code>:</p>
<div class="sourceCode" id="cb3597"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3597-1"><a href="discriminant-analysis.html#cb3597-1"></a><span class="kw">class</span>(p)</span></code></pre></div>
<pre><code>## [1] &quot;list&quot;</code></pre>
<div class="sourceCode" id="cb3599"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3599-1"><a href="discriminant-analysis.html#cb3599-1"></a><span class="kw">glimpse</span>(p)</span></code></pre></div>
<pre><code>## List of 3
##  $ class    : Factor w/ 17 levels &quot;BBall_female&quot;,..: 12 6 6 6 7 6 6 6 6 6 ...
##  $ posterior: num [1:202, 1:17] 0.1235 0.0493 0.084 0.0282 0.1538 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:202] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:17] &quot;BBall_female&quot; &quot;BBall_male&quot; &quot;Field_female&quot; &quot;Field_male&quot; ...
##  $ x        : num [1:202, 1:2] -1.325 -1.487 -0.96 -1.885 0.114 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:202] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:2] &quot;LD1&quot; &quot;LD2&quot;</code></pre>
<p>Our standard procedure is to <code>cbind</code> the predictions together with the original data (including the combo), and get a huge data frame (in this case):</p>
<div class="sourceCode" id="cb3601"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3601-1"><a href="discriminant-analysis.html#cb3601-1"></a>d &lt;-<span class="st"> </span><span class="kw">cbind</span>(athletesc, p)</span>
<span id="cb3601-2"><a href="discriminant-analysis.html#cb3601-2"></a><span class="kw">head</span>(d)</span></code></pre></div>
<pre><code>##            combo  RCC  WCC   Hc   Hg Ferr   BMI   SSF %Bfat   LBM    Ht   Wt
## 1 Netball_female 4.56 13.3 42.2 13.6   20 19.16  49.0 11.29 53.14 176.8 59.9
## 2 Netball_female 4.15  6.0 38.0 12.7   59 21.15 110.2 25.26 47.09 172.6 63.0
## 3 Netball_female 4.16  7.6 37.5 12.3   22 21.40  89.0 19.39 53.44 176.0 66.3
## 4 Netball_female 4.32  6.4 37.7 12.3   30 21.03  98.3 19.63 48.78 169.9 60.7
## 5 Netball_female 4.06  5.8 38.7 12.8   78 21.77 122.1 23.11 56.05 183.0 72.9
## 6 Netball_female 4.12  6.1 36.6 11.8   21 21.38  90.4 16.86 56.45 178.2 67.9
##            class posterior.BBall_female posterior.BBall_male posterior.Field_female
## 1     T400m_male             0.12348360         3.479619e-04           0.0002835604
## 2 Netball_female             0.04927852         7.263143e-05           0.0041253799
## 3 Netball_female             0.08402197         4.567927e-04           0.0032633771
## 4 Netball_female             0.02820743         1.539520e-05           0.0048909758
## 5     Row_female             0.15383834         1.197089e-02           0.0011443415
## 6 Netball_female             0.11219817         1.320889e-03           0.0021761290
##   posterior.Field_male posterior.Gym_female posterior.Netball_female posterior.Row_female
## 1         1.460578e-05         4.206308e-05                0.1699941            0.1241779
## 2         9.838207e-05         3.101597e-04                0.2333569            0.1414225
## 3         2.676308e-04         3.414854e-05                0.2291353            0.1816810
## 4         4.524089e-05         1.531681e-03                0.2122221            0.1045723
## 5         1.169322e-03         2.247239e-07                0.1326885            0.1822427
## 6         3.751161e-04         8.019783e-06                0.2054332            0.1917380
##   posterior.Row_male posterior.Swim_female posterior.Swim_male posterior.T400m_female
## 1       0.0023825007            0.07434038         0.011678465            0.103051973
## 2       0.0025370630            0.11730520         0.009274681            0.119270442
## 3       0.0077872436            0.08659049         0.023136399            0.058696177
## 4       0.0009826883            0.13254329         0.004132741            0.179336337
## 5       0.0456717871            0.02802782         0.089868173            0.008428382
## 6       0.0133925352            0.06557996         0.036249576            0.036328215
##   posterior.T400m_male posterior.Tennis_female posterior.Tennis_male
## 1           0.25594274             0.047204095           0.017883433
## 2           0.13618567             0.075858992           0.008601514
## 3           0.17305732             0.035224944           0.017564554
## 4           0.09812128             0.120824963           0.004345342
## 5           0.17333438             0.004456769           0.046106286
## 6           0.19213811             0.020599135           0.025565109
##   posterior.TSprnt_female posterior.TSprnt_male posterior.WPolo_male      x.LD1
## 1             0.040192120            0.02616911         0.0028113441 -1.3251857
## 2             0.050772549            0.04902216         0.0025072687 -1.4873604
## 3             0.028170015            0.06274649         0.0081661381 -0.9595628
## 4             0.070141970            0.03716409         0.0009221631 -1.8846129
## 5             0.005144923            0.06163897         0.0542681927  0.1138304
## 6             0.018513425            0.06367698         0.0147074229 -0.6545817
##         x.LD2
## 1 -1.34799600
## 2 -0.15015145
## 3 -0.36154960
## 4  0.05956819
## 5 -0.82211820
## 6 -0.56820566</code></pre>
<p>And so, to the graph:</p>
<div class="sourceCode" id="cb3603"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3603-1"><a href="discriminant-analysis.html#cb3603-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> x.LD1, <span class="dt">y =</span> x.LD2, <span class="dt">colour =</span> combo)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1802-1.png" width="672"  /></p>
<p>If you can distinguish seventeen different colours, your eyes are
better than mine! You might prefer to use seventeen different shapes,
although I wonder how much better that will be:</p>
<div class="sourceCode" id="cb3604"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3604-1"><a href="discriminant-analysis.html#cb3604-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> x.LD1, <span class="dt">y =</span> x.LD2, <span class="dt">shape =</span> combo)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb3604-2"><a href="discriminant-analysis.html#cb3604-2"></a><span class="st">  </span><span class="kw">scale_shape_manual</span>(<span class="dt">values =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">17</span>)</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1803-1.png" width="672"  /></p>
<p>You have to do something special to get as many as seventeen
shapes. This idea came from the MANOVA question in the last
assignment.</p>
<p>Or even this:</p>
<div class="sourceCode" id="cb3605"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3605-1"><a href="discriminant-analysis.html#cb3605-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> x.LD1, <span class="dt">y =</span> x.LD2, <span class="dt">shape =</span> combo, <span class="dt">colour =</span> combo)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb3605-2"><a href="discriminant-analysis.html#cb3605-2"></a><span class="st">  </span><span class="kw">scale_shape_manual</span>(<span class="dt">values =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">17</span>)</span></code></pre></div>
<p><img src="pasias_files/figure-html/unnamed-chunk-1804-1.png" width="672"  /></p>
<p>Perhaps having colours <em>and</em> shapes makes the combos easier to
distinguish. We’re beginning to stray onto the boundary between
statistics and aesthetics here!</p>
<p>Extra: earlier, I asked you to guess which group(s) of athletes had a
high (positive) score on LD1. These are the ones on the right side of
this plot: male basketball players bottom right and male field
athletes top right. Was that what you guessed? What about the other
guesses you might have made?</p>
<ol start="8" style="list-style-type: lower-alpha">
<li>Look on your graph for the four athletes with the smallest
(most negative) scores on <code>LD2</code>. What do they have in common?
Does this make sense, given your answer to part (<a href="#part:ld2">here</a>)?
Explain briefly.</li>
</ol>
<p>Solution</p>
<p>These are the four athletes at the bottom of the plot. If you can
distinguish the colours, two of these are red and two of them are
orange, so they are all basketball players (two male and two
female). If you plotted the shapes, and you used the same shapes I
did, two of them are circles and the other two are upward-facing
triangles, leading you to the same conclusion. (You could also
denote each combo by a letter and plot with those letters, as per
the solutions to the last assignment.)
Back in part (<a href="#part:ld2">here</a>), I said that what would make an
athlete come out very negative on <code>LD2</code> is if they were
<em>tall</em> and <em>not heavy</em>. This is the stereotypical
description of a basketball player, so it makes perfect sense to
me.
Extra: some basketball players are tall and <em>heavier</em>; these
are the ones on the right of the plot, with a larger <code>LD1</code>
score, to reflect that they are both tall and heavy, but with an
<code>LD2</code> score closer to zero, reflecting that, given how tall
they are, their weight is about what you’d expect. LD2 is really
saying something like “weight relative to height”, with someone
at the top of the picture being unusually heavy and someone at the
bottom unusually light.</p>
<ol style="list-style-type: lower-roman">
<li>Obtain a (very large) square table, or a (very long) table
with frequencies, of actual and predicted sport-gender
combinations. You will probably have to make the square table very
small to fit it on the page. For that, displaying the columns in two
or more sets is OK (for example, six columns and all the rows, six
more columns and all the rows, then the last five columns for all
the rows). Are there any sport-gender combinations that
seem relatively easy to classify correctly? Explain briefly.</li>
</ol>
<p>Solution</p>
<p>Let’s see what happens:</p>
<div class="sourceCode" id="cb3606"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3606-1"><a href="discriminant-analysis.html#cb3606-1"></a>tab &lt;-<span class="st"> </span><span class="kw">with</span>(d, <span class="kw">table</span>(combo, class))</span>
<span id="cb3606-2"><a href="discriminant-analysis.html#cb3606-2"></a>tab</span></code></pre></div>
<pre><code>##                 class
## combo            BBall_female BBall_male Field_female Field_male Gym_female
##   BBall_female              3          1            0          0          0
##   BBall_male                0          9            0          0          0
##   Field_female              0          0            5          0          0
##   Field_male                0          1            0          7          0
##   Gym_female                0          0            0          0          4
##   Netball_female            0          0            1          0          0
##   Row_female                0          0            0          0          1
##   Row_male                  0          2            0          1          0
##   Swim_female               0          0            0          0          0
##   Swim_male                 0          4            0          0          0
##   T400m_female              0          0            0          0          0
##   T400m_male                3          1            0          0          0
##   Tennis_female             0          0            1          0          1
##   Tennis_male               1          0            0          0          0
##   TSprnt_female             0          0            0          0          0
##   TSprnt_male               0          0            0          0          0
##   WPolo_male                1          3            0          2          0
##                 class
## combo            Netball_female Row_female Row_male Swim_female Swim_male T400m_female
##   BBall_female                5          1        0           0         0            0
##   BBall_male                  0          0        0           0         2            0
##   Field_female                1          0        0           0         0            1
##   Field_male                  0          2        0           0         0            0
##   Gym_female                  0          0        0           0         0            0
##   Netball_female             13          4        0           0         0            1
##   Row_female                  5         10        0           0         1            0
##   Row_male                    0          0        1           0         0            1
##   Swim_female                 4          1        0           0         0            3
##   Swim_male                   2          3        0           0         0            0
##   T400m_female                3          0        0           0         0            6
##   T400m_male                  5          3        0           0         0            1
##   Tennis_female               2          0        0           0         0            2
##   Tennis_male                 0          3        0           0         0            0
##   TSprnt_female               1          0        0           0         0            2
##   TSprnt_male                 6          3        0           0         0            0
##   WPolo_male                  0          3        1           0         0            0
##                 class
## combo            T400m_male Tennis_female Tennis_male TSprnt_female TSprnt_male
##   BBall_female            2             0           0             0           0
##   BBall_male              0             0           0             0           0
##   Field_female            0             0           0             0           0
##   Field_male              0             0           0             0           0
##   Gym_female              0             0           0             0           0
##   Netball_female          4             0           0             0           0
##   Row_female              4             0           0             0           0
##   Row_male                0             0           0             0           0
##   Swim_female             1             0           0             0           0
##   Swim_male               1             0           0             0           0
##   T400m_female            2             0           0             0           0
##   T400m_male              5             0           0             0           0
##   Tennis_female           1             0           0             0           0
##   Tennis_male             0             0           0             0           0
##   TSprnt_female           1             0           0             0           0
##   TSprnt_male             0             0           0             0           0
##   WPolo_male              0             0           0             0           0
##                 class
## combo            WPolo_male
##   BBall_female            1
##   BBall_male              1
##   Field_female            0
##   Field_male              2
##   Gym_female              0
##   Netball_female          0
##   Row_female              1
##   Row_male               10
##   Swim_female             0
##   Swim_male               3
##   T400m_female            0
##   T400m_male              0
##   Tennis_female           0
##   Tennis_male             0
##   TSprnt_female           0
##   TSprnt_male             2
##   WPolo_male              7</code></pre>
<p>That’s kind of long.</p>
<p>For combos that are easy to classify, you’re looking for a largish
number on the diagonal of the table (classified correctly), bearing in
mind that you only see about four columns of the table at once, and
(much) smaller numbers in the rest of the row and column. I don’t mind
which ones you pick out, but see if you can find a few:</p>
<ul>
<li><p>Male basketball players (9 out of 12 classified correctly)</p></li>
<li><p>Male field athletes (7 out of 10 classified correctly)</p></li>
<li><p>Female netball players (13 out of about 23)</p></li>
<li><p>Female rowers (10 out of about 22)</p></li>
</ul>
<p>Or you can turn it into a tibble:</p>
<div class="sourceCode" id="cb3608"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3608-1"><a href="discriminant-analysis.html#cb3608-1"></a>tab <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>()</span></code></pre></div>
<pre><code>## # A tibble: 289 x 3
##    combo          class            n
##    &lt;chr&gt;          &lt;chr&gt;        &lt;int&gt;
##  1 BBall_female   BBall_female     3
##  2 BBall_male     BBall_female     0
##  3 Field_female   BBall_female     0
##  4 Field_male     BBall_female     0
##  5 Gym_female     BBall_female     0
##  6 Netball_female BBall_female     0
##  7 Row_female     BBall_female     0
##  8 Row_male       BBall_female     0
##  9 Swim_female    BBall_female     0
## 10 Swim_male      BBall_female     0
## # … with 279 more rows</code></pre>
<p>This makes the <code>tidyverse</code> output, with frequencies. You
probably want to omit the zero ones:</p>
<div class="sourceCode" id="cb3610"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3610-1"><a href="discriminant-analysis.html#cb3610-1"></a>tab <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## # A tibble: 70 x 3
##    combo        class            n
##    &lt;chr&gt;        &lt;chr&gt;        &lt;int&gt;
##  1 BBall_female BBall_female     3
##  2 T400m_male   BBall_female     3
##  3 Tennis_male  BBall_female     1
##  4 WPolo_male   BBall_female     1
##  5 BBall_female BBall_male       1
##  6 BBall_male   BBall_male       9
##  7 Field_male   BBall_male       1
##  8 Row_male     BBall_male       2
##  9 Swim_male    BBall_male       4
## 10 T400m_male   BBall_male       1
## # … with 60 more rows</code></pre>
<p>This is the same output as below. See there for comments.</p>
<p>The other, perhaps easier, way to tackle this one is the
<code>tidyverse</code> way, making a “long” table of frequencies. Here is some of it. You’ll be able to click to see more:</p>
<div class="sourceCode" id="cb3612"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3612-1"><a href="discriminant-analysis.html#cb3612-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(combo, class)</span></code></pre></div>
<pre><code>##             combo          class  n
## 1    BBall_female   BBall_female  3
## 2    BBall_female     BBall_male  1
## 3    BBall_female Netball_female  5
## 4    BBall_female     Row_female  1
## 5    BBall_female     T400m_male  2
## 6    BBall_female     WPolo_male  1
## 7      BBall_male     BBall_male  9
## 8      BBall_male      Swim_male  2
## 9      BBall_male     WPolo_male  1
## 10   Field_female   Field_female  5
## 11   Field_female Netball_female  1
## 12   Field_female   T400m_female  1
## 13     Field_male     BBall_male  1
## 14     Field_male     Field_male  7
## 15     Field_male     Row_female  2
## 16     Field_male     WPolo_male  2
## 17     Gym_female     Gym_female  4
## 18 Netball_female   Field_female  1
## 19 Netball_female Netball_female 13
## 20 Netball_female     Row_female  4
## 21 Netball_female   T400m_female  1
## 22 Netball_female     T400m_male  4
## 23     Row_female     Gym_female  1
## 24     Row_female Netball_female  5
## 25     Row_female     Row_female 10
## 26     Row_female      Swim_male  1
## 27     Row_female     T400m_male  4
## 28     Row_female     WPolo_male  1
## 29       Row_male     BBall_male  2
## 30       Row_male     Field_male  1
## 31       Row_male       Row_male  1
## 32       Row_male   T400m_female  1
## 33       Row_male     WPolo_male 10
## 34    Swim_female Netball_female  4
## 35    Swim_female     Row_female  1
## 36    Swim_female   T400m_female  3
## 37    Swim_female     T400m_male  1
## 38      Swim_male     BBall_male  4
## 39      Swim_male Netball_female  2
## 40      Swim_male     Row_female  3
## 41      Swim_male     T400m_male  1
## 42      Swim_male     WPolo_male  3
## 43   T400m_female Netball_female  3
## 44   T400m_female   T400m_female  6
## 45   T400m_female     T400m_male  2
## 46     T400m_male   BBall_female  3
## 47     T400m_male     BBall_male  1
## 48     T400m_male Netball_female  5
## 49     T400m_male     Row_female  3
## 50     T400m_male   T400m_female  1
## 51     T400m_male     T400m_male  5
## 52  Tennis_female   Field_female  1
## 53  Tennis_female     Gym_female  1
## 54  Tennis_female Netball_female  2
## 55  Tennis_female   T400m_female  2
## 56  Tennis_female     T400m_male  1
## 57    Tennis_male   BBall_female  1
## 58    Tennis_male     Row_female  3
## 59  TSprnt_female Netball_female  1
## 60  TSprnt_female   T400m_female  2
## 61  TSprnt_female     T400m_male  1
## 62    TSprnt_male Netball_female  6
## 63    TSprnt_male     Row_female  3
## 64    TSprnt_male     WPolo_male  2
## 65     WPolo_male   BBall_female  1
## 66     WPolo_male     BBall_male  3
## 67     WPolo_male     Field_male  2
## 68     WPolo_male     Row_female  3
## 69     WPolo_male       Row_male  1
## 70     WPolo_male     WPolo_male  7</code></pre>
<p>The zeroes never show up here.
The <code>combo</code> column is the truth, and the <code>class</code> column
is the prediction. Again, you can see where the big frequencies are; a
lot of the female netball players were gotten right, but there were a
lot of them to begin with.</p>
<p>Extra: let’s see if we can work out <em>proportions</em> correct. I’ve
changed my mind from how I originally wrote this. I still use
<code>count</code>, but I start with the overall misclassification. Let’s
take it in steps:</p>
<div class="sourceCode" id="cb3614"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3614-1"><a href="discriminant-analysis.html#cb3614-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3614-2"><a href="discriminant-analysis.html#cb3614-2"></a><span class="st">  </span><span class="kw">count</span>(combo, class) <span class="op">%&gt;%</span></span>
<span id="cb3614-3"><a href="discriminant-analysis.html#cb3614-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">stat =</span> <span class="kw">ifelse</span>(combo <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>))</span></code></pre></div>
<pre><code>##             combo          class  n    stat
## 1    BBall_female   BBall_female  3 correct
## 2    BBall_female     BBall_male  1   wrong
## 3    BBall_female Netball_female  5   wrong
## 4    BBall_female     Row_female  1   wrong
## 5    BBall_female     T400m_male  2   wrong
## 6    BBall_female     WPolo_male  1   wrong
## 7      BBall_male     BBall_male  9 correct
## 8      BBall_male      Swim_male  2   wrong
## 9      BBall_male     WPolo_male  1   wrong
## 10   Field_female   Field_female  5 correct
## 11   Field_female Netball_female  1   wrong
## 12   Field_female   T400m_female  1   wrong
## 13     Field_male     BBall_male  1   wrong
## 14     Field_male     Field_male  7 correct
## 15     Field_male     Row_female  2   wrong
## 16     Field_male     WPolo_male  2   wrong
## 17     Gym_female     Gym_female  4 correct
## 18 Netball_female   Field_female  1   wrong
## 19 Netball_female Netball_female 13 correct
## 20 Netball_female     Row_female  4   wrong
## 21 Netball_female   T400m_female  1   wrong
## 22 Netball_female     T400m_male  4   wrong
## 23     Row_female     Gym_female  1   wrong
## 24     Row_female Netball_female  5   wrong
## 25     Row_female     Row_female 10 correct
## 26     Row_female      Swim_male  1   wrong
## 27     Row_female     T400m_male  4   wrong
## 28     Row_female     WPolo_male  1   wrong
## 29       Row_male     BBall_male  2   wrong
## 30       Row_male     Field_male  1   wrong
## 31       Row_male       Row_male  1 correct
## 32       Row_male   T400m_female  1   wrong
## 33       Row_male     WPolo_male 10   wrong
## 34    Swim_female Netball_female  4   wrong
## 35    Swim_female     Row_female  1   wrong
## 36    Swim_female   T400m_female  3   wrong
## 37    Swim_female     T400m_male  1   wrong
## 38      Swim_male     BBall_male  4   wrong
## 39      Swim_male Netball_female  2   wrong
## 40      Swim_male     Row_female  3   wrong
## 41      Swim_male     T400m_male  1   wrong
## 42      Swim_male     WPolo_male  3   wrong
## 43   T400m_female Netball_female  3   wrong
## 44   T400m_female   T400m_female  6 correct
## 45   T400m_female     T400m_male  2   wrong
## 46     T400m_male   BBall_female  3   wrong
## 47     T400m_male     BBall_male  1   wrong
## 48     T400m_male Netball_female  5   wrong
## 49     T400m_male     Row_female  3   wrong
## 50     T400m_male   T400m_female  1   wrong
## 51     T400m_male     T400m_male  5 correct
## 52  Tennis_female   Field_female  1   wrong
## 53  Tennis_female     Gym_female  1   wrong
## 54  Tennis_female Netball_female  2   wrong
## 55  Tennis_female   T400m_female  2   wrong
## 56  Tennis_female     T400m_male  1   wrong
## 57    Tennis_male   BBall_female  1   wrong
## 58    Tennis_male     Row_female  3   wrong
## 59  TSprnt_female Netball_female  1   wrong
## 60  TSprnt_female   T400m_female  2   wrong
## 61  TSprnt_female     T400m_male  1   wrong
## 62    TSprnt_male Netball_female  6   wrong
## 63    TSprnt_male     Row_female  3   wrong
## 64    TSprnt_male     WPolo_male  2   wrong
## 65     WPolo_male   BBall_female  1   wrong
## 66     WPolo_male     BBall_male  3   wrong
## 67     WPolo_male     Field_male  2   wrong
## 68     WPolo_male     Row_female  3   wrong
## 69     WPolo_male       Row_male  1   wrong
## 70     WPolo_male     WPolo_male  7 correct</code></pre>
<p>That makes a new column <code>stat</code> that contains whether the
predicted sport-gender combination was correct or wrong. For an
overall misclassification rate we have to count these, but <em>not</em>
simply counting the number of rows; rather, we need to total up the
things in the <code>n</code> column:</p>
<div class="sourceCode" id="cb3616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3616-1"><a href="discriminant-analysis.html#cb3616-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3616-2"><a href="discriminant-analysis.html#cb3616-2"></a><span class="st">  </span><span class="kw">count</span>(combo, class) <span class="op">%&gt;%</span></span>
<span id="cb3616-3"><a href="discriminant-analysis.html#cb3616-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">stat =</span> <span class="kw">ifelse</span>(combo <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3616-4"><a href="discriminant-analysis.html#cb3616-4"></a><span class="st">  </span><span class="kw">count</span>(stat, <span class="dt">wt =</span> n)</span></code></pre></div>
<pre><code>##      stat   n
## 1 correct  70
## 2   wrong 132</code></pre>
<p>This tells us how many predictions overall were right and how many
wrong.</p>
<p>To make those into proportions, another <code>mutate</code>, dividing by
the total of <code>n</code>:</p>
<div class="sourceCode" id="cb3618"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3618-1"><a href="discriminant-analysis.html#cb3618-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3618-2"><a href="discriminant-analysis.html#cb3618-2"></a><span class="st">  </span><span class="kw">count</span>(combo, class) <span class="op">%&gt;%</span></span>
<span id="cb3618-3"><a href="discriminant-analysis.html#cb3618-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">stat =</span> <span class="kw">ifelse</span>(combo <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3618-4"><a href="discriminant-analysis.html#cb3618-4"></a><span class="st">  </span><span class="kw">count</span>(stat, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3618-5"><a href="discriminant-analysis.html#cb3618-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</span></code></pre></div>
<pre><code>##      stat   n proportion
## 1 correct  70  0.3465347
## 2   wrong 132  0.6534653</code></pre>
<p>65% of the sport-gender combinations were misclassified. This is
awful, but is a lot better than guessing (we’d then get about 5% of
them right and about 95% wrong).</p>
<p>There’s a subtlety here that will make sense when we do the
corresponding calculation by sport-gender combination. To do
<em>that</em>, we put a <code>group_by(combo)</code> either before or after
we define <code>stat</code> (it doesn’t matter which way):</p>
<div class="sourceCode" id="cb3620"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3620-1"><a href="discriminant-analysis.html#cb3620-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3620-2"><a href="discriminant-analysis.html#cb3620-2"></a><span class="st">  </span><span class="kw">count</span>(combo, class) <span class="op">%&gt;%</span></span>
<span id="cb3620-3"><a href="discriminant-analysis.html#cb3620-3"></a><span class="st">  </span><span class="kw">group_by</span>(combo) <span class="op">%&gt;%</span></span>
<span id="cb3620-4"><a href="discriminant-analysis.html#cb3620-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">stat =</span> <span class="kw">ifelse</span>(combo <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3620-5"><a href="discriminant-analysis.html#cb3620-5"></a><span class="st">  </span><span class="kw">count</span>(stat, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3620-6"><a href="discriminant-analysis.html#cb3620-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</span></code></pre></div>
<pre><code>## # A tibble: 27 x 4
## # Groups:   combo [17]
##    combo          stat        n proportion
##    &lt;chr&gt;          &lt;chr&gt;   &lt;int&gt;      &lt;dbl&gt;
##  1 BBall_female   correct     3      0.231
##  2 BBall_female   wrong      10      0.769
##  3 BBall_male     correct     9      0.75 
##  4 BBall_male     wrong       3      0.25 
##  5 Field_female   correct     5      0.714
##  6 Field_female   wrong       2      0.286
##  7 Field_male     correct     7      0.583
##  8 Field_male     wrong       5      0.417
##  9 Gym_female     correct     4      1    
## 10 Netball_female correct    13      0.565
## # … with 17 more rows</code></pre>
<p>That last <code>sum(n)</code>: what is it summing over? The answer is
“within <code>combo</code>”, since that is the <code>group_by</code>. You
see that the two <code>proportion</code> values within, say,
<code>BBall_female</code>, add up to 1.</p>
<p>We don’t actually see all the answers, because there are too many of
them. Let’s try to get the proportion correct and wrong in their own
columns. This almost works:</p>
<div class="sourceCode" id="cb3622"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3622-1"><a href="discriminant-analysis.html#cb3622-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3622-2"><a href="discriminant-analysis.html#cb3622-2"></a><span class="st">  </span><span class="kw">count</span>(combo, class) <span class="op">%&gt;%</span></span>
<span id="cb3622-3"><a href="discriminant-analysis.html#cb3622-3"></a><span class="st">  </span><span class="kw">group_by</span>(combo) <span class="op">%&gt;%</span></span>
<span id="cb3622-4"><a href="discriminant-analysis.html#cb3622-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">stat =</span> <span class="kw">ifelse</span>(combo <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3622-5"><a href="discriminant-analysis.html#cb3622-5"></a><span class="st">  </span><span class="kw">count</span>(stat, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3622-6"><a href="discriminant-analysis.html#cb3622-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span></span>
<span id="cb3622-7"><a href="discriminant-analysis.html#cb3622-7"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from=</span>stat, <span class="dt">values_from=</span>proportion)</span></code></pre></div>
<pre><code>## # A tibble: 27 x 4
## # Groups:   combo [17]
##    combo              n correct  wrong
##    &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;
##  1 BBall_female       3   0.231 NA    
##  2 BBall_female      10  NA      0.769
##  3 BBall_male         9   0.75  NA    
##  4 BBall_male         3  NA      0.25 
##  5 Field_female       5   0.714 NA    
##  6 Field_female       2  NA      0.286
##  7 Field_male         7   0.583 NA    
##  8 Field_male         5  NA      0.417
##  9 Gym_female         4   1     NA    
## 10 Netball_female    13   0.565 NA    
## # … with 17 more rows</code></pre>
<p>This doesn’t work because everything outside of the <code>pivot_wider</code> is
tested for uniqueness; if it’s unique, it gets its own row. Thus,
<code>BBall_male</code> and 3 is different from <code>BBall_male</code> and
9. But we only want one row of <code>BBall_male</code>. I think the
easiest way around this is to get rid of <code>n</code>, since it has
served its purpose:</p>
<div class="sourceCode" id="cb3624"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3624-1"><a href="discriminant-analysis.html#cb3624-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3624-2"><a href="discriminant-analysis.html#cb3624-2"></a><span class="st">  </span><span class="kw">count</span>(combo, class) <span class="op">%&gt;%</span></span>
<span id="cb3624-3"><a href="discriminant-analysis.html#cb3624-3"></a><span class="st">  </span><span class="kw">group_by</span>(combo) <span class="op">%&gt;%</span></span>
<span id="cb3624-4"><a href="discriminant-analysis.html#cb3624-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">stat =</span> <span class="kw">ifelse</span>(combo <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3624-5"><a href="discriminant-analysis.html#cb3624-5"></a><span class="st">  </span><span class="kw">count</span>(stat, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3624-6"><a href="discriminant-analysis.html#cb3624-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span></span>
<span id="cb3624-7"><a href="discriminant-analysis.html#cb3624-7"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>n) <span class="op">%&gt;%</span></span>
<span id="cb3624-8"><a href="discriminant-analysis.html#cb3624-8"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from=</span>stat, <span class="dt">values_from=</span>proportion, <span class="dt">values_fill =</span> <span class="kw">list</span>(<span class="dt">proportion=</span><span class="dv">0</span>))</span></code></pre></div>
<pre><code>## # A tibble: 17 x 3
## # Groups:   combo [17]
##    combo          correct wrong
##    &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;
##  1 BBall_female    0.231  0.769
##  2 BBall_male      0.75   0.25 
##  3 Field_female    0.714  0.286
##  4 Field_male      0.583  0.417
##  5 Gym_female      1      0    
##  6 Netball_female  0.565  0.435
##  7 Row_female      0.455  0.545
##  8 Row_male        0.0667 0.933
##  9 Swim_female     0      1    
## 10 Swim_male       0      1    
## 11 T400m_female    0.545  0.455
## 12 T400m_male      0.278  0.722
## 13 Tennis_female   0      1    
## 14 Tennis_male     0      1    
## 15 TSprnt_female   0      1    
## 16 TSprnt_male     0      1    
## 17 WPolo_male      0.412  0.588</code></pre>
<p>One extra thing: some of the <code>proportion</code> values were missing, because there weren’t any misclassified (or maybe correctly-classified!) athletes. The <code>values_fill</code> sets any missings in <code>proportion</code> to zero.</p>
<p>While we’re about it, let’s arrange in
order of misclassification probability:</p>
<div class="sourceCode" id="cb3626"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3626-1"><a href="discriminant-analysis.html#cb3626-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3626-2"><a href="discriminant-analysis.html#cb3626-2"></a><span class="st">  </span><span class="kw">count</span>(combo, class) <span class="op">%&gt;%</span></span>
<span id="cb3626-3"><a href="discriminant-analysis.html#cb3626-3"></a><span class="st">  </span><span class="kw">group_by</span>(combo) <span class="op">%&gt;%</span></span>
<span id="cb3626-4"><a href="discriminant-analysis.html#cb3626-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">stat =</span> <span class="kw">ifelse</span>(combo <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3626-5"><a href="discriminant-analysis.html#cb3626-5"></a><span class="st">  </span><span class="kw">count</span>(stat, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb3626-6"><a href="discriminant-analysis.html#cb3626-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span></span>
<span id="cb3626-7"><a href="discriminant-analysis.html#cb3626-7"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>n) <span class="op">%&gt;%</span></span>
<span id="cb3626-8"><a href="discriminant-analysis.html#cb3626-8"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from=</span>stat, <span class="dt">values_from=</span>proportion, <span class="dt">values_fill =</span> <span class="kw">list</span>(<span class="dt">proportion=</span><span class="dv">0</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3626-9"><a href="discriminant-analysis.html#cb3626-9"></a><span class="st">  </span><span class="kw">replace_na</span>(<span class="kw">list</span>(<span class="dt">correct =</span> <span class="dv">0</span>, <span class="dt">wrong =</span> <span class="dv">0</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3626-10"><a href="discriminant-analysis.html#cb3626-10"></a><span class="st">  </span><span class="kw">arrange</span>(wrong)</span></code></pre></div>
<pre><code>## # A tibble: 17 x 3
## # Groups:   combo [17]
##    combo          correct wrong
##    &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;
##  1 Gym_female      1      0    
##  2 BBall_male      0.75   0.25 
##  3 Field_female    0.714  0.286
##  4 Field_male      0.583  0.417
##  5 Netball_female  0.565  0.435
##  6 T400m_female    0.545  0.455
##  7 Row_female      0.455  0.545
##  8 WPolo_male      0.412  0.588
##  9 T400m_male      0.278  0.722
## 10 BBall_female    0.231  0.769
## 11 Row_male        0.0667 0.933
## 12 Swim_female     0      1    
## 13 Swim_male       0      1    
## 14 Tennis_female   0      1    
## 15 Tennis_male     0      1    
## 16 TSprnt_female   0      1    
## 17 TSprnt_male     0      1</code></pre>
<p>The most distinctive athletes were the female gymnasts (tiny!),
followed by the male basketball players (tall) and the female field
athletes (heavy). These were easiest to predict from their height and
weight. The ones at the bottom of the list were very confusible since
the discriminant analysis guessed them all wrong!
So what were the most common <em>misclassifications</em>? Let’s go back
to this:</p>
<div class="sourceCode" id="cb3628"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3628-1"><a href="discriminant-analysis.html#cb3628-1"></a><span class="kw">head</span>(d)</span></code></pre></div>
<pre><code>##            combo  RCC  WCC   Hc   Hg Ferr   BMI   SSF %Bfat   LBM    Ht   Wt
## 1 Netball_female 4.56 13.3 42.2 13.6   20 19.16  49.0 11.29 53.14 176.8 59.9
## 2 Netball_female 4.15  6.0 38.0 12.7   59 21.15 110.2 25.26 47.09 172.6 63.0
## 3 Netball_female 4.16  7.6 37.5 12.3   22 21.40  89.0 19.39 53.44 176.0 66.3
## 4 Netball_female 4.32  6.4 37.7 12.3   30 21.03  98.3 19.63 48.78 169.9 60.7
## 5 Netball_female 4.06  5.8 38.7 12.8   78 21.77 122.1 23.11 56.05 183.0 72.9
## 6 Netball_female 4.12  6.1 36.6 11.8   21 21.38  90.4 16.86 56.45 178.2 67.9
##            class posterior.BBall_female posterior.BBall_male posterior.Field_female
## 1     T400m_male             0.12348360         3.479619e-04           0.0002835604
## 2 Netball_female             0.04927852         7.263143e-05           0.0041253799
## 3 Netball_female             0.08402197         4.567927e-04           0.0032633771
## 4 Netball_female             0.02820743         1.539520e-05           0.0048909758
## 5     Row_female             0.15383834         1.197089e-02           0.0011443415
## 6 Netball_female             0.11219817         1.320889e-03           0.0021761290
##   posterior.Field_male posterior.Gym_female posterior.Netball_female posterior.Row_female
## 1         1.460578e-05         4.206308e-05                0.1699941            0.1241779
## 2         9.838207e-05         3.101597e-04                0.2333569            0.1414225
## 3         2.676308e-04         3.414854e-05                0.2291353            0.1816810
## 4         4.524089e-05         1.531681e-03                0.2122221            0.1045723
## 5         1.169322e-03         2.247239e-07                0.1326885            0.1822427
## 6         3.751161e-04         8.019783e-06                0.2054332            0.1917380
##   posterior.Row_male posterior.Swim_female posterior.Swim_male posterior.T400m_female
## 1       0.0023825007            0.07434038         0.011678465            0.103051973
## 2       0.0025370630            0.11730520         0.009274681            0.119270442
## 3       0.0077872436            0.08659049         0.023136399            0.058696177
## 4       0.0009826883            0.13254329         0.004132741            0.179336337
## 5       0.0456717871            0.02802782         0.089868173            0.008428382
## 6       0.0133925352            0.06557996         0.036249576            0.036328215
##   posterior.T400m_male posterior.Tennis_female posterior.Tennis_male
## 1           0.25594274             0.047204095           0.017883433
## 2           0.13618567             0.075858992           0.008601514
## 3           0.17305732             0.035224944           0.017564554
## 4           0.09812128             0.120824963           0.004345342
## 5           0.17333438             0.004456769           0.046106286
## 6           0.19213811             0.020599135           0.025565109
##   posterior.TSprnt_female posterior.TSprnt_male posterior.WPolo_male      x.LD1
## 1             0.040192120            0.02616911         0.0028113441 -1.3251857
## 2             0.050772549            0.04902216         0.0025072687 -1.4873604
## 3             0.028170015            0.06274649         0.0081661381 -0.9595628
## 4             0.070141970            0.03716409         0.0009221631 -1.8846129
## 5             0.005144923            0.06163897         0.0542681927  0.1138304
## 6             0.018513425            0.06367698         0.0147074229 -0.6545817
##         x.LD2
## 1 -1.34799600
## 2 -0.15015145
## 3 -0.36154960
## 4  0.05956819
## 5 -0.82211820
## 6 -0.56820566</code></pre>
<div class="sourceCode" id="cb3630"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3630-1"><a href="discriminant-analysis.html#cb3630-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3630-2"><a href="discriminant-analysis.html#cb3630-2"></a><span class="st">  </span><span class="kw">count</span>(combo, class) <span class="op">%&gt;%</span></span>
<span id="cb3630-3"><a href="discriminant-analysis.html#cb3630-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">stat =</span> <span class="kw">ifelse</span>(combo <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>))</span></code></pre></div>
<pre><code>##             combo          class  n    stat
## 1    BBall_female   BBall_female  3 correct
## 2    BBall_female     BBall_male  1   wrong
## 3    BBall_female Netball_female  5   wrong
## 4    BBall_female     Row_female  1   wrong
## 5    BBall_female     T400m_male  2   wrong
## 6    BBall_female     WPolo_male  1   wrong
## 7      BBall_male     BBall_male  9 correct
## 8      BBall_male      Swim_male  2   wrong
## 9      BBall_male     WPolo_male  1   wrong
## 10   Field_female   Field_female  5 correct
## 11   Field_female Netball_female  1   wrong
## 12   Field_female   T400m_female  1   wrong
## 13     Field_male     BBall_male  1   wrong
## 14     Field_male     Field_male  7 correct
## 15     Field_male     Row_female  2   wrong
## 16     Field_male     WPolo_male  2   wrong
## 17     Gym_female     Gym_female  4 correct
## 18 Netball_female   Field_female  1   wrong
## 19 Netball_female Netball_female 13 correct
## 20 Netball_female     Row_female  4   wrong
## 21 Netball_female   T400m_female  1   wrong
## 22 Netball_female     T400m_male  4   wrong
## 23     Row_female     Gym_female  1   wrong
## 24     Row_female Netball_female  5   wrong
## 25     Row_female     Row_female 10 correct
## 26     Row_female      Swim_male  1   wrong
## 27     Row_female     T400m_male  4   wrong
## 28     Row_female     WPolo_male  1   wrong
## 29       Row_male     BBall_male  2   wrong
## 30       Row_male     Field_male  1   wrong
## 31       Row_male       Row_male  1 correct
## 32       Row_male   T400m_female  1   wrong
## 33       Row_male     WPolo_male 10   wrong
## 34    Swim_female Netball_female  4   wrong
## 35    Swim_female     Row_female  1   wrong
## 36    Swim_female   T400m_female  3   wrong
## 37    Swim_female     T400m_male  1   wrong
## 38      Swim_male     BBall_male  4   wrong
## 39      Swim_male Netball_female  2   wrong
## 40      Swim_male     Row_female  3   wrong
## 41      Swim_male     T400m_male  1   wrong
## 42      Swim_male     WPolo_male  3   wrong
## 43   T400m_female Netball_female  3   wrong
## 44   T400m_female   T400m_female  6 correct
## 45   T400m_female     T400m_male  2   wrong
## 46     T400m_male   BBall_female  3   wrong
## 47     T400m_male     BBall_male  1   wrong
## 48     T400m_male Netball_female  5   wrong
## 49     T400m_male     Row_female  3   wrong
## 50     T400m_male   T400m_female  1   wrong
## 51     T400m_male     T400m_male  5 correct
## 52  Tennis_female   Field_female  1   wrong
## 53  Tennis_female     Gym_female  1   wrong
## 54  Tennis_female Netball_female  2   wrong
## 55  Tennis_female   T400m_female  2   wrong
## 56  Tennis_female     T400m_male  1   wrong
## 57    Tennis_male   BBall_female  1   wrong
## 58    Tennis_male     Row_female  3   wrong
## 59  TSprnt_female Netball_female  1   wrong
## 60  TSprnt_female   T400m_female  2   wrong
## 61  TSprnt_female     T400m_male  1   wrong
## 62    TSprnt_male Netball_female  6   wrong
## 63    TSprnt_male     Row_female  3   wrong
## 64    TSprnt_male     WPolo_male  2   wrong
## 65     WPolo_male   BBall_female  1   wrong
## 66     WPolo_male     BBall_male  3   wrong
## 67     WPolo_male     Field_male  2   wrong
## 68     WPolo_male     Row_female  3   wrong
## 69     WPolo_male       Row_male  1   wrong
## 70     WPolo_male     WPolo_male  7 correct</code></pre>
<p>We want to express those <code>n</code> values as proportions out of their
actual sport-gender combo, so we group by <code>combo</code> before
defining the proportions:</p>
<div class="sourceCode" id="cb3632"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3632-1"><a href="discriminant-analysis.html#cb3632-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3632-2"><a href="discriminant-analysis.html#cb3632-2"></a><span class="st">  </span><span class="kw">count</span>(combo, class) <span class="op">%&gt;%</span></span>
<span id="cb3632-3"><a href="discriminant-analysis.html#cb3632-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">stat =</span> <span class="kw">ifelse</span>(combo <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3632-4"><a href="discriminant-analysis.html#cb3632-4"></a><span class="st">  </span><span class="kw">group_by</span>(combo) <span class="op">%&gt;%</span></span>
<span id="cb3632-5"><a href="discriminant-analysis.html#cb3632-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</span></code></pre></div>
<pre><code>## # A tibble: 70 x 5
## # Groups:   combo [17]
##    combo        class              n stat    proportion
##    &lt;chr&gt;        &lt;fct&gt;          &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
##  1 BBall_female BBall_female       3 correct     0.231 
##  2 BBall_female BBall_male         1 wrong       0.0769
##  3 BBall_female Netball_female     5 wrong       0.385 
##  4 BBall_female Row_female         1 wrong       0.0769
##  5 BBall_female T400m_male         2 wrong       0.154 
##  6 BBall_female WPolo_male         1 wrong       0.0769
##  7 BBall_male   BBall_male         9 correct     0.75  
##  8 BBall_male   Swim_male          2 wrong       0.167 
##  9 BBall_male   WPolo_male         1 wrong       0.0833
## 10 Field_female Field_female       5 correct     0.714 
## # … with 60 more rows</code></pre>
<p>Only pick out the ones that were gotten wrong, and arrange the remaining
proportions in descending order:</p>
<div class="sourceCode" id="cb3634"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3634-1"><a href="discriminant-analysis.html#cb3634-1"></a>d <span class="op">%&gt;%</span></span>
<span id="cb3634-2"><a href="discriminant-analysis.html#cb3634-2"></a><span class="st">  </span><span class="kw">count</span>(combo, class) <span class="op">%&gt;%</span></span>
<span id="cb3634-3"><a href="discriminant-analysis.html#cb3634-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">stat =</span> <span class="kw">ifelse</span>(combo <span class="op">==</span><span class="st"> </span>class, <span class="st">&quot;correct&quot;</span>, <span class="st">&quot;wrong&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb3634-4"><a href="discriminant-analysis.html#cb3634-4"></a><span class="st">  </span><span class="kw">group_by</span>(combo) <span class="op">%&gt;%</span></span>
<span id="cb3634-5"><a href="discriminant-analysis.html#cb3634-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span></span>
<span id="cb3634-6"><a href="discriminant-analysis.html#cb3634-6"></a><span class="st">  </span><span class="kw">filter</span>(stat <span class="op">==</span><span class="st"> &quot;wrong&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb3634-7"><a href="discriminant-analysis.html#cb3634-7"></a><span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(proportion))</span></code></pre></div>
<pre><code>## # A tibble: 59 x 5
## # Groups:   combo [16]
##    combo         class              n stat  proportion
##    &lt;chr&gt;         &lt;fct&gt;          &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;
##  1 Tennis_male   Row_female         3 wrong      0.75 
##  2 Row_male      WPolo_male        10 wrong      0.667
##  3 TSprnt_male   Netball_female     6 wrong      0.545
##  4 TSprnt_female T400m_female       2 wrong      0.5  
##  5 Swim_female   Netball_female     4 wrong      0.444
##  6 BBall_female  Netball_female     5 wrong      0.385
##  7 Swim_female   T400m_female       3 wrong      0.333
##  8 Swim_male     BBall_male         4 wrong      0.308
##  9 Tennis_female Netball_female     2 wrong      0.286
## 10 Tennis_female T400m_female       2 wrong      0.286
## # … with 49 more rows</code></pre>
<p>The embarrassment champion is the three male tennis players that were
taken to be — female rowers! Most of the other mistakes are more
forgivable: the male rowers being taken for male water polo players,
for example.</p>

</div>
</div>
<p style="text-align: center;">
<a href="repeated-measures.html"><button class="btn btn-default">Previous</button></a>
<a href="cluster-analysis.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
