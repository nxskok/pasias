[["logistic-regression-with-nominal-response.html", "Chapter 28 Logistic regression with nominal response 28.1 Finding non-missing values 28.2 European Social Survey and voting 28.3 Alligator food 28.4 Crimes in San Francisco 28.5 Crimes in San Francisco – the data 28.6 What sports do these athletes play? 28.7 Finding non-missing values 28.8 European Social Survey and voting 28.9 Alligator food 28.10 Crimes in San Francisco 28.11 Crimes in San Francisco – the data 28.12 What sports do these athletes play?", " Chapter 28 Logistic regression with nominal response library(nnet) library(tidyverse) 28.1 Finding non-missing values * This is to prepare you for something in the next question. It’s meant to be easy. In R, the code NA stands for “missing value” or “value not known”. In R, NA should not have quotes around it. (It is a special code, not a piece of text.) Create a vector v that contains some numbers and some missing values, using c(). Put those values into a one-column data frame. Obtain a new column containing is.na(v). When is this true and when is this false? The symbol ! means “not” in R (and other programming languages). What does !is.na(v) do? Create a new column containing that. Use filter to display just the rows of your data frame that have a non-missing value of v. 28.2 European Social Survey and voting The European Social Survey is a giant survey carried out across Europe covering demographic information, attitudes to and amount of education, politics and so on. In this question, we will investigate what might make British people vote for a certain political party. The information for this question is in a (large) spreadsheet at link. There is also a “codebook” at link that tells you what all the variables are. The ones we will use are the last five columns of the spreadsheet, described on pages 11 onwards of the codebook. (I could have given you more, but I didn’t want to make this any more complicated than it already was.) Read in the .csv file, and verify that you have lots of rows and columns. * Use the codebook to find out what the columns prtvtgb, gndr, agea, eduyrs and inwtm are. What do the values 1 and 2 for gndr mean? (You don’t, at this point, have to worry about the values for the other variables.) The three major political parties in Britain are the Conservative, Labour and Liberal Democrat. (These, for your information, correspond roughly to the Canadian Progressive Conservative, NDP and Liberal parties.) For the variable that corresponds to “political party voted for at the last election”, which values correspond to these three parties? Normally, I would give you a tidied-up data set. But I figure you could use some practice tidying this one up. As the codebook shows, there are some numerical codes for missing values, and we want to omit those. We want just the columns prtvtgb through inwtm from the right side of the spreadsheet. Use dplyr or tidyr tools to (i) select only these columns, (ii) include the rows that correspond to people who voted for one of the three major parties, (iii) include the rows that have an age at interview less than 999, (iv) include the rows that have less than 40 years of education, (v) include the rows that are not missing on inwtm (use the idea from Question~here for (v)). The last four of those (the inclusion of rows) can be done in one go. Why is my response variable nominal rather than ordinal? How can I tell? Which R function should I use, therefore, to fit my model? * Take the political party voted for, and turn it into a factor, by feeding it into factor. Fit an appropriate model to predict political party voted for at the last election (as a factor) from all the other variables. Gender is really a categorical variable too, but since there are only two possible values it can be treated as a number. We have a lot of explanatory variables. The standard way to test whether we need all of them is to take one of them out at a time, and test which ones we can remove. This is a lot of work. We won’t do that. Instead, the R function step does what you want. You feed step two things: a fitted model object, and the option trace=0 (otherwise you get a lot of output). The final part of the output from step tells you which explanatory variables you need to keep. Run step on your fitted model. Which explanatory variables need to stay in the model here? Fit the model indicated by step (in the last part). I didn’t think that interview length could possibly be relevant to which party a person voted for. Test whether interview length can be removed from your model of the last part. What do you conclude? (Note that step and this test may disagree.) Use your best model to obtain predictions from some suitably chosen combinations of values of the explanatory variables that remain. (If you have quantitative explanatory variables left, you could use their first and third quartiles as values to predict from. Running summary on the data frame will get summaries of all the variables.) What is the effect of increasing age? What is the effect of an increase in years of education? 28.3 Alligator food What do alligators most like to eat? 219 alligators were captured in four Florida lakes. Each alligator’s stomach contents were observed, and the food that the alligator had eaten was classified into one of five categories: fish, invertebrates (such as snails or insects), reptiles (such as turtles), birds, and “other” (such as amphibians, plants or rocks). The researcher noted for each alligator what that alligator had most of in its stomach, as well as the gender of each alligator and whether it was “large” or “small” (greater or less than 2.3 metres in length). The data can be found in link. The numbers in the data set (apart from the first column) are all frequencies. (You can ignore that first column “profile”.) Our aim is to predict food type from the other variables. Read in the data and display the first few lines. Describe how the data are not “tidy”. Use pivot_longer to arrange the data suitably for analysis (which will be using multinom). Demonstrate (by looking at the first few rows of your new data frame) that you now have something tidy. What is different about this problem, compared to Question here, that would make multinom the right tool to use? Fit a suitable multinomial model predicting food type from gender, size and lake. Does each row represent one alligator or more than one? If more than one, account for this in your modelling. Do a test to see whether Gender should stay in the model. (This will entail fitting another model.) What do you conclude? Predict the probability that an alligator prefers each food type, given its size, gender (if necessary) and the lake it was found in, using the more appropriate of the two models that you have fitted so far. This means (i) obtaining all the sizes and lake names, (ii) making a data frame for prediction, and (iii) obtaining and displaying the predicted probabilities. What do you think is the most important way in which the lakes differ? (Hint: look at where the biggest predicted probabilities are.) How would you describe the major difference between the diets of the small and large alligators? 28.4 Crimes in San Francisco The data in link is a subset of a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are: Dates: the date and time of the crime Category: the category of crime, eg. “larceny” or “vandalism” (response). Descript: detailed description of crime. DayOfWeek: the day of the week of the crime. PdDistrict: the name of the San Francisco Police Department district in which the crime took place. Resolution: how the crime was resolved Address: approximate street address of crime X: longitude Y: latitude Our aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which are the ones included in this data file. Some of the model-fitting takes a while (you’ll see why below). If you’re using R Markdown, you can wait for the models to fit each time you re-run your document, or insert cache=T in the top line of your code chunk (the one with r in curly brackets in it, above the actual code). Put a comma and the cache=T inside the curly brackets. What that does is to re-run that code chunk only if it changes; if it hasn’t changed it will use the saved results from last time it was run. That can save you a lot of waiting around. Read in the data and display the dataset (or, at least, part of it). Fit a multinomial logistic regression that predicts crime category from day of week and district. (You don’t need to look at it.) The model-fitting produces some output. (If you’re using R Markdown, that will come with it.) Fit a model that predicts Category from only the district. Hand in the output from the fitting process as well. Use anova to compare the two models you just obtained. What does the anova tell you? Using your preferred model, obtain predicted probabilities that a crime will be of each of these four categories for each day of the week in the TENDERLOIN district (the name is ALL CAPS). This will mean constructing a data frame to predict from, obtaining the predictions and then displaying them suitably. Describe briefly how the weekend days Saturday and Sunday differ from the rest. 28.5 Crimes in San Francisco – the data The data in link is a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are: Dates: the date and time of the crime Category: the category of crime, eg. “larceny” or “vandalism” (response). Descript: detailed description of crime. DayOfWeek: the day of the week of the crime. PdDistrict: the name of the San Francisco Police Department district in which the crime took place. Resolution: how the crime was resolved Address: approximate street address of crime X: longitude Y: latitude Our aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which we will have to discover. Read in the data and verify that you have these columns and a lot of rows. (The data may take a moment to read in. You will see why.) How is the response variable here different to the one in the question about steak preferences (and therefore why would multinom from package nnet be the method of choice)? Find out which crime categories there are, and arrange them in order of how many crimes there were in each category. Which are the four most frequent “interesting” crime categories, that is to say, not including “other offenses” and “non-criminal”? Get them into a vector called my.crimes. See if you can find a way of doing this that doesn’t involve typing them in (for full marks). (Digression, but needed for the next part.) The R vector letters contains the lowercase letters from a to z. Consider the vector ('a','m',3,'Q'). Some of these are found amongst the lowercase letters, and some not. Type these into a vector v and explain briefly why v %in% letters produces what it does. We are going to filter only the rows of our data frame that have one of the crimes in my.crimes as their Category. Also, select only the columns Category, DayOfWeek and PdDistrict. Save the resulting data frame and display its structure. (You should have a lot fewer rows than you did before.) Save these data in a file sfcrime1.csv. 28.6 What sports do these athletes play? The data at link are physical and physiological measurements of 202 male and female Australian elite athletes. The data values are separated by tabs. We are going to see whether we can predict the sport an athlete plays from their height and weight. The sports, if you care, are respectively basketball, “field athletics” (eg. shot put, javelin throw, long jump etc.), gymnastics, netball, rowing, swimming, 400m running, tennis, sprinting (100m or 200m running), water polo. Read in the data and display the first few rows. Make a scatterplot of height vs. weight, with the points coloured by what sport the athlete plays. Put height on the \\(x\\)-axis and weight on the \\(y\\)-axis. Explain briefly why a multinomial model (multinom from nnet) would be the best thing to use to predict sport played from the other variables. Fit a suitable model for predicting sport played from height and weight. (You don’t need to look at the results.) 100 steps isn’t quite enough, so set maxit equal to a larger number to allow the estimation to finish. Demonstrate using anova that Wt should not be removed from this model. Make a data frame consisting of all combinations of Ht 160, 180 and 200 (cm), and Wt 50, 75, and 100 (kg), and use it to obtain predicted probabilities of athletes of those heights and weights playing each of the sports. Display the results. You might have to display them smaller, or reduce the number of decimal places36 to fit them on the page. For an athlete who is 180 cm tall and weighs 100 kg, what sport would you guess they play? How sure are you that you are right? Explain briefly. My solutions follow: 28.7 Finding non-missing values * This is to prepare you for something in the next question. It’s meant to be easy. In R, the code NA stands for “missing value” or “value not known”. In R, NA should not have quotes around it. (It is a special code, not a piece of text.) Create a vector v that contains some numbers and some missing values, using c(). Put those values into a one-column data frame. Solution Like this. The arrangement of numbers and missing values doesn’t matter, as long as you have some of each: v &lt;- c(1, 2, NA, 4, 5, 6, 9, NA, 11) mydata &lt;- tibble(v) mydata ## # A tibble: 9 x 1 ## v ## &lt;dbl&gt; ## 1 1 ## 2 2 ## 3 NA ## 4 4 ## 5 5 ## 6 6 ## 7 9 ## 8 NA ## 9 11 This has one column called v. \\(\\blacksquare\\) Obtain a new column containing is.na(v). When is this true and when is this false? Solution mydata &lt;- mydata %&gt;% mutate(isna = is.na(v)) mydata ## # A tibble: 9 x 2 ## v isna ## &lt;dbl&gt; &lt;lgl&gt; ## 1 1 FALSE ## 2 2 FALSE ## 3 NA TRUE ## 4 4 FALSE ## 5 5 FALSE ## 6 6 FALSE ## 7 9 FALSE ## 8 NA TRUE ## 9 11 FALSE This is TRUE if the corresponding element of v is missing (in my case, the third value and the second-last one), and FALSE otherwise (when there is an actual value there). \\(\\blacksquare\\) The symbol ! means “not” in R (and other programming languages). What does !is.na(v) do? Create a new column containing that. Solution Try it and see. Give it whatever name you like. My name reflects that I know what it’s going to do: mydata &lt;- mydata %&gt;% mutate(notisna = !is.na(v)) mydata ## # A tibble: 9 x 3 ## v isna notisna ## &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 FALSE TRUE ## 2 2 FALSE TRUE ## 3 NA TRUE FALSE ## 4 4 FALSE TRUE ## 5 5 FALSE TRUE ## 6 6 FALSE TRUE ## 7 9 FALSE TRUE ## 8 NA TRUE FALSE ## 9 11 FALSE TRUE This is the logical opposite of is.na: it’s true if there is a value, and false if it’s missing. \\(\\blacksquare\\) Use filter to display just the rows of your data frame that have a non-missing value of v. Solution filter takes a column to say which rows to pick, in which case the column should contain something that either is TRUE or FALSE, or something that can be interpreted that way: mydata %&gt;% filter(notisna) ## # A tibble: 7 x 3 ## v isna notisna ## &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 FALSE TRUE ## 2 2 FALSE TRUE ## 3 4 FALSE TRUE ## 4 5 FALSE TRUE ## 5 6 FALSE TRUE ## 6 9 FALSE TRUE ## 7 11 FALSE TRUE or you can provide filter something that can be calculated from what’s in the data frame, and also returns something that is either true or false: mydata %&gt;% filter(!is.na(v)) ## # A tibble: 7 x 3 ## v isna notisna ## &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 FALSE TRUE ## 2 2 FALSE TRUE ## 3 4 FALSE TRUE ## 4 5 FALSE TRUE ## 5 6 FALSE TRUE ## 6 9 FALSE TRUE ## 7 11 FALSE TRUE In either case, I only have non-missing values of v. \\(\\blacksquare\\) 28.8 European Social Survey and voting The European Social Survey is a giant survey carried out across Europe covering demographic information, attitudes to and amount of education, politics and so on. In this question, we will investigate what might make British people vote for a certain political party. The information for this question is in a (large) spreadsheet at link. There is also a “codebook” at link that tells you what all the variables are. The ones we will use are the last five columns of the spreadsheet, described on pages 11 onwards of the codebook. (I could have given you more, but I didn’t want to make this any more complicated than it already was.) Read in the .csv file, and verify that you have lots of rows and columns. Solution The obvious way. Printing it out will display some of the data and tell you how many rows and columns you have: my_url &lt;- &quot;http://ritsokiguess.site/datafiles/ess.csv&quot; ess &lt;- read_csv(my_url) ## ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## cols( ## cntry = col_character(), ## cname = col_character(), ## cedition = col_double(), ## cproddat = col_character(), ## cseqno = col_double(), ## name = col_character(), ## essround = col_double(), ## edition = col_double(), ## idno = col_double(), ## dweight = col_double(), ## pspwght = col_double(), ## pweight = col_double(), ## prtvtgb = col_double(), ## gndr = col_double(), ## agea = col_double(), ## eduyrs = col_double(), ## inwtm = col_double() ## ) ess ## # A tibble: 2,286 x 17 ## cntry cname cedition cproddat cseqno name essround edition idno dweight pspwght pweight prtvtgb gndr agea eduyrs inwtm ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GB ESS1-6e01 1 26.11.2014 134168 ESS6e02_1 6 2.1 101014 1.01 0.895 2.28 1 2 63 16 35 ## 2 GB ESS1-6e01 1 26.11.2014 134169 ESS6e02_1 6 2.1 101048 2.02 1.49 2.28 1 2 51 13 75 ## 3 GB ESS1-6e01 1 26.11.2014 134170 ESS6e02_1 6 2.1 101055 1.01 0.907 2.28 1 1 67 13 39 ## 4 GB ESS1-6e01 1 26.11.2014 134171 ESS6e02_1 6 2.1 101089 0.505 0.454 2.28 66 1 86 10 77 ## 5 GB ESS1-6e01 1 26.11.2014 134172 ESS6e02_1 6 2.1 101097 0.505 0.448 2.28 2 2 80 9 69 ## 6 GB ESS1-6e01 1 26.11.2014 134173 ESS6e02_1 6 2.1 101113 1.01 0.907 2.28 1 1 67 21 46 ## 7 GB ESS1-6e01 1 26.11.2014 134174 ESS6e02_1 6 2.1 101121 0.505 0.448 2.28 7 2 69 17 162 ## 8 GB ESS1-6e01 1 26.11.2014 134175 ESS6e02_1 6 2.1 101139 0.505 0.517 2.28 66 1 36 15 30 ## 9 GB ESS1-6e01 1 26.11.2014 134176 ESS6e02_1 6 2.1 101154 1.01 1.03 2.28 1 1 50 13 54 ## 10 GB ESS1-6e01 1 26.11.2014 134177 ESS6e02_1 6 2.1 101170 1.01 0.895 2.28 2 2 82 16 37 ## # … with 2,276 more rows 2286 rows and 17 columns. \\(\\blacksquare\\) * Use the codebook to find out what the columns prtvtgb, gndr, agea, eduyrs and inwtm are. What do the values 1 and 2 for gndr mean? (You don’t, at this point, have to worry about the values for the other variables.) Solution Respectively, political party voted for at last election, gender (of respondent), age at interview, years of full-time education, length of interview (in minutes). For gndr, male is 1 and female is 2. \\(\\blacksquare\\) The three major political parties in Britain are the Conservative, Labour and Liberal Democrat. (These, for your information, correspond roughly to the Canadian Progressive Conservative, NDP and Liberal parties.) For the variable that corresponds to “political party voted for at the last election”, which values correspond to these three parties? Solution 1, 2 and 3 respectively. (That was easy!) \\(\\blacksquare\\) Normally, I would give you a tidied-up data set. But I figure you could use some practice tidying this one up. As the codebook shows, there are some numerical codes for missing values, and we want to omit those. We want just the columns prtvtgb through inwtm from the right side of the spreadsheet. Use dplyr or tidyr tools to (i) select only these columns, (ii) include the rows that correspond to people who voted for one of the three major parties, (iii) include the rows that have an age at interview less than 999, (iv) include the rows that have less than 40 years of education, (v) include the rows that are not missing on inwtm (use the idea from Question~here for (v)). The last four of those (the inclusion of rows) can be done in one go. Solution This seems to call for a pipeline. The major parties are numbered 1, 2 and 3, so we can select the ones less than 4 (or &lt;=3). The reference back to the last question is a hint to use !is.na(). ess %&gt;% select(prtvtgb:inwtm) %&gt;% filter(prtvtgb &lt; 4, agea &lt; 999, eduyrs &lt; 40, !is.na(inwtm)) -&gt; ess.major You might get a weird error in select, something about “unused argument”. If this happens to you, it’s not because you used select wrong, it’s because you used the wrong select! There is one in MASS, and you need to make sure that this package is “detached” so that you use the select you want, namely the one in dplyr, loaded with the tidyverse. Use the instructions at the end of the mobile phones question or the abortion question to do this. The other way around this is to say, instead of select, dplyr::select with two colons. This means “the select that lives in dplyr, no other”, and is what Wikipedia calls “disambiguation”: out of several things with the same name, you say which one you mean. If you do the pipeline, you will probably not get it right the first time. (I didn’t.) For debugging, try out one step at a time, and summarize what you have so far, so that you can check it for correctness. A handy trick for that is to make the last piece of your pipeline summary(), which produces a summary of the columns of the resulting data frame. For example, I first did this (note that my filter is a lot simpler than the one above): ess %&gt;% select(prtvtgb:inwtm) %&gt;% filter(prtvtgb &lt; 4, !is.na(inwtm)) %&gt;% summary() ## prtvtgb gndr agea eduyrs inwtm ## Min. :1.000 Min. :1.000 Min. : 18.00 Min. : 0.00 Min. : 7.00 ## 1st Qu.:1.000 1st Qu.:1.000 1st Qu.: 44.00 1st Qu.:11.00 1st Qu.: 35.00 ## Median :2.000 Median :2.000 Median : 58.00 Median :13.00 Median : 41.00 ## Mean :1.803 Mean :1.572 Mean : 61.74 Mean :14.23 Mean : 43.54 ## 3rd Qu.:2.000 3rd Qu.:2.000 3rd Qu.: 71.00 3rd Qu.:16.00 3rd Qu.: 50.00 ## Max. :3.000 Max. :2.000 Max. :999.00 Max. :88.00 Max. :160.00 The mean of a categorical variable like party voted for or gender doesn’t make much sense, but it looks as if all the values are sensible ones (1 to 3 and 1, 2 respectively). However, the maximum values of age and years of education look like missing value codes, hence the other requirements I put in the question.37 Displaying as the last step of your pipeline also works, but the advantage of summary is that you get to see whether there are any unusual values, in this case unusually large values that are missing value codes. \\(\\blacksquare\\) Why is my response variable nominal rather than ordinal? How can I tell? Which R function should I use, therefore, to fit my model? Solution The response variable is political party voted for. There is no (obvious) ordering to this (unless you want to try to place the parties on a left-right spectrum), so this is nominal, and you’ll need multinom from package nnet. If I had included the minor parties and you were working on a left-right spectrum, you would have had to decide where to put the somewhat libertarian Greens38 or the parties that exist only in Northern Ireland.39 \\(\\blacksquare\\) * Take the political party voted for, and turn it into a factor, by feeding it into factor. Fit an appropriate model to predict political party voted for at the last election (as a factor) from all the other variables. Gender is really a categorical variable too, but since there are only two possible values it can be treated as a number. Solution This, or something like it. multinom lives in package nnet, which you’ll have to install first if you haven’t already: library(nnet) ess.1 &lt;- multinom(factor(prtvtgb) ~ gndr + agea + eduyrs + inwtm, data = ess.major) ## # weights: 18 (10 variable) ## initial value 1343.602829 ## iter 10 value 1256.123798 ## final value 1247.110080 ## converged Or create a factor version of your response in the data frame first: ess.major &lt;- ess.major %&gt;% mutate(party = factor(prtvtgb)) and then: ess.1a &lt;- multinom(party ~ gndr + agea + eduyrs + inwtm, data = ess.major) ## # weights: 18 (10 variable) ## initial value 1343.602829 ## iter 10 value 1256.123798 ## final value 1247.110080 ## converged \\(\\blacksquare\\) We have a lot of explanatory variables. The standard way to test whether we need all of them is to take one of them out at a time, and test which ones we can remove. This is a lot of work. We won’t do that. Instead, the R function step does what you want. You feed step two things: a fitted model object, and the option trace=0 (otherwise you get a lot of output). The final part of the output from step tells you which explanatory variables you need to keep. Run step on your fitted model. Which explanatory variables need to stay in the model here? Solution I tried to give you lots of hints here: ess.2a &lt;- step(ess.1, trace = 0) ## trying - gndr ## trying - agea ## trying - eduyrs ## trying - inwtm ## # weights: 15 (8 variable) ## initial value 1343.602829 ## iter 10 value 1248.343563 ## final value 1248.253638 ## converged ## trying - agea ## trying - eduyrs ## trying - inwtm ess.2a ## Call: ## multinom(formula = factor(prtvtgb) ~ agea + eduyrs + inwtm, data = ess.major) ## ## Coefficients: ## (Intercept) agea eduyrs inwtm ## 2 1.632266 -0.02153694 -0.0593757 0.009615167 ## 3 -1.281031 -0.01869263 0.0886487 0.009337084 ## ## Residual Deviance: 2496.507 ## AIC: 2512.507 If you didn’t save your output in a variable, you’ll get my last bit automatically. The end of the output gives us coefficients for (and thus tells us we need to keep) age, years of education and interview length. The actual numbers don’t mean much; it’s the indication that the variable has stayed in the model that makes a difference.40 If you’re wondering about the process: first step tries to take out each explanatory variable, one at a time, from the starting model (the one that contains all the variables). Then it finds the best model out of those and fits it. (It doesn’t tell us which model this is, but evidently it’s the one without gender.) Then it takes that model and tries to remove its explanatory variables one at a time (there are only three of them left). Having decided it cannot remove any of them, it stops, and shows us what’s left. Leaving out the trace=0 shows more output and more detail on the process, but I figured this was enough (and this way, you don’t have to wade through all of that output). Try values like 1 or 2 for trace and see what you get. \\(\\blacksquare\\) Fit the model indicated by step (in the last part). Solution Copy and paste, and take out the variables you don’t need. Or, better, save the output from step in a variable. This then becomes a fitted model object and you can look at it any of the ways you can look at a model fit. I found that gender needed to be removed, but if yours is different, follow through with whatever your step said to do. ess.2 &lt;- multinom(party ~ agea + eduyrs + inwtm, data = ess.major) ## # weights: 15 (8 variable) ## initial value 1343.602829 ## iter 10 value 1248.343563 ## final value 1248.253638 ## converged If you saved the output from step, you’ll already have this and you don’t need to do it again: anova(ess.2, ess.2a) ## Likelihood ratio tests of Multinomial Models ## ## Response: party ## Response: factor(prtvtgb) ## Model Resid. df Resid. Dev Test Df LR stat. Pr(Chi) ## 1 agea + eduyrs + inwtm 2438 2496.507 ## 2 agea + eduyrs + inwtm 2438 2496.507 1 vs 2 0 0 1 Same model. \\(\\blacksquare\\) I didn’t think that interview length could possibly be relevant to which party a person voted for. Test whether interview length can be removed from your model of the last part. What do you conclude? (Note that step and this test may disagree.) Solution Fit the model without inwtm: ess.3 &lt;- multinom(party ~ agea + eduyrs, data = ess.major) ## # weights: 12 (6 variable) ## initial value 1343.602829 ## iter 10 value 1250.418281 ## final value 1250.417597 ## converged and then use anova to compare them: anova(ess.3, ess.2) ## Likelihood ratio tests of Multinomial Models ## ## Response: party ## Model Resid. df Resid. Dev Test Df LR stat. Pr(Chi) ## 1 agea + eduyrs 2440 2500.835 ## 2 agea + eduyrs + inwtm 2438 2496.507 1 vs 2 2 4.327917 0.1148695 The P-value, 0.1149, is not small, which says that the smaller model is good, ie. the one without interview length. I thought drop1 would also work here, but it appears not to: drop1(ess.1, test = &quot;Chisq&quot;) ## trying - gndr ## Error in if (trace) {: argument is not interpretable as logical I think that’s a bug in multinom, since normally if step works, then drop1 will work too (normally step uses drop1). The reason for the disagreement between step and anova is that step will tend to keep marginal explanatory variables, that is, ones that are “potentially interesting” but whose P-values might not be less than 0.05. There is still no substitute for your judgement in figuring out what to do! step uses a thing called AIC to decide what to do, rather than actually doing a test. If you know about “adjusted R-squared” in choosing explanatory variables for a regression, it’s the same idea: a variable can be not quite significant but still make the adjusted R-squared go up (typically only a little). \\(\\blacksquare\\) Use your best model to obtain predictions from some suitably chosen combinations of values of the explanatory variables that remain. (If you have quantitative explanatory variables left, you could use their first and third quartiles as values to predict from. Running summary on the data frame will get summaries of all the variables.) Solution First make our new data frame of values to predict from. crossing is our friend. You can use quantile or summary to find the quartiles. I only had agea and eduyrs left, having decided that interview time really ought to come out: summary(ess.major) ## prtvtgb gndr agea eduyrs inwtm party ## Min. :1.000 Min. :1.000 Min. :18.00 Min. : 0.00 Min. : 7.0 1:484 ## 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:44.00 1st Qu.:11.00 1st Qu.: 35.0 2:496 ## Median :2.000 Median :2.000 Median :58.00 Median :13.00 Median : 41.0 3:243 ## Mean :1.803 Mean :1.574 Mean :57.19 Mean :13.45 Mean : 43.7 ## 3rd Qu.:2.000 3rd Qu.:2.000 3rd Qu.:71.00 3rd Qu.:16.00 3rd Qu.: 50.0 ## Max. :3.000 Max. :2.000 Max. :94.00 Max. :33.00 Max. :160.0 Quartiles for age are 44 and 71, and for years of education are 11 and 16: new &lt;- crossing(agea = c(44, 71), eduyrs = c(11, 16)) new ## # A tibble: 4 x 2 ## agea eduyrs ## &lt;dbl&gt; &lt;dbl&gt; ## 1 44 11 ## 2 44 16 ## 3 71 11 ## 4 71 16 Now we feed this into predict. An annoying feature of this kind of prediction is that type may not be what you expect. The best model is the one I called ess.3: pp &lt;- predict(ess.3, new, type = &quot;probs&quot;) cbind(new, pp) ## agea eduyrs 1 2 3 ## 1 44 11 0.3331882 0.5093898 0.1574220 ## 2 44 16 0.3471967 0.3955666 0.2572367 ## 3 71 11 0.4551429 0.4085120 0.1363451 ## 4 71 16 0.4675901 0.3127561 0.2196538 \\(\\blacksquare\\) What is the effect of increasing age? What is the effect of an increase in years of education? Solution To assess the effect of age, hold years of education constant. Thus, compare lines 1 and 3 (or 2 and 4): increasing age tends to increase the chance that a person will vote Conservative (party 1), and decrease the chance that a person will vote Labour (party 2). There doesn’t seem to be much effect of age on the chance that a person will vote Liberal Democrat. To assess education, hold age constant, and thus compare rows 1 and 2 (or rows 3 and 4). This time, there isn’t much effect on the chances of voting Conservative, but as education increases, the chance of voting Labour goes down, and the chance of voting Liberal Democrat goes up. A little history: back 150 or so years ago, Britain had two political parties, the Tories and the Whigs. The Tories became the Conservative party (and hence, in Britain and in Canada, the Conservatives are nicknamed Tories41). The Whigs became Liberals. At about the same time as working people got to vote (not women, yet, but working men) the Labour Party came into existence. The Labour Party has always been affiliated with working people and trades unions, like the NDP here. But power has typically alternated between Conservative and Labour goverments, with the Liberals as a third party. In the 1980s a new party called the Social Democrats came onto the scene, but on realizing that they couldn’t make much of a dent by themselves, they merged with the Liberals to form the Liberal Democrats, which became a slightly stronger third party. I was curious about what the effect of interview length would be. Presumably, the effect is small, but I have no idea which way it would be. To assess this, it’s the same idea over again: create a new data frame with all combinations of agea, eduyrs and inwtm. I need the quartiles of interview time first: quantile(ess.major$inwtm) ## 0% 25% 50% 75% 100% ## 7 35 41 50 160 and then ess.new &lt;- crossing(agea = c(44, 71), eduyrs = c(11, 16), inwtm = c(35, 50)) ess.new ## # A tibble: 8 x 3 ## agea eduyrs inwtm ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 44 11 35 ## 2 44 11 50 ## 3 44 16 35 ## 4 44 16 50 ## 5 71 11 35 ## 6 71 11 50 ## 7 71 16 35 ## 8 71 16 50 and then predict using the model that contained interview time: pp &lt;- predict(ess.2, ess.new, type = &quot;probs&quot;) cbind(ess.new, pp) ## agea eduyrs inwtm 1 2 3 ## 1 44 11 35 0.3455998 0.4993563 0.1550439 ## 2 44 11 50 0.3139582 0.5240179 0.1620238 ## 3 44 16 35 0.3606728 0.3872735 0.2520536 ## 4 44 16 50 0.3284883 0.4074380 0.2640737 ## 5 71 11 35 0.4810901 0.3886175 0.1302924 ## 6 71 11 50 0.4455030 0.4157036 0.1387934 ## 7 71 16 35 0.4945171 0.2968551 0.2086277 ## 8 71 16 50 0.4589823 0.3182705 0.2227472 The effects of age and education are as they were before. A longer interview time is associated with a slightly decreased chance of voting Conservative and a slightly increased chance of voting Labour. Compare, for example, lines 1 and 2. But, as we suspected, the effect is small and not really worth worrying about. \\(\\blacksquare\\) 28.9 Alligator food What do alligators most like to eat? 219 alligators were captured in four Florida lakes. Each alligator’s stomach contents were observed, and the food that the alligator had eaten was classified into one of five categories: fish, invertebrates (such as snails or insects), reptiles (such as turtles), birds, and “other” (such as amphibians, plants or rocks). The researcher noted for each alligator what that alligator had most of in its stomach, as well as the gender of each alligator and whether it was “large” or “small” (greater or less than 2.3 metres in length). The data can be found in link. The numbers in the data set (apart from the first column) are all frequencies. (You can ignore that first column “profile”.) Our aim is to predict food type from the other variables. Read in the data and display the first few lines. Describe how the data are not “tidy”. Solution Separated by exactly one space: my_url &lt;- &quot;http://ritsokiguess.site/datafiles/alligator.txt&quot; gators.orig &lt;- read_delim(my_url, &quot; &quot;) ## ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## cols( ## profile = col_double(), ## Gender = col_character(), ## Size = col_character(), ## Lake = col_character(), ## Fish = col_double(), ## Invertebrate = col_double(), ## Reptile = col_double(), ## Bird = col_double(), ## Other = col_double() ## ) gators.orig ## # A tibble: 16 x 9 ## profile Gender Size Lake Fish Invertebrate Reptile Bird Other ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 f &lt;2.3 george 3 9 1 0 1 ## 2 2 m &lt;2.3 george 13 10 0 2 2 ## 3 3 f &gt;2.3 george 8 1 0 0 1 ## 4 4 m &gt;2.3 george 9 0 0 1 2 ## 5 5 f &lt;2.3 hancock 16 3 2 2 3 ## 6 6 m &lt;2.3 hancock 7 1 0 0 5 ## 7 7 f &gt;2.3 hancock 3 0 1 2 3 ## 8 8 m &gt;2.3 hancock 4 0 0 1 2 ## 9 9 f &lt;2.3 oklawaha 3 9 1 0 2 ## 10 10 m &lt;2.3 oklawaha 2 2 0 0 1 ## 11 11 f &gt;2.3 oklawaha 0 1 0 1 0 ## 12 12 m &gt;2.3 oklawaha 13 7 6 0 0 ## 13 13 f &lt;2.3 trafford 2 4 1 1 4 ## 14 14 m &lt;2.3 trafford 3 7 1 0 1 ## 15 15 f &gt;2.3 trafford 0 1 0 0 0 ## 16 16 m &gt;2.3 trafford 8 6 6 3 5 The last five columns are all frequencies. Or, one of the variables (food type) is spread over five columns instead of being contained in one. Either is good. My choice of “temporary” name reflects that I’m going to obtain a “tidy” data frame called gators in a moment. \\(\\blacksquare\\) Use pivot_longer to arrange the data suitably for analysis (which will be using multinom). Demonstrate (by looking at the first few rows of your new data frame) that you now have something tidy. Solution I’m creating my “official” data frame here: gators.orig %&gt;% pivot_longer(Fish:Other, names_to = &quot;Food.type&quot;, values_to = &quot;Frequency&quot;) -&gt; gators gators ## # A tibble: 80 x 6 ## profile Gender Size Lake Food.type Frequency ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 f &lt;2.3 george Fish 3 ## 2 1 f &lt;2.3 george Invertebrate 9 ## 3 1 f &lt;2.3 george Reptile 1 ## 4 1 f &lt;2.3 george Bird 0 ## 5 1 f &lt;2.3 george Other 1 ## 6 2 m &lt;2.3 george Fish 13 ## 7 2 m &lt;2.3 george Invertebrate 10 ## 8 2 m &lt;2.3 george Reptile 0 ## 9 2 m &lt;2.3 george Bird 2 ## 10 2 m &lt;2.3 george Other 2 ## # … with 70 more rows I gave my column names Capital Letters to make them consistent with the others (and in an attempt to stop infesting my brain with annoying variable-name errors when I fit models later). Looking at the first few lines reveals that I now have a column of food types and one column of frequencies, both of which are what I wanted. I can check that I have all the different food types by finding the distinct ones: gators %&gt;% distinct(Food.type) ## # A tibble: 5 x 1 ## Food.type ## &lt;chr&gt; ## 1 Fish ## 2 Invertebrate ## 3 Reptile ## 4 Bird ## 5 Other (Think about why count would be confusing here.) Note that Food.type is text (chr) rather than being a factor. I’ll hold my breath and see what happens when I fit a model where it is supposed to be a factor. \\(\\blacksquare\\) What is different about this problem, compared to Question here, that would make multinom the right tool to use? Solution Look at the response variable Food.type (or whatever you called it): this has multiple categories, but they are not ordered in any logical way. Thus, in short, a nominal response. \\(\\blacksquare\\) Fit a suitable multinomial model predicting food type from gender, size and lake. Does each row represent one alligator or more than one? If more than one, account for this in your modelling. Solution Each row of the tidy gators represents as many alligators as are in the Frequency column. That is, if you look at female small alligators in Lake George that ate mainly fish, there are three of those.42 This to remind you to include the weights piece, otherwise multinom will assume that you have one observation per line and not as many as the number in Frequency. That is the reason that count earlier would have been confusing: it would have told you how many rows contained each food type, rather than how many alligators, and these would have been different: gators %&gt;% count(Food.type) ## # A tibble: 5 x 2 ## Food.type n ## &lt;chr&gt; &lt;int&gt; ## 1 Bird 16 ## 2 Fish 16 ## 3 Invertebrate 16 ## 4 Other 16 ## 5 Reptile 16 gators %&gt;% count(Food.type, wt = Frequency) ## # A tibble: 5 x 2 ## Food.type n ## &lt;chr&gt; &lt;dbl&gt; ## 1 Bird 13 ## 2 Fish 94 ## 3 Invertebrate 61 ## 4 Other 32 ## 5 Reptile 19 Each food type appears on 16 rows, but is the favoured diet of very different numbers of alligators. Note the use of wt= to specify a frequency variable.43 You ought to understand why those are different. All right, back to modelling: library(nnet) gators.1 &lt;- multinom(Food.type ~ Gender + Size + Lake, weights = Frequency, data = gators ) ## # weights: 35 (24 variable) ## initial value 352.466903 ## iter 10 value 270.228588 ## iter 20 value 268.944257 ## final value 268.932741 ## converged This worked, even though Food.type was actually text. I guess it got converted to a factor. The ordering of the levels doesn’t matter here anyway, since this is not an ordinal model. No need to look at it, since the output is kind of confusing anyway: summary(gators.1) ## Call: ## multinom(formula = Food.type ~ Gender + Size + Lake, data = gators, ## weights = Frequency) ## ## Coefficients: ## (Intercept) Genderm Size&gt;2.3 Lakehancock Lakeoklawaha Laketrafford ## Fish 2.4322304 0.60674971 -0.7308535 -0.5751295 0.5513785 -1.23681053 ## Invertebrate 2.6012531 0.14378459 -2.0671545 -2.3557377 1.4645820 -0.08096493 ## Other 1.0014505 0.35423803 -1.0214847 0.1914537 0.5775317 0.32097943 ## Reptile -0.9829064 -0.02053375 -0.1741207 0.5534169 3.0807416 1.82333205 ## ## Std. Errors: ## (Intercept) Genderm Size&gt;2.3 Lakehancock Lakeoklawaha Laketrafford ## Fish 0.7706940 0.6888904 0.6523273 0.7952147 1.210229 0.8661187 ## Invertebrate 0.7917210 0.7292510 0.7084028 0.9463640 1.232835 0.8814625 ## Other 0.8747773 0.7623738 0.7250455 0.9072182 1.374545 0.9589807 ## Reptile 1.2827234 0.9088217 0.8555051 1.3797755 1.591542 1.3388017 ## ## Residual Deviance: 537.8655 ## AIC: 585.8655 You get one coefficient for each variable (along the top) and for each response group (down the side), using the first group as a baseline everywhere. These numbers are hard to interpret; doing predictions is much easier. \\(\\blacksquare\\) Do a test to see whether Gender should stay in the model. (This will entail fitting another model.) What do you conclude? Solution The other model to fit is the one without the variable you’re testing: gators.2 &lt;- update(gators.1, . ~ . - Gender) ## # weights: 30 (20 variable) ## initial value 352.466903 ## iter 10 value 272.246275 ## iter 20 value 270.046891 ## final value 270.040139 ## converged I did update here to show you that it works, but of course there’s no problem in just writing out the whole model again and taking out Gender, preferably by copying and pasting: gators.2x &lt;- multinom(Food.type ~ Size + Lake, weights = Frequency, data = gators ) ## # weights: 30 (20 variable) ## initial value 352.466903 ## iter 10 value 272.246275 ## iter 20 value 270.046891 ## final value 270.040139 ## converged and then you compare the models with and without Gender using anova: anova(gators.2, gators.1) ## Likelihood ratio tests of Multinomial Models ## ## Response: Food.type ## Model Resid. df Resid. Dev Test Df LR stat. Pr(Chi) ## 1 Size + Lake 300 540.0803 ## 2 Gender + Size + Lake 296 537.8655 1 vs 2 4 2.214796 0.6963214 The P-value is not small, so the two models fit equally well, and therefore we should go with the smaller, simpler one: that is, the one without Gender. Sometimes drop1 works here too (and sometimes it doesn’t, for reasons I haven’t figured out): drop1(gators.1, test = &quot;Chisq&quot;) ## trying - Gender ## Error in if (trace) {: argument is not interpretable as logical I don’t even know what this error message means, never mind what to do about it. \\(\\blacksquare\\) Predict the probability that an alligator prefers each food type, given its size, gender (if necessary) and the lake it was found in, using the more appropriate of the two models that you have fitted so far. This means (i) obtaining all the sizes and lake names, (ii) making a data frame for prediction, and (iii) obtaining and displaying the predicted probabilities. Solution To get the different categories, use distinct and pull: Lakes &lt;- gators %&gt;% distinct(Lake) %&gt;% pull(Lake) Lakes ## [1] &quot;george&quot; &quot;hancock&quot; &quot;oklawaha&quot; &quot;trafford&quot; Sizes &lt;- gators %&gt;% distinct(Size) %&gt;% pull(Size) Sizes ## [1] &quot;&lt;2.3&quot; &quot;&gt;2.3&quot; I didn’t need to think about Genders because that’s not in the better model. See below for what happens if you include it anyway. I have persisted with the Capital Letters, for consistency. Next, a data frame for predicting from, using crossing, and called, as per my tradition, new: new &lt;- crossing(Lake = Lakes, Size = Sizes) new ## # A tibble: 8 x 2 ## Lake Size ## &lt;chr&gt; &lt;chr&gt; ## 1 george &lt;2.3 ## 2 george &gt;2.3 ## 3 hancock &lt;2.3 ## 4 hancock &gt;2.3 ## 5 oklawaha &lt;2.3 ## 6 oklawaha &gt;2.3 ## 7 trafford &lt;2.3 ## 8 trafford &gt;2.3 and then, obtain the predictions and glue them onto the data frame of values for which they are predictions. Don’t forget to use the second model, the one without Gender. If you do forget, you’ll get an error anyway, because your data frame of values to predict from doesn’t have any Gender in it: pp &lt;- predict(gators.2, new, type = &quot;p&quot;) preds1 &lt;- cbind(new, pp) preds1 ## Lake Size Bird Fish Invertebrate Other Reptile ## 1 george &lt;2.3 0.029671502 0.4521032 0.41285699 0.09380190 0.01156641 ## 2 george &gt;2.3 0.081071082 0.6574394 0.13967877 0.09791193 0.02389880 ## 3 hancock &lt;2.3 0.070400215 0.5353040 0.09309885 0.25374163 0.04745531 ## 4 hancock &gt;2.3 0.140898571 0.5701968 0.02307179 0.19400899 0.07182382 ## 5 oklawaha &lt;2.3 0.008818267 0.2581872 0.60189518 0.05387241 0.07722691 ## 6 oklawaha &gt;2.3 0.029419560 0.4584368 0.24864408 0.06866206 0.19483754 ## 7 trafford &lt;2.3 0.035892547 0.1842997 0.51683770 0.17420330 0.08876673 ## 8 trafford &gt;2.3 0.108222209 0.2957526 0.19296148 0.20066241 0.20240133 Success. You won’t get this right the first time. I certainly didn’t. Anyway, these are the correct predictions that I discuss later. If you thought that the better model was the one with Gender in it, or you otherwise forgot that you didn’t need Gender then you needed to do something like this as well: Genders &lt;- gators %&gt;% distinct(Gender) %&gt;% pull(Gender) new &lt;- crossing(Lake = Lakes, Size = Sizes, Gender = Genders) new ## # A tibble: 16 x 3 ## Lake Size Gender ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 george &lt;2.3 f ## 2 george &lt;2.3 m ## 3 george &gt;2.3 f ## 4 george &gt;2.3 m ## 5 hancock &lt;2.3 f ## 6 hancock &lt;2.3 m ## 7 hancock &gt;2.3 f ## 8 hancock &gt;2.3 m ## 9 oklawaha &lt;2.3 f ## 10 oklawaha &lt;2.3 m ## 11 oklawaha &gt;2.3 f ## 12 oklawaha &gt;2.3 m ## 13 trafford &lt;2.3 f ## 14 trafford &lt;2.3 m ## 15 trafford &gt;2.3 f ## 16 trafford &gt;2.3 m If you predict this in the model without Gender, you’ll get the following: pp &lt;- predict(gators.2, new, type = &quot;p&quot;) cbind(new, pp) ## Lake Size Gender Bird Fish Invertebrate Other Reptile ## 1 george &lt;2.3 f 0.029671502 0.4521032 0.41285699 0.09380190 0.01156641 ## 2 george &lt;2.3 m 0.029671502 0.4521032 0.41285699 0.09380190 0.01156641 ## 3 george &gt;2.3 f 0.081071082 0.6574394 0.13967877 0.09791193 0.02389880 ## 4 george &gt;2.3 m 0.081071082 0.6574394 0.13967877 0.09791193 0.02389880 ## 5 hancock &lt;2.3 f 0.070400215 0.5353040 0.09309885 0.25374163 0.04745531 ## 6 hancock &lt;2.3 m 0.070400215 0.5353040 0.09309885 0.25374163 0.04745531 ## 7 hancock &gt;2.3 f 0.140898571 0.5701968 0.02307179 0.19400899 0.07182382 ## 8 hancock &gt;2.3 m 0.140898571 0.5701968 0.02307179 0.19400899 0.07182382 ## 9 oklawaha &lt;2.3 f 0.008818267 0.2581872 0.60189518 0.05387241 0.07722691 ## 10 oklawaha &lt;2.3 m 0.008818267 0.2581872 0.60189518 0.05387241 0.07722691 ## 11 oklawaha &gt;2.3 f 0.029419560 0.4584368 0.24864408 0.06866206 0.19483754 ## 12 oklawaha &gt;2.3 m 0.029419560 0.4584368 0.24864408 0.06866206 0.19483754 ## 13 trafford &lt;2.3 f 0.035892547 0.1842997 0.51683770 0.17420330 0.08876673 ## 14 trafford &lt;2.3 m 0.035892547 0.1842997 0.51683770 0.17420330 0.08876673 ## 15 trafford &gt;2.3 f 0.108222209 0.2957526 0.19296148 0.20066241 0.20240133 ## 16 trafford &gt;2.3 m 0.108222209 0.2957526 0.19296148 0.20066241 0.20240133 Here, the predictions for each gender are exactly the same, because not having Gender in the model means that we take its effect to be exactly zero. Alternatively, if you really thought the model with Gender was the better one, then you’d do this: pp &lt;- predict(gators.1, new, type = &quot;p&quot;) cbind(new, pp) ## Lake Size Gender Bird Fish Invertebrate Other Reptile ## 1 george &lt;2.3 f 0.034528820 0.3930846 0.46546988 0.09399531 0.012921439 ## 2 george &lt;2.3 m 0.023983587 0.5008716 0.37330933 0.09304268 0.008792768 ## 3 george &gt;2.3 f 0.105463159 0.5780952 0.17991064 0.10337131 0.033159675 ## 4 george &gt;2.3 m 0.067888065 0.6826531 0.13371938 0.09482794 0.020911502 ## 5 hancock &lt;2.3 f 0.079170901 0.5071007 0.10120258 0.26099808 0.051527690 ## 6 hancock &lt;2.3 m 0.051120681 0.6006664 0.07545143 0.24016625 0.032595228 ## 7 hancock &gt;2.3 f 0.167234031 0.5157600 0.02705184 0.19850489 0.091449234 ## 8 hancock &gt;2.3 m 0.110233536 0.6236556 0.02058879 0.18646783 0.059054291 ## 9 oklawaha &lt;2.3 f 0.010861171 0.2146058 0.63335376 0.05267686 0.088502364 ## 10 oklawaha &lt;2.3 m 0.008370118 0.3033923 0.56356778 0.05785201 0.066817794 ## 11 oklawaha &gt;2.3 f 0.037755986 0.3592072 0.27861325 0.06593316 0.258490381 ## 12 oklawaha &gt;2.3 m 0.027647882 0.4825354 0.23557146 0.06880557 0.185439712 ## 13 trafford &lt;2.3 f 0.043846177 0.1449092 0.54510349 0.16453404 0.101607074 ## 14 trafford &lt;2.3 m 0.034440739 0.2088069 0.49438416 0.18417900 0.078189237 ## 15 trafford &gt;2.3 f 0.133999340 0.2132364 0.21081238 0.18105124 0.260900674 ## 16 trafford &gt;2.3 m 0.104507582 0.3050805 0.18983929 0.20122886 0.199343761 and this time there is an effect of gender, but it is smallish, as befits an effect that is not significant.44 \\(\\blacksquare\\) What do you think is the most important way in which the lakes differ? (Hint: look at where the biggest predicted probabilities are.) Solution Here are the predictions again: preds1 ## Lake Size Bird Fish Invertebrate Other Reptile ## 1 george &lt;2.3 0.029671502 0.4521032 0.41285699 0.09380190 0.01156641 ## 2 george &gt;2.3 0.081071082 0.6574394 0.13967877 0.09791193 0.02389880 ## 3 hancock &lt;2.3 0.070400215 0.5353040 0.09309885 0.25374163 0.04745531 ## 4 hancock &gt;2.3 0.140898571 0.5701968 0.02307179 0.19400899 0.07182382 ## 5 oklawaha &lt;2.3 0.008818267 0.2581872 0.60189518 0.05387241 0.07722691 ## 6 oklawaha &gt;2.3 0.029419560 0.4584368 0.24864408 0.06866206 0.19483754 ## 7 trafford &lt;2.3 0.035892547 0.1842997 0.51683770 0.17420330 0.08876673 ## 8 trafford &gt;2.3 0.108222209 0.2957526 0.19296148 0.20066241 0.20240133 Following my own hint: the preferred diet in George and Hancock lakes is fish, but the preferred diet in Oklawaha and Trafford lakes is (at least sometimes) invertebrates. That is to say, the preferred diet in those last two lakes is less likely to be invertebrates than it is in the first two (comparing for alligators of the same size). This is true for both large and small alligators, as it should be, since there is no interaction in the model. That will do, though you can also note that reptiles are more commonly found in the last two lakes, and birds sometimes appear in the diet in Hancock and Trafford but rarely in the other two lakes. Another way to think about this is to hold size constant and compare lakes (and then check that it applies to the other size too). In this case, you’d find the biggest predictions among the first four rows, and then check that the pattern persists in the second four rows. (It does.) I think looking at predicted probabilities like this is the easiest way to see what the model is telling you. I also think that having a consistent recipe for doing predictions makes the process require a good deal less thought: get the values you want to predict for and store them in vectors with plural names; create a data frame for prediction using crossing, where the things inside are all “singular=plural”; run predict with model, new data and (if needed) type of value to predict, glue the predictions onto the new data. If you somehow mess up the creation of your new data frame (this typically happens by forgetting a variable that you should have included, or giving it the wrong name), predict will silently use the original data to predict from. Your only warning that this has happened is the size of the output; it should have as many rows as new did above, and the original data will typically have many more. (This happened to me just now. I recognized what the problem was, and how I would be able to fix it.) \\(\\blacksquare\\) How would you describe the major difference between the diets of the small and large alligators? Solution Same idea: hold lake constant, and compare small and large, then check that your conclusion holds for the other lakes as it should. For example, in George Lake, the large alligators are more likely to eat fish, and less likely to eat invertebrates, compared to the small ones. The other food types are not that much different, though you might also note that birds appear more in the diets of large alligators than small ones. Does that hold in the other lakes? I think so, though there is less difference for fish in Hancock lake than the others (where invertebrates are rare for both sizes). Birds don’t commonly appear in any alligator’s diets, but where they do, they are commoner for large alligators than small ones. \\(\\blacksquare\\) 28.10 Crimes in San Francisco The data in link is a subset of a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are: Dates: the date and time of the crime Category: the category of crime, eg. “larceny” or “vandalism” (response). Descript: detailed description of crime. DayOfWeek: the day of the week of the crime. PdDistrict: the name of the San Francisco Police Department district in which the crime took place. Resolution: how the crime was resolved Address: approximate street address of crime X: longitude Y: latitude Our aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which are the ones included in this data file. Some of the model-fitting takes a while (you’ll see why below). If you’re using R Markdown, you can wait for the models to fit each time you re-run your document, or insert cache=T in the top line of your code chunk (the one with r in curly brackets in it, above the actual code). Put a comma and the cache=T inside the curly brackets. What that does is to re-run that code chunk only if it changes; if it hasn’t changed it will use the saved results from last time it was run. That can save you a lot of waiting around. Read in the data and display the dataset (or, at least, part of it). Solution The usual: my_url &lt;- &quot;http://utsc.utoronto.ca/~butler/d29/sfcrime1.csv&quot; sfcrime &lt;- read_csv(my_url) ## ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## cols( ## Category = col_character(), ## DayOfWeek = col_character(), ## PdDistrict = col_character() ## ) sfcrime ## # A tibble: 359,528 x 3 ## Category DayOfWeek PdDistrict ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 LARCENY/THEFT Wednesday NORTHERN ## 2 LARCENY/THEFT Wednesday PARK ## 3 LARCENY/THEFT Wednesday INGLESIDE ## 4 VEHICLE THEFT Wednesday INGLESIDE ## 5 VEHICLE THEFT Wednesday BAYVIEW ## 6 LARCENY/THEFT Wednesday RICHMOND ## 7 LARCENY/THEFT Wednesday CENTRAL ## 8 LARCENY/THEFT Wednesday CENTRAL ## 9 LARCENY/THEFT Wednesday NORTHERN ## 10 ASSAULT Wednesday INGLESIDE ## # … with 359,518 more rows This is a tidied-up version of the data, with only the variables we’ll look at, and only the observations from one of the “big four” crimes, a mere 300,000 of them. This is the data set we created earlier. \\(\\blacksquare\\) Fit a multinomial logistic regression that predicts crime category from day of week and district. (You don’t need to look at it.) The model-fitting produces some output. (If you’re using R Markdown, that will come with it.) Solution The modelling part is easy enough, as long as you can get the uppercase letters in the right places: sfcrime.1 &lt;- multinom(Category~DayOfWeek+PdDistrict,data=sfcrime) ## # weights: 68 (48 variable) ## initial value 498411.639069 ## iter 10 value 430758.073422 ## iter 20 value 430314.270403 ## iter 30 value 423303.587698 ## iter 40 value 420883.528523 ## iter 50 value 418355.242764 ## final value 418149.979622 ## converged \\(\\blacksquare\\) Fit a model that predicts Category from only the district. Hand in the output from the fitting process as well. Solution Same idea. Write it out, or use update: sfcrime.2 &lt;- update(sfcrime.1,.~.-DayOfWeek) ## # weights: 44 (30 variable) ## initial value 498411.639069 ## iter 10 value 426003.543845 ## iter 20 value 425542.806828 ## iter 30 value 421715.787609 ## final value 418858.235297 ## converged \\(\\blacksquare\\) Use anova to compare the two models you just obtained. What does the anova tell you? Solution This: anova(sfcrime.2,sfcrime.1) ## Likelihood ratio tests of Multinomial Models ## ## Response: Category ## Model Resid. df Resid. Dev Test Df LR stat. Pr(Chi) ## 1 PdDistrict 1078554 837716.5 ## 2 DayOfWeek + PdDistrict 1078536 836300.0 1 vs 2 18 1416.511 0 This is a very small P-value. The null hypothesis is that the two models are equally good, and this is clearly rejected. We need the bigger model: that is, we need to keep DayOfWeek in there, because the pattern of crimes (in each district) differs over day of week. One reason the P-value came out so small is that we have a ton of data, so that even a very small difference between days of the week could come out very strongly significant. The Machine Learning people (this is a machine learning dataset) don’t worry so much about tests for that reason: they are more concerned with predicting things well, so they just throw everything into the model and see what comes out. \\(\\blacksquare\\) Using your preferred model, obtain predicted probabilities that a crime will be of each of these four categories for each day of the week in the TENDERLOIN district (the name is ALL CAPS). This will mean constructing a data frame to predict from, obtaining the predictions and then displaying them suitably. Solution I left this one fairly open, because you’ve done this kind of thing before, so what you need to do ought to be fairly clear: Construct the values to predict for with plural names:45 DayOfWeeks &lt;- sfcrime %&gt;% distinct(DayOfWeek) %&gt;% pull(DayOfWeek) DayOfWeeks ## [1] &quot;Wednesday&quot; &quot;Tuesday&quot; &quot;Monday&quot; &quot;Sunday&quot; &quot;Saturday&quot; &quot;Friday&quot; &quot;Thursday&quot; PdDistricts &lt;- &quot;TENDERLOIN&quot; The days of the week are in the order they came out in the data. I could fix this up, but the thing that matters for us is that Saturday and Sunday came out next to each other. Anyway, it’s about right, even though only one of the variables is actually plural and the other plural came out wrong. Then, make data frame of combinations. The nice thing is that if we had more than one district, we’d just define PdDistricts appropriately above and the rest of it would be the same: sfcrime.new &lt;- crossing(DayOfWeek=DayOfWeeks,PdDistrict=PdDistricts) sfcrime.new ## # A tibble: 7 x 2 ## DayOfWeek PdDistrict ## &lt;chr&gt; &lt;chr&gt; ## 1 Friday TENDERLOIN ## 2 Monday TENDERLOIN ## 3 Saturday TENDERLOIN ## 4 Sunday TENDERLOIN ## 5 Thursday TENDERLOIN ## 6 Tuesday TENDERLOIN ## 7 Wednesday TENDERLOIN Then do the predictions: p &lt;- predict(sfcrime.1,sfcrime.new,type=&quot;probs&quot;) I can never remember the type thing for these models, because they are all different. I just make a guess, and if I guess something that this version of predict doesn’t know about, it’ll tell me: p &lt;- predict(sfcrime.1,sfcrime.new,type=&quot;bananas&quot;) ## Error in match.arg(type): &#39;arg&#39; should be one of &quot;class&quot;, &quot;probs&quot; I can work out from this that type should be probs, since the other one makes the best guess at which category of response you’ll get (the one with the highest probability). The predicted probabilities are more informative, since then you can see how they change, even if the predicted category stays the same.46 Finally, display the results. I thought cbind wouldn’t work here, because some of the variables are factors and some are numbers, but cbind is smarter than that:47 cbind(sfcrime.new,p) ## DayOfWeek PdDistrict ASSAULT DRUG/NARCOTIC LARCENY/THEFT VEHICLE THEFT ## 1 Friday TENDERLOIN 0.2125997 0.4658122 0.2907793 0.03080879 ## 2 Monday TENDERLOIN 0.2072430 0.5001466 0.2656069 0.02700348 ## 3 Saturday TENDERLOIN 0.2425998 0.4200751 0.3060611 0.03126405 ## 4 Sunday TENDERLOIN 0.2548597 0.4287291 0.2868682 0.02954299 ## 5 Thursday TENDERLOIN 0.1938754 0.5179500 0.2617658 0.02640888 ## 6 Tuesday TENDERLOIN 0.1942447 0.5208668 0.2593331 0.02555535 ## 7 Wednesday TENDERLOIN 0.1874867 0.5400287 0.2479498 0.02453490 \\(\\blacksquare\\) Describe briefly how the weekend days Saturday and Sunday differ from the rest. Solution The days ended up in some quasi-random order, but Saturday and Sunday are still together, so we can still easily compare them with the rest. My take is that the last two columns don’t differ much between weekday and weekend, but the first two columns do: the probability of a crime being an assault is a bit higher on the weekend, and the probability of a crime being drug-related is a bit lower. I will accept anything reasonable supported by the predictions you got. We said there was a strongly significant day-of-week effect, but the changes from weekday to weekend are actually pretty small (but the changes from one weekday to another are even smaller). This supports what I guessed before, that with this much data even a small effect (the one shown here) is statistically significant.48 I want to compare another district. What districts do we have? sfcrime %&gt;% count(PdDistrict) ## # A tibble: 10 x 2 ## PdDistrict n ## &lt;chr&gt; &lt;int&gt; ## 1 BAYVIEW 31693 ## 2 CENTRAL 38052 ## 3 INGLESIDE 30102 ## 4 MISSION 45277 ## 5 NORTHERN 47750 ## 6 PARK 19197 ## 7 RICHMOND 18211 ## 8 SOUTHERN 67981 ## 9 TARAVAL 24981 ## 10 TENDERLOIN 36284 This is the number of our “big four” crimes committed in each district. Let’s look at the lowest-crime district RICHMOND. I copy and paste my code. Since I want to compare two districts, I include both: DayOfWeeks &lt;- sfcrime %&gt;% distinct(DayOfWeek) %&gt;% pull(DayOfWeek) PdDistricts &lt;- c(&quot;RICHMOND&quot;,&quot;TENDERLOIN&quot;) sfcrime.new &lt;- crossing(DayOfWeek=DayOfWeeks,PdDistrict=PdDistricts) sfcrime.new ## # A tibble: 14 x 2 ## DayOfWeek PdDistrict ## &lt;chr&gt; &lt;chr&gt; ## 1 Friday RICHMOND ## 2 Friday TENDERLOIN ## 3 Monday RICHMOND ## 4 Monday TENDERLOIN ## 5 Saturday RICHMOND ## 6 Saturday TENDERLOIN ## 7 Sunday RICHMOND ## 8 Sunday TENDERLOIN ## 9 Thursday RICHMOND ## 10 Thursday TENDERLOIN ## 11 Tuesday RICHMOND ## 12 Tuesday TENDERLOIN ## 13 Wednesday RICHMOND ## 14 Wednesday TENDERLOIN p1 &lt;- predict(sfcrime.1,sfcrime.new,type=&quot;probs&quot;) d1 &lt;- cbind(sfcrime.new,p1) ; d1 ## DayOfWeek PdDistrict ASSAULT DRUG/NARCOTIC LARCENY/THEFT VEHICLE THEFT ## 1 Friday RICHMOND 0.1666304 0.04944552 0.5466635 0.23726062 ## 2 Friday TENDERLOIN 0.2125997 0.46581221 0.2907793 0.03080879 ## 3 Monday RICHMOND 0.1760175 0.05753044 0.5411034 0.22534869 ## 4 Monday TENDERLOIN 0.2072430 0.50014661 0.2656069 0.02700348 ## 5 Saturday RICHMOND 0.1809352 0.04243108 0.5475273 0.22910644 ## 6 Saturday TENDERLOIN 0.2425998 0.42007509 0.3060611 0.03126405 ## 7 Sunday RICHMOND 0.1973675 0.04496576 0.5328708 0.22479591 ## 8 Sunday TENDERLOIN 0.2548597 0.42872910 0.2868682 0.02954299 ## 9 Thursday RICHMOND 0.1683841 0.06092432 0.5453260 0.22536564 ## 10 Thursday TENDERLOIN 0.1938754 0.51795000 0.2617658 0.02640888 ## 11 Tuesday RICHMOND 0.1706999 0.06199196 0.5466472 0.22066087 ## 12 Tuesday TENDERLOIN 0.1942447 0.52086682 0.2593331 0.02555535 ## 13 Wednesday RICHMOND 0.1709963 0.06670490 0.5424317 0.21986703 ## 14 Wednesday TENDERLOIN 0.1874867 0.54002868 0.2479498 0.02453490 Richmond is obviously not a drug-dealing kind of place; most of its crimes are theft of one kind or another. But the predicted effect of weekday vs. weekend is the same: Richmond doesn’t have many assaults or drug crimes, but it also has more assaults and fewer drug crimes on the weekend than during the week. There is not much effect of day of the week on the other two crime types in either place. The consistency of pattern, even though the prevalence of the different crime types differs by location, is a feature of the model: we fitted an additive model, that says there is an effect of weekday, and independently there is an effect of location. The pattern over weekday is the same for each location, implied by the model. This may or may not be supported by the actual data. The way to assess this is to fit a model with interaction (we will see more of this when we revisit ANOVA later), and compare the fit: sfcrime.3 &lt;- update(sfcrime.1,.~.+DayOfWeek*PdDistrict) ## # weights: 284 (210 variable) ## initial value 498411.639069 ## iter 10 value 429631.807781 ## iter 20 value 429261.427210 ## iter 30 value 428111.625547 ## iter 40 value 423807.031450 ## iter 50 value 421129.496196 ## iter 60 value 420475.833895 ## iter 70 value 419523.235916 ## iter 80 value 418621.612920 ## iter 90 value 418147.629782 ## iter 100 value 418036.670485 ## final value 418036.670485 ## stopped after 100 iterations # anova(sfcrime.1,sfcrime.3) This one didn’t actually complete the fitting process: it got to 100 times around and stopped (since that’s the default limit). We can make it go a bit further thus: sfcrime.3 &lt;- update(sfcrime.1,.~.+DayOfWeek*PdDistrict,maxit=300) ## # weights: 284 (210 variable) ## initial value 498411.639069 ## iter 10 value 429631.807781 ## iter 20 value 429261.427210 ## iter 30 value 428111.625547 ## iter 40 value 423807.031450 ## iter 50 value 421129.496196 ## iter 60 value 420475.833895 ## iter 70 value 419523.235916 ## iter 80 value 418621.612920 ## iter 90 value 418147.629782 ## iter 100 value 418036.670485 ## iter 110 value 417957.337016 ## iter 120 value 417908.465189 ## iter 130 value 417890.580843 ## iter 140 value 417874.839492 ## iter 150 value 417867.449342 ## iter 160 value 417862.626213 ## iter 170 value 417858.654628 ## final value 417858.031854 ## converged anova(sfcrime.1,sfcrime.3) ## Likelihood ratio tests of Multinomial Models ## ## Response: Category ## Model Resid. df Resid. Dev Test Df ## 1 DayOfWeek + PdDistrict 1078536 836300.0 ## 2 DayOfWeek + PdDistrict + DayOfWeek:PdDistrict 1078374 835716.1 1 vs 2 162 ## LR stat. Pr(Chi) ## 1 ## 2 583.8955 0 This time, we got to the end. (The maxit=300 gets passed on to multinom, and says “go around up to 300 times if needed”.) As you will see if you try it, this takes a bit of time to run. This anova is also strongly significant, but in the light of the previous discussion, the differential effect of day of week in different districts might not be very big. We can even assess that; we have all the machinery for the predictions, and we just have to apply them to this model: p3 &lt;- predict(sfcrime.3,sfcrime.new,type=&quot;probs&quot;) d3 &lt;- cbind(sfcrime.new,p3) ; d3 ## DayOfWeek PdDistrict ASSAULT DRUG/NARCOTIC LARCENY/THEFT VEHICLE THEFT ## 1 Friday RICHMOND 0.1756062 0.05241934 0.5447797 0.22719474 ## 2 Friday TENDERLOIN 0.2163513 0.44958359 0.3006968 0.03336831 ## 3 Monday RICHMOND 0.1668855 0.06038366 0.5537600 0.21897087 ## 4 Monday TENDERLOIN 0.2061337 0.51256922 0.2569954 0.02430166 ## 5 Saturday RICHMOND 0.1859514 0.04414673 0.5244262 0.24547574 ## 6 Saturday TENDERLOIN 0.2368561 0.41026767 0.3221820 0.03069417 ## 7 Sunday RICHMOND 0.1851406 0.05211970 0.5403967 0.22234300 ## 8 Sunday TENDERLOIN 0.2570331 0.40019234 0.3101184 0.03265625 ## 9 Thursday RICHMOND 0.1811185 0.05621201 0.5394484 0.22322107 ## 10 Thursday TENDERLOIN 0.1951851 0.52867900 0.2528465 0.02328943 ## 11 Tuesday RICHMOND 0.1758111 0.05587178 0.5471405 0.22117663 ## 12 Tuesday TENDERLOIN 0.1907137 0.53955543 0.2419757 0.02775520 ## 13 Wednesday RICHMOND 0.1603796 0.06279956 0.5525565 0.22426437 ## 14 Wednesday TENDERLOIN 0.1906309 0.54577677 0.2401585 0.02343381 It doesn’t look much different. Maybe the Tenderloin has a larger weekend increase in assaults than Richmond does. I saved these predictions in data frames so that I could compare them. Columns 3 through 6 contain the actual predictions; let’s take the difference between them, and glue it back onto the district and day of week. I rounded the prediction differences off to 4 decimals to make the largest ones easier to find: pdiff &lt;- round(p3-p1, 4) cbind(sfcrime.new, pdiff) ## DayOfWeek PdDistrict ASSAULT DRUG/NARCOTIC LARCENY/THEFT VEHICLE THEFT ## 1 Friday RICHMOND 0.0090 0.0030 -0.0019 -0.0101 ## 2 Friday TENDERLOIN 0.0038 -0.0162 0.0099 0.0026 ## 3 Monday RICHMOND -0.0091 0.0029 0.0127 -0.0064 ## 4 Monday TENDERLOIN -0.0011 0.0124 -0.0086 -0.0027 ## 5 Saturday RICHMOND 0.0050 0.0017 -0.0231 0.0164 ## 6 Saturday TENDERLOIN -0.0057 -0.0098 0.0161 -0.0006 ## 7 Sunday RICHMOND -0.0122 0.0072 0.0075 -0.0025 ## 8 Sunday TENDERLOIN 0.0022 -0.0285 0.0233 0.0031 ## 9 Thursday RICHMOND 0.0127 -0.0047 -0.0059 -0.0021 ## 10 Thursday TENDERLOIN 0.0013 0.0107 -0.0089 -0.0031 ## 11 Tuesday RICHMOND 0.0051 -0.0061 0.0005 0.0005 ## 12 Tuesday TENDERLOIN -0.0035 0.0187 -0.0174 0.0022 ## 13 Wednesday RICHMOND -0.0106 -0.0039 0.0101 0.0044 ## 14 Wednesday TENDERLOIN 0.0031 0.0057 -0.0078 -0.0011 None of the predicted probabilities differ by more than 0.04, which is consistent with the size of the interaction effect being small even though significant. Some of the differences largest in size are Drugs-Narcotics in Tenderloin, but even they are not big. The programmer in me wants to find a way to display the largest and smallest few differences. My idea is to use pivot_longer to make one column of differences and sort it, then display the top and bottom few values. Note that I have to put those little “backticks” around VEHICLE THEFT since the variable name has a space in it. My code inefficiently sorts the differences twice: cbind(sfcrime.new,pdiff) %&gt;% pivot_longer(ASSAULT:`VEHICLE THEFT`, names_to=&quot;crimetype&quot;, values_to = &quot;difference&quot;) -&gt; d1 d1 %&gt;% arrange(difference) %&gt;% slice(1:6) ## # A tibble: 6 x 4 ## DayOfWeek PdDistrict crimetype difference ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Sunday TENDERLOIN DRUG/NARCOTIC -0.0285 ## 2 Saturday RICHMOND LARCENY/THEFT -0.0231 ## 3 Tuesday TENDERLOIN LARCENY/THEFT -0.0174 ## 4 Friday TENDERLOIN DRUG/NARCOTIC -0.0162 ## 5 Sunday RICHMOND ASSAULT -0.0122 ## 6 Wednesday RICHMOND ASSAULT -0.0106 d1 %&gt;% arrange(desc(difference)) %&gt;% slice(1:6) ## # A tibble: 6 x 4 ## DayOfWeek PdDistrict crimetype difference ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Sunday TENDERLOIN LARCENY/THEFT 0.0233 ## 2 Tuesday TENDERLOIN DRUG/NARCOTIC 0.0187 ## 3 Saturday RICHMOND VEHICLE THEFT 0.0164 ## 4 Saturday TENDERLOIN LARCENY/THEFT 0.0161 ## 5 Monday RICHMOND LARCENY/THEFT 0.0127 ## 6 Thursday RICHMOND ASSAULT 0.0127 I don’t know what, if anything, you make of those. Extra: there is a better way of doing those in one go: d1 %&gt;% top_n(6, difference) ## # A tibble: 6 x 4 ## DayOfWeek PdDistrict crimetype difference ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Monday RICHMOND LARCENY/THEFT 0.0127 ## 2 Saturday RICHMOND VEHICLE THEFT 0.0164 ## 3 Saturday TENDERLOIN LARCENY/THEFT 0.0161 ## 4 Sunday TENDERLOIN LARCENY/THEFT 0.0233 ## 5 Thursday RICHMOND ASSAULT 0.0127 ## 6 Tuesday TENDERLOIN DRUG/NARCOTIC 0.0187 d1 %&gt;% top_n(6, -difference) ## # A tibble: 6 x 4 ## DayOfWeek PdDistrict crimetype difference ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Friday TENDERLOIN DRUG/NARCOTIC -0.0162 ## 2 Saturday RICHMOND LARCENY/THEFT -0.0231 ## 3 Sunday RICHMOND ASSAULT -0.0122 ## 4 Sunday TENDERLOIN DRUG/NARCOTIC -0.0285 ## 5 Tuesday TENDERLOIN LARCENY/THEFT -0.0174 ## 6 Wednesday RICHMOND ASSAULT -0.0106 that is just slicker. What it actually does is almost the same as my code, but it saves you worrying about the details. (It picks out the top 6, but without putting them in order.) \\(\\blacksquare\\) 28.11 Crimes in San Francisco – the data The data in link is a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are: Dates: the date and time of the crime Category: the category of crime, eg. “larceny” or “vandalism” (response). Descript: detailed description of crime. DayOfWeek: the day of the week of the crime. PdDistrict: the name of the San Francisco Police Department district in which the crime took place. Resolution: how the crime was resolved Address: approximate street address of crime X: longitude Y: latitude Our aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which we will have to discover. Read in the data and verify that you have these columns and a lot of rows. (The data may take a moment to read in. You will see why.) Solution my_url=&quot;http://utsc.utoronto.ca/~butler/d29/sfcrime.csv&quot; sfcrime=read_csv(my_url) ## ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## cols( ## Dates = col_datetime(format = &quot;&quot;), ## Category = col_character(), ## Descript = col_character(), ## DayOfWeek = col_character(), ## PdDistrict = col_character(), ## Resolution = col_character(), ## Address = col_character(), ## X = col_double(), ## Y = col_double() ## ) sfcrime ## # A tibble: 878,049 x 9 ## Dates Category Descript DayOfWeek PdDistrict Resolution Address X ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2015-05-13 23:53:00 WARRANTS WARRANT A… Wednesday NORTHERN ARREST, B… OAK ST … -122. ## 2 2015-05-13 23:53:00 OTHER OF… TRAFFIC V… Wednesday NORTHERN ARREST, B… OAK ST … -122. ## 3 2015-05-13 23:33:00 OTHER OF… TRAFFIC V… Wednesday NORTHERN ARREST, B… VANNESS… -122. ## 4 2015-05-13 23:30:00 LARCENY/… GRAND THE… Wednesday NORTHERN NONE 1500 Bl… -122. ## 5 2015-05-13 23:30:00 LARCENY/… GRAND THE… Wednesday PARK NONE 100 Blo… -122. ## 6 2015-05-13 23:30:00 LARCENY/… GRAND THE… Wednesday INGLESIDE NONE 0 Block… -122. ## 7 2015-05-13 23:30:00 VEHICLE … STOLEN AU… Wednesday INGLESIDE NONE AVALON … -122. ## 8 2015-05-13 23:30:00 VEHICLE … STOLEN AU… Wednesday BAYVIEW NONE KIRKWOO… -122. ## 9 2015-05-13 23:00:00 LARCENY/… GRAND THE… Wednesday RICHMOND NONE 600 Blo… -123. ## 10 2015-05-13 23:00:00 LARCENY/… GRAND THE… Wednesday CENTRAL NONE JEFFERS… -122. ## # … with 878,039 more rows, and 1 more variable: Y &lt;dbl&gt; Those columns indeed, and pushing a million rows! That’s why it took so long! There are also 39 categories of crime, so we need to cut that down some. There are only ten districts, however, so we should be able to use that as is. \\(\\blacksquare\\) How is the response variable here different to the one in the question about steak preferences (and therefore why would multinom from package nnet be the method of choice)? Solution Steak preferences have a natural order, while crime categories do not. Since they are unordered, multinom is better than polr. \\(\\blacksquare\\) Find out which crime categories there are, and arrange them in order of how many crimes there were in each category. Solution This can be the one you know, a group-by and summarize, followed by arrange to sort: sfcrime %&gt;% group_by(Category) %&gt;% summarize(count=n()) %&gt;% arrange(desc(count)) ## # A tibble: 39 x 2 ## Category count ## &lt;chr&gt; &lt;int&gt; ## 1 LARCENY/THEFT 174900 ## 2 OTHER OFFENSES 126182 ## 3 NON-CRIMINAL 92304 ## 4 ASSAULT 76876 ## 5 DRUG/NARCOTIC 53971 ## 6 VEHICLE THEFT 53781 ## 7 VANDALISM 44725 ## 8 WARRANTS 42214 ## 9 BURGLARY 36755 ## 10 SUSPICIOUS OCC 31414 ## # … with 29 more rows or this one does the same thing and saves a step: sfcrime %&gt;% count(Category) %&gt;% arrange(desc(n)) ## # A tibble: 39 x 2 ## Category n ## &lt;chr&gt; &lt;int&gt; ## 1 LARCENY/THEFT 174900 ## 2 OTHER OFFENSES 126182 ## 3 NON-CRIMINAL 92304 ## 4 ASSAULT 76876 ## 5 DRUG/NARCOTIC 53971 ## 6 VEHICLE THEFT 53781 ## 7 VANDALISM 44725 ## 8 WARRANTS 42214 ## 9 BURGLARY 36755 ## 10 SUSPICIOUS OCC 31414 ## # … with 29 more rows For this one, do the count step first, to see what you get. It produces a two-column data frame with the column of counts called n. So now you know that the second line has to be arrange(desc(n)), whereas before you tried count, all you knew is that it was arrange-desc-something. You need to sort in descending order so that the categories you want to see actually do appear at the top. \\(\\blacksquare\\) Which are the four most frequent “interesting” crime categories, that is to say, not including “other offenses” and “non-criminal”? Get them into a vector called my.crimes. See if you can find a way of doing this that doesn’t involve typing them in (for full marks). Solution The most frequent interesting ones are, in order, larceny-theft, assault, drug-narcotic and vehicle theft. The fact that “other offenses” is so big indicates that there are a lot of possible crimes out there, and even 39 categories of crime isn’t enough. “Non-criminal” means, I think, that the police were called, but on arriving at the scene, they found that no law had been broken. I think the easy way to get the “top four” crimes out is to pull them out of the data frame that count produces. They are rows 1, 4, 5 and 6, so add a slice to your pipeline: my.rows=c(1,4,5,6) my.crimes = sfcrime %&gt;% count(Category) %&gt;% arrange(desc(n)) %&gt;% slice(my.rows) %&gt;% pull(Category) my.crimes ## [1] &quot;LARCENY/THEFT&quot; &quot;ASSAULT&quot; &quot;DRUG/NARCOTIC&quot; &quot;VEHICLE THEFT&quot; I just want the Category column (as a vector), and pull is the way to get that. (If I don’t do pull, I get a data frame.) If you can’t think of anything, just type them into a vector with c, or better, copy-paste them from the console. But that’s a last resort, and would cost you a point. If you do this, they have to match exactly, UPPERCASE and all. \\(\\blacksquare\\) (Digression, but needed for the next part.) The R vector letters contains the lowercase letters from a to z. Consider the vector ('a','m',3,'Q'). Some of these are found amongst the lowercase letters, and some not. Type these into a vector v and explain briefly why v %in% letters produces what it does. Solution This is the ultimate “try it and see”: v=c(&#39;a&#39;,&#39;m&#39;,3,&#39;Q&#39;) v %in% letters ## [1] TRUE TRUE FALSE FALSE The first two elements of the answer are TRUE because lowercase-a and lowercase-m can be found in the lowercase letters somewhere. The other two are false because the number 3 and the uppercase-Q cannot be found anywhere in the lowercase letters. The name is %in% because it’s asking whether each element of the first vector (one at a time) is in the set defined by the second thing: “is a a lowercase letter?” … is “Q a lowercase letter?” and getting the answers “yes, yes, no, no”. \\(\\blacksquare\\) We are going to filter only the rows of our data frame that have one of the crimes in my.crimes as their Category. Also, select only the columns Category, DayOfWeek and PdDistrict. Save the resulting data frame and display its structure. (You should have a lot fewer rows than you did before.) Solution The hard part about this is to get the inputs to %in% the right way around. We are testing the things in Category one at a time for membership in the set in my.crimes, so this: sfcrimea = sfcrime %&gt;% filter(Category %in% my.crimes) %&gt;% select(c(Category,DayOfWeek,PdDistrict)) sfcrimea ## # A tibble: 359,528 x 3 ## Category DayOfWeek PdDistrict ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 LARCENY/THEFT Wednesday NORTHERN ## 2 LARCENY/THEFT Wednesday PARK ## 3 LARCENY/THEFT Wednesday INGLESIDE ## 4 VEHICLE THEFT Wednesday INGLESIDE ## 5 VEHICLE THEFT Wednesday BAYVIEW ## 6 LARCENY/THEFT Wednesday RICHMOND ## 7 LARCENY/THEFT Wednesday CENTRAL ## 8 LARCENY/THEFT Wednesday CENTRAL ## 9 LARCENY/THEFT Wednesday NORTHERN ## 10 ASSAULT Wednesday INGLESIDE ## # … with 359,518 more rows I had trouble thinking of a good name for this one, so I put an “a” on the end. (I would have used a number, but I prefer those for models.) Down to a “mere” 359,000 rows. Don’t be stressed that the Category factor still has 39 levels (the original 39 crime categories); only four of them have any data in them: sfcrimea %&gt;% count(Category) ## # A tibble: 4 x 2 ## Category n ## &lt;chr&gt; &lt;int&gt; ## 1 ASSAULT 76876 ## 2 DRUG/NARCOTIC 53971 ## 3 LARCENY/THEFT 174900 ## 4 VEHICLE THEFT 53781 So all of the crimes that are left are one of the four Categories we want to look at. \\(\\blacksquare\\) Save these data in a file sfcrime1.csv. Solution This is write_csv again: write_csv(sfcrimea,&quot;sfcrime1.csv&quot;) \\(\\blacksquare\\) 28.12 What sports do these athletes play? The data at link are physical and physiological measurements of 202 male and female Australian elite athletes. The data values are separated by tabs. We are going to see whether we can predict the sport an athlete plays from their height and weight. The sports, if you care, are respectively basketball, “field athletics” (eg. shot put, javelin throw, long jump etc.), gymnastics, netball, rowing, swimming, 400m running, tennis, sprinting (100m or 200m running), water polo. Read in the data and display the first few rows. Solution The data values are separated by tabs, so read_tsv is the thing: my_url &lt;- &quot;http://ritsokiguess.site/datafiles/ais.txt&quot; athletes &lt;- read_tsv(my_url) ## ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## cols( ## Sex = col_character(), ## Sport = col_character(), ## RCC = col_double(), ## WCC = col_double(), ## Hc = col_double(), ## Hg = col_double(), ## Ferr = col_double(), ## BMI = col_double(), ## SSF = col_double(), ## `%Bfat` = col_double(), ## LBM = col_double(), ## Ht = col_double(), ## Wt = col_double() ## ) athletes ## # A tibble: 202 x 13 ## Sex Sport RCC WCC Hc Hg Ferr BMI SSF `%Bfat` LBM Ht Wt ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female Netball 4.56 13.3 42.2 13.6 20 19.2 49 11.3 53.1 177. 59.9 ## 2 female Netball 4.15 6 38 12.7 59 21.2 110. 25.3 47.1 173. 63 ## 3 female Netball 4.16 7.6 37.5 12.3 22 21.4 89 19.4 53.4 176 66.3 ## 4 female Netball 4.32 6.4 37.7 12.3 30 21.0 98.3 19.6 48.8 170. 60.7 ## 5 female Netball 4.06 5.8 38.7 12.8 78 21.8 122. 23.1 56.0 183 72.9 ## 6 female Netball 4.12 6.1 36.6 11.8 21 21.4 90.4 16.9 56.4 178. 67.9 ## 7 female Netball 4.17 5 37.4 12.7 109 21.5 107. 21.3 53.1 177. 67.5 ## 8 female Netball 3.8 6.6 36.5 12.4 102 24.4 157. 26.6 54.4 174. 74.1 ## 9 female Netball 3.96 5.5 36.3 12.4 71 22.6 101. 17.9 56.0 174. 68.2 ## 10 female Netball 4.44 9.7 41.4 14.1 64 22.8 126. 25.0 51.6 174. 68.8 ## # … with 192 more rows If you didn’t remember that, this also works: athletes &lt;- read_delim(my_url, &quot;\\t&quot;) ## ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## cols( ## Sex = col_character(), ## Sport = col_character(), ## RCC = col_double(), ## WCC = col_double(), ## Hc = col_double(), ## Hg = col_double(), ## Ferr = col_double(), ## BMI = col_double(), ## SSF = col_double(), ## `%Bfat` = col_double(), ## LBM = col_double(), ## Ht = col_double(), ## Wt = col_double() ## ) (this is the R way of expressing “tab”.) \\(\\blacksquare\\) Make a scatterplot of height vs. weight, with the points coloured by what sport the athlete plays. Put height on the \\(x\\)-axis and weight on the \\(y\\)-axis. Solution I’m doing this to give you a little intuition for later: ggplot(athletes, aes(x = Ht, y = Wt, colour = Sport)) + geom_point() The reason for giving you the axes to use is (i) neither variable is really a response, so it doesn’t actually matter which one goes on which axis, and (ii) I wanted to give the grader something consistent to look at. \\(\\blacksquare\\) Explain briefly why a multinomial model (multinom from nnet) would be the best thing to use to predict sport played from the other variables. Solution The categories of Sport are not in any kind of order, and there are more than two of them. That’s really all you needed, for which two marks is kind of generous. \\(\\blacksquare\\) Fit a suitable model for predicting sport played from height and weight. (You don’t need to look at the results.) 100 steps isn’t quite enough, so set maxit equal to a larger number to allow the estimation to finish. Solution 120 steps is actually enough, but any number larger than 110 is fine. It doesn’t matter if your guess is way too high. Like this: library(nnet) sport.1 &lt;- multinom(Sport ~ Ht + Wt, data = athletes, maxit = 200) ## # weights: 40 (27 variable) ## initial value 465.122189 ## iter 10 value 410.089598 ## iter 20 value 391.426721 ## iter 30 value 365.829150 ## iter 40 value 355.326457 ## iter 50 value 351.255395 ## iter 60 value 350.876479 ## iter 70 value 350.729699 ## iter 80 value 350.532323 ## iter 90 value 350.480130 ## iter 100 value 350.349271 ## iter 110 value 350.312029 ## final value 350.311949 ## converged As long as you see the word converged at the end, you’re good. \\(\\blacksquare\\) Demonstrate using anova that Wt should not be removed from this model. Solution The idea is to fit a model without Wt, and then show that it fits significantly worse: sport.2 &lt;- update(sport.1, . ~ . - Wt) ## # weights: 30 (18 variable) ## initial value 465.122189 ## iter 10 value 447.375728 ## iter 20 value 413.597441 ## iter 30 value 396.685596 ## iter 40 value 394.121380 ## iter 50 value 394.116993 ## iter 60 value 394.116434 ## final value 394.116429 ## converged anova(sport.2, sport.1, test = &quot;Chisq&quot;) ## Likelihood ratio tests of Multinomial Models ## ## Response: Sport ## Model Resid. df Resid. Dev Test Df LR stat. Pr(Chi) ## 1 Ht 1800 788.2329 ## 2 Ht + Wt 1791 700.6239 1 vs 2 9 87.60896 4.884981e-15 The P-value is very small indeed, so the bigger model sport.1 is definitely better (or the smaller model sport.2 is significantly worse, however you want to say it). So taking Wt out is definitely a mistake. This is what I would have guessed (I actually wrote the question in anticipation of this being the answer) because weight certainly seems to help in distinguishing the sports. For example, the field athletes seem to be heavy for their height compared to the other athletes (look back at the graph you made). drop1, the obvious thing, doesn’t work here: drop1(sport.1, test = &quot;Chisq&quot;, trace = T) ## trying - Ht ## Error in if (trace) {: argument is not interpretable as logical I gotta figure out what that error is. Does step? step(sport.1, direction = &quot;backward&quot;, test = &quot;Chisq&quot;) ## Start: AIC=754.62 ## Sport ~ Ht + Wt ## ## trying - Ht ## # weights: 30 (18 variable) ## initial value 465.122189 ## iter 10 value 441.367394 ## iter 20 value 381.021649 ## iter 30 value 380.326030 ## final value 380.305003 ## converged ## trying - Wt ## # weights: 30 (18 variable) ## initial value 465.122189 ## iter 10 value 447.375728 ## iter 20 value 413.597441 ## iter 30 value 396.685596 ## iter 40 value 394.121380 ## iter 50 value 394.116993 ## iter 60 value 394.116434 ## final value 394.116429 ## converged ## Df AIC ## &lt;none&gt; 27 754.6239 ## - Ht 18 796.6100 ## - Wt 18 824.2329 ## Call: ## multinom(formula = Sport ~ Ht + Wt, data = athletes, maxit = 200) ## ## Coefficients: ## (Intercept) Ht Wt ## Field 59.98535 -0.4671650 0.31466413 ## Gym 112.49889 -0.5027056 -0.57087657 ## Netball 47.70209 -0.2947852 0.07992763 ## Row 35.90829 -0.2517942 0.14164007 ## Swim 36.82832 -0.2444077 0.10544986 ## T400m 32.73554 -0.1482589 -0.07830622 ## Tennis 41.92855 -0.2278949 -0.01979877 ## TSprnt 51.43723 -0.3359534 0.12378285 ## WPolo 23.35291 -0.2089807 0.18819526 ## ## Residual Deviance: 700.6239 ## AIC: 754.6239 Curiously enough, it does. The final model is the same as the initial one, telling us that neither variable should be removed. \\(\\blacksquare\\) Make a data frame consisting of all combinations of Ht 160, 180 and 200 (cm), and Wt 50, 75, and 100 (kg), and use it to obtain predicted probabilities of athletes of those heights and weights playing each of the sports. Display the results. You might have to display them smaller, or reduce the number of decimal places49 to fit them on the page. Solution This is (again) the easier way: fill vectors with the given values, use crossing to get the combinations, and feed that into predict, thus: Hts &lt;- c(160, 180, 200) Wts &lt;- c(50, 75, 100) new &lt;- crossing(Ht = Hts, Wt = Wts) new ## # A tibble: 9 x 2 ## Ht Wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 160 50 ## 2 160 75 ## 3 160 100 ## 4 180 50 ## 5 180 75 ## 6 180 100 ## 7 200 50 ## 8 200 75 ## 9 200 100 and then p &lt;- predict(sport.1, new, type = &quot;probs&quot;) cbind(new, p) ## Ht Wt BBall Field Gym Netball Row Swim ## 1 160 50 2.147109e-03 5.676191e-03 7.269646e-02 0.1997276344 0.0320511810 0.0429354624 ## 2 160 75 1.044581e-04 7.203893e-01 2.240720e-09 0.0716686494 0.0537984065 0.0291616004 ## 3 160 100 5.541050e-08 9.968725e-01 7.530504e-19 0.0002804029 0.0009845934 0.0002159577 ## 4 180 50 9.142760e-02 2.116103e-05 1.331347e-04 0.0233985667 0.0088717680 0.0137765580 ## 5 180 75 7.787300e-02 4.701849e-02 7.184344e-11 0.1469948033 0.2607097148 0.1638164831 ## 6 180 100 5.379479e-04 8.473139e-01 3.144321e-19 0.0074895973 0.0621366751 0.0157985864 ## 7 200 50 6.907544e-01 1.399715e-08 4.326057e-08 0.0004863664 0.0004357120 0.0007843113 ## 8 200 75 8.889269e-01 4.698989e-05 3.527126e-14 0.0046164597 0.0193454719 0.0140908812 ## 9 200 100 2.738448e-01 3.776290e-02 6.884080e-21 0.0104894025 0.2056152576 0.0606015871 ## T400m Tennis TSprnt WPolo ## 1 3.517558e-01 1.885847e-01 1.033315e-01 1.094045e-03 ## 2 2.416185e-03 5.592833e-03 1.109879e-01 5.880674e-03 ## 3 1.809594e-07 1.808504e-06 1.299813e-03 3.446523e-04 ## 4 7.721547e-01 8.418976e-02 5.313751e-03 7.129763e-04 ## 5 9.285706e-02 4.371258e-02 9.992310e-02 6.709478e-02 ## 6 9.056685e-05 1.840761e-04 1.523963e-02 5.120898e-02 ## 7 3.007396e-01 6.668610e-03 4.848339e-05 8.244005e-05 ## 8 5.464293e-02 5.231368e-03 1.377496e-03 1.172154e-02 ## 9 2.376695e-03 9.824069e-04 9.368804e-03 3.989581e-01 I’ll take this, but read on for an improvement. This still spills onto a second line (even printed this small). Let’s round the predicted probabilities to 2 decimals, which, with luck, will also kill the scientific notation: cbind(new, round(p, 2)) ## Ht Wt BBall Field Gym Netball Row Swim T400m Tennis TSprnt WPolo ## 1 160 50 0.00 0.01 0.07 0.20 0.03 0.04 0.35 0.19 0.10 0.00 ## 2 160 75 0.00 0.72 0.00 0.07 0.05 0.03 0.00 0.01 0.11 0.01 ## 3 160 100 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## 4 180 50 0.09 0.00 0.00 0.02 0.01 0.01 0.77 0.08 0.01 0.00 ## 5 180 75 0.08 0.05 0.00 0.15 0.26 0.16 0.09 0.04 0.10 0.07 ## 6 180 100 0.00 0.85 0.00 0.01 0.06 0.02 0.00 0.00 0.02 0.05 ## 7 200 50 0.69 0.00 0.00 0.00 0.00 0.00 0.30 0.01 0.00 0.00 ## 8 200 75 0.89 0.00 0.00 0.00 0.02 0.01 0.05 0.01 0.00 0.01 ## 9 200 100 0.27 0.04 0.00 0.01 0.21 0.06 0.00 0.00 0.01 0.40 Better. Much better. \\(\\blacksquare\\) For an athlete who is 180 cm tall and weighs 100 kg, what sport would you guess they play? How sure are you that you are right? Explain briefly. Solution Find this height and weight in your predictions (it’s row 6). Look along the line for the highest probability, which is 0.85 for Field (that is, field athletics). All the other probabilities are much smaller (the biggest of the others is 0.06). So this means we would guess the athlete to be a field athlete, and because the predicted probability is so big, we are very likely to be right. This kind of thought process is characteristic of discriminant analysis, which we’ll see more of later in the course. Compare that with the scatterplot you drew earlier: the field athletes do seem to be distinct from the rest in terms of height-weight combination. Some of the other height-weight combinations are almost equally obvious: for example, very tall people who are not very heavy are likely to play basketball. 400m runners are likely to be of moderate height but light weight. Some of the other sports, or height-weight combinations, are difficult to judge. Consider also that we have mixed up males and females in this data set. We might gain some clarity by including Sex in the model, and also in the predictions. But I wanted to keep this question relatively simple for you, and I wanted to stop from getting unreasonably long. (You can decide whether you think it’s already too long.) \\(\\blacksquare\\) For this, use round.↩ If you do not take out the NA values, they are shown separately on the end of the summary for that column.↩ The American Green party is more libertarian than Green parties elsewhere.↩ Northern Ireland’s political parties distinguish themselves by politics and religion. Northern Ireland has always had political tensions between its Protestants and its Catholics.↩ There are three political parties; using the first as a baseline, there are therefore \\(3-1=2\\) coefficients for each variable.↩ It amuses me that Toronto’s current (2021) mayor, named Tory, is politically a Tory.↩ When you have variables that are categories, you might have more than one individual with exactly the same categories; on the other hand, if they had measured Size as, say, length in centimetres, it would have been very unlikely to get two alligators of exactly the same size.↩ Discovered by me two minutes ago.↩ There were only 216 alligators total, which is a small sample size for this kind of thing, especially with all those parameters to estimate.↩ You are almost certainly going to get the Capital Letters wrong in DayOfWeek, once, somewhere in the process. I did.↩ This is something we’ll see again in discriminant analysis.↩ The old base-R stuff like cbind makes some guesses about what you want. If it guesses right, it makes things work very smoothly, but if not, you can end up with some mysterious problems to debug. The old read.table for reading in data, likewise, used to turn text into factors, without telling you. The tidyverse, on the other hand, requires you to be more explicit about what you want, or to have things be the same type, so that you don’t run into these kinds of things.↩ Statistical significance as an idea grew up in the days before “big data”.↩ For this, use round.↩ "]]
