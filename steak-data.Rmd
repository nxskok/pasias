##  How do you like your steak -- the data


  <a name="q:steak-data">*</a> 
This question takes you through the data preparation for one
of the other questions. You don't have to do *this*
question, but you may find it interesting or useful.

When you order a steak in a restaurant, the server will ask
you how you would like it cooked, or to be precise, *how much*
you would like it cooked: rare (hardly cooked at all), through medium
rare, medium, medium well to well (which means "well done", so that
the meat has only a little red to it). Could you guess how a person
likes their steak cooked, from some other information about them? The
website [link](fivethirtyeight.com) commissioned a survey where they
asked a number of people how they preferred their steak, along with as
many other things as they could think of to ask. (Many of the
variables below are related to risk-taking, which was something the
people designing the survey thought might have something to do with
liking steak rare.) The variables of interest are all factors or true/false:



* `respondent_ID`: a ten-digit number identifying each
person who responded to the survey.

* `lottery_a`: true if the respondent preferred lottery A
with a small chance to win a lot of money, to lottery B, with a
larger chance to win less money.

* `smoke`: true if the respondent is currently a smoker

* `alcohol`: true if the respondent at least occasionally
drinks alcohol.

* `gamble`: true if the respondent likes to gamble (eg.
betting on horse racing or playing the lottery)

* `skydiving`: true if the respondent has ever been
skydiving.

* `speed`: true if the respondent likes to drive fast

* `cheated`: true if the respondent has ever cheated on a
spouse or girlfriend/boyfriend

* `steak`: true if the respondent likes to eat steak

* `steak_prep` (response): how the respondent likes their
steak cooked (factor, as described above, with 5 levels).

* `female`: true if the respondent is female

* `age`: age group, from 18--29 to 60+.

* `hhold_income`: household income group, from \$0--24,999
to \$150,000+.

* `educ`: highest level of education attained, from 
"less  than high school" 
up to "graduate degree"

* `region`: region (of the US)
that the respondent lives in (five values).


The data are in
[link](https://raw.githubusercontent.com/nxskok/datafiles/master/steak.csv). 



(a) Read in the data and display the first few lines.


Solution


The usual:

```{r steak-data-1 }
my_url <- "https://raw.githubusercontent.com/nxskok/datafiles/master/steak.csv"
steak0 <- read_csv(my_url)
steak0
```

 

I'm using a temporary name for reasons that will become clear shortly.


$\blacksquare$

(b) What do you immediately notice about your data frame? Run `summary` on the entire data frame. Would you say you have a lot of missing values, or only a few?

Solution


I see missing values, starting in the very first row.
Running the data frame through `summary` gives this, either as `summary(steak0)` or this way:
```{r steak-data-2 }
steak0 %>% summary()
```

     

Make a call about whether you think that's a lot of missing values or only a few. This might not be all of them, because missing text doesn't show here (we see later how to make it show up).

$\blacksquare$

(c) What does the function `drop_na` do when applied to a data frame with missing values? To find out, pass the data frame into `drop_na()`, then into `summary` again. What has happened?

Solution


Let's try it and see.

```{r steak-data-3 }
steak0 %>% drop_na() %>% summary()
```

 

The missing values, the ones we can see anyway, have all gone. Precisely, `drop_na`, as its
name suggests, drops all the rows that have missing values in them
anywhere. This is potentially wasteful, since a row might be missing
only one value, and we drop the entire rest of the row, throwing away
the good data as well. If you check, we started with 550 rows, and we
now have only 311 left. Ouch.

So now we'll save this into our "good" data frame, which means doing it again (now that we know it works):

```{r steak-data-4 }
steak0 %>% drop_na() -> steak
```

 

Extra: another way to handle missing data is called "imputation":
what you do is to *estimate* a value for any missing data, and
then use that later on as if it were the truth. One way of estimating
missing values is to do a regression (of appropriate kind: regular or
logistic) to predict a column with missing values from all the other
columns.

Extra extra: below we see how we used to have to do this, for your information.

First, we run `complete.cases` on the data frame:

```{r steak-data-5 }
complete.cases(steak0)
```

 

You might be able to guess what this does, in the light of what we
just did, but if not, you can investigate. Let's pick three rows where
`complete.cases` is 
`TRUE` and three where it's
`FALSE`, and see what happens.

I'll pick rows 496, 497, and 498 for the TRUE rows, and 540, 541 and
542 for the FALSE ones. Let's assemble these rows into a vector and
use `slice` to display the rows with these numbers:

```{r steak-data-6 }
rows <- c(496, 497, 498, 540, 541, 542)
rows
```

 

Like this:

```{r steak-data-7 }
steak0 %>% slice(rows)
```

 

What's the difference? 
The rows where `complete.cases` is FALSE have one (or more)
missing values in them; where `complete.cases` is TRUE the
rows have no missing values. (Depending on the rows you choose,
you may not see the missing value(s), as I didn't.)
Extra (within "extra extra": I hope you are keeping track): this
is a bit tricky to investigate more thoroughly, because the text
variables might have missing values in them, and they won't show
up unless we turn them into a factor first:
```{r steak-data-8 }
steak0 %>%
  mutate(across(where(is.character), ~factor(.))) %>%
  summary()
```

     

There are missing values everywhere. What the `where` 
does is to do something for each column where the first thing is true:
here, if the column is text, then replace it by the factor version of
itself. This makes for a better summary, one that shows how many
observations are in each category, and, more important for us, how
many are missing (a lot).

All right, so there are 15 columns, so let's investigate missingness
in our rows by looking at the columns 1 through 8 and then 9 through
15, so they all fit on the screen. Recall that you can `select`
columns by number:

```{r steak-data-9 }
steak0 %>% select(1:8) %>% slice(rows)
```

 

and

```{r steak-data-10 }
steak0 %>% select(9:15) %>% slice(rows)
```

 

In this case, the first three rows have no missing values anywhere,
and the last three rows have exactly one missing value. This
corresponds to what we would expect, with `complete.cases`
identifying rows that have any missing values.

What we now need to do is to obtain a data frame that contains only
the rows with non-missing values. This can be done by saving the
result of `complete.cases` in a variable first; `filter`
can take anything that produces a true or a false for each row, and
will return the rows for which the thing it was fed was true.
```{r cca}
cc <- complete.cases(steak0)
steak0 %>% filter(cc) -> steak.complete
```

     

A quick check that we got rid of the missing values:

```{r ccc}
steak.complete
```

 

There are no missing values *there*. Of course, this is not a
proof, and there might be some missing values further down, but at
least it suggests that we might be good.

For proof, this is the easiest way I know:

```{r ccd}
steak.complete %>%
  mutate(across(where(is.character), ~factor(.))) %>%
  summary()
```

 

If there were any missing values, they would be listed on the end of
the counts of observations for each level, or on the bottom of the
five-number sumamries. But there aren't.  So here's your proof.


$\blacksquare$

(d) Write the data into a `.csv` file, with a name like
`steak1.csv`.  Open this file in a spreadsheet and (quickly)
verify that you have the right columns and no missing values.


Solution


This is `write_csv`, using my output from 
`drop_na`:
```{r steak-data-11 }
write_csv(steak, "steak1.csv")
```

     

Open up Excel, or whatever you have, and take a look. You should have
all the right columns, and, scrolling down, no visible missing values.

$\blacksquare$





