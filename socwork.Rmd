##  Salaries of social workers


 Another salary-prediction question: does the number of years
of work experience that a social worker has help to predict their 
salary? Data for 50 social workers are in
[link](http://ritsokiguess.site/datafiles/socwork.txt). 



(a) Read the data into R. Check that you have 50 observations on
two variables. Also do something to check that the years of
experience and annual salary figures look reasonable overall.


Solution


```{r }
my_url <- "http://ritsokiguess.site/datafiles/socwork.txt"
soc <- read_delim(my_url, " ")
soc
```

 

That checks that we have the right *number* of observations; to
check that we have sensible *values*, something like
`summary` is called for:

```{r }
summary(soc)
```

 

A person working in any field cannot have a negative number of years
of experience, and cannot have more than about 40 years of experience
(or else they would have retired). Our experience numbers fit
that. Salaries had better be five or six figures, and salaries for
social workers are not generally all that high, so these figures look
reasonable. 

A rather more `tidyverse` way is this:

```{r }
soc %>% 
  summarize(across(everything(), list(min = ~min(.),  max = ~max(.))))
```

 

This gets the minimum and maximum of all the variables. I would have
liked them arranged in a nice rectangle (`min` and `max`
as rows, the variables as columns), but that's not how this came out.

The code so far uses `across`. This means to do something across multiple columns. In this case, we want to do the calculation on *all* the columns, so we use the select-helper `everything`. You can use any of the other select-helpers like `starts_with`, or you could do something like `where(is.numeric)` to do your summaries only on the quantitative columns (which would also work here). The dots inside `min` and `max` mean "the variable we are looking at at the moment", and the squiggles before `min` and `max` mean "calculate", so you read the above code as "for each column, work out the smallest and largest value of it".

What, you want a nice rectangle? This is a pivot-longer, but the fancy version because the column names encode two kinds of things, a variable and a statistic:

```{r}
soc %>% 
  summarize(across(everything(), list(min = ~min(.),  max = ~max(.)))) %>% 
  pivot_longer(everything(), 
               names_to = c("variable", "statistic"), 
               names_sep = "_",
               values_to = "value"
               )
```
and then

```{r}
soc %>% 
  summarize(across(everything(), list(min = ~min(.),  max = ~max(.)))) %>% 
  pivot_longer(everything(), 
               names_to = c("variable", "statistic"), 
               names_sep = "_",
               values_to = "value"
               ) %>% 
  pivot_wider(names_from = statistic, values_from = value)
```

Note the strategy: make it longer first, then figure out what to do next. This is different from `rowwise`; in fact, we are working "columnwise", doing something for each column, no matter how many there are. My go-to for this stuff is [here](https://dplyr.tidyverse.org/articles/colwise.html).

Another way to work is with the five-number summary. This gives a more nuanced picture of the data values we have.^[This might be overkill at this point, since we really only care about whether our data values are reasonable, and often just looking at the highest and lowest values will tell us that.] 

The base-R five-number summary looks like this:

```{r }
qq <- quantile(soc$experience)
qq
```

This is what's known as a "named vector". The numbers on the bottom are the summaries themselves, and the names above say which percentile you are looking at. Unfortunately, the `tidyverse` doesn't like names, so modelling after the above doesn't quite work:

```{r}
soc %>% 
  summarize(across(everything(), list(q = ~quantile(.))))
```

You can guess which percentile is which (they have to be in order), but this is not completely satisfactory. A workaround is to get hold of the `names` and add them to the result:

```{r}
soc %>% 
  summarize(across(everything(), list(q = ~quantile(.)))) %>% 
  mutate(pct = names(qq))
```

$\blacksquare$

(b) Make a scatterplot showing how salary depends on
experience. Does the nature of the trend make sense?


Solution


The usual:
```{r }
ggplot(soc, aes(x = experience, y = salary)) + geom_point()
```

 

As experience goes up, salary also goes up, as you would expect. Also,
the trend seems more or less straight.

$\blacksquare$

(c) Fit a regression predicting salary from experience, and
display the results. Is the slope positive or negative? Does that
make sense?


Solution


```{r }
soc.1 <- lm(salary ~ experience, data = soc)
summary(soc.1)
```

 

The slope is (significantly) positive, which squares with our guess
(more experience goes with greater salary), and also the upward trend
on the scatterplot. The value of the slope is about 2,000; this means
that one more year of experience goes with about a \$2,000 increase in
salary. 

$\blacksquare$

(d) Obtain and plot the residuals against the fitted values. What
problem do you see?


Solution


The easiest way to do this with `ggplot` is to plot the
*regression object* (even though it is not actually a data
frame), and plot the `.fitted` and `.resid`
columns in it, not forgetting the initial dots:
```{r }
ggplot(soc.1, aes(x = .fitted, y = .resid)) + geom_point()
```

       
I see a "fanning-out": the residuals are getting bigger *in size* 
(further away from zero) as the fitted values get bigger. That
is, when the (estimated) salary gets larger, it also gets more
variable. 

Fanning-out is sometimes hard to see. What you can do if you suspect
that it might have happened is to plot the *absolute value* of
the residuals against the fitted values. The absolute value is the
residual without its plus or minus sign, so if the residuals are
getting bigger in size, their absolute values are getting bigger. That
would look like this:

```{r }
ggplot(soc.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()
```

 

I added a smooth trend to this to help us judge whether the
absolute-value-residuals are getting bigger as the fitted values get
bigger. It looks to me as if the overall trend is an increasing one,
apart from those few small fitted values that have larger-sized
residuals. Don't get thrown off by the kinks in the smooth trend. Here
is a smoother version:

```{r }
ggplot(soc.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth(span = 2)
```

 

The larger fitted values, according to this, have residuals larger in size.

The thing that controls the smoothness of the smooth trend is the
value of `span` in `geom_smooth`. The default is
0.75. The larger the value you use, the smoother the trend; the
smaller, the more wiggly. I'm inclined to think that the default value
is a bit too small. Possibly this value is too big, but it shows you
the idea.

$\blacksquare$

(e) The problem you unearthed in the previous part is often helped
by a transformation. Run Box-Cox on your data to find a suitable
transformation. What transformation is suggested?


Solution


You'll need to call in (and install if necessary) the package
`MASS` that contains `boxcox`:
```{r }
library(MASS)
```

 

I explain that "masked" thing below.

```{r }
boxcox(salary ~ experience, data = soc)
```

 

That one looks like $\lambda=0$ or log. You could probably also
justify fourth root (power 0.25), but log is a very common
transformation, which people won't need much persuasion to accept.

There's one annoyance with `MASS`: it has a `select`
(which I have never used), and if you load `tidyverse` first
and `MASS` second, as I have done here, when you mean to run
the column-selection `select`, it will actually run the
`select` that comes from `MASS`, and give you an error
that you will have a terrible time debugging. That's what that
"masked" message was when you loaded `MASS`. This is a great place to learn about the `conflicted` package. See [here](https://github.com/r-lib/conflicted) for how it works. (Scroll down to under the list of files.)

If you want to insist on something like "the `select` that lives in `dplyr`", 
you can do that by saying
`dplyr::select`. But this is kind of cumbersome if you don't
need to do it.

$\blacksquare$

(f) Calculate a new variable as suggested by your
transformation. Use your transformed response in a regression,
showing the summary.


Solution


The best way is to add the new variable to the data frame using
`mutate`, and save that new data frame. That goes like this:
```{r }
soc.2 <- soc %>% mutate(log_salary = log(salary))
```

       

and then

```{r }
soc.3 <- lm(log_salary ~ experience, data = soc.2)
summary(soc.3)
```

 

I think it's best to save the data frame with `log_salary` in
it, since we'll be doing a couple of things with it, and it's best to
be able to start from `soc.2`. But you can also do this:

```{r }
soc %>%
  mutate(log_salary = log(salary)) %>%
  lm(log_salary ~ experience, data = .) %>%
  summary()
```

 

The second line is where the fun starts: `lm` wants the data
frame as a `data=` at the end. So, to specify a data frame in
something like `lm`, we have to use the special symbol
`.`, which is another way to say 
"the data frame that came out of the previous step".

Got that? All right. The last line is a piece of cake in
comparison. Normally `summary` would require a data frame or a
fitted model object, but the second line produces one (a fitted model
object) as output, which goes into `summary` as the first
(and only) thing, so all is good and we get the regression output.

What we lose by doing this is that if we need something later from this
fitted model object, we are out of luck since we didn't save
it. That's why I created `soc.2` and `soc.3` above.

You can also put functions of things directly into `lm`:

```{r }
soc.1a <- lm(log(salary) ~ experience, data = soc)
summary(soc.1a)
```

 

$\blacksquare$

(g) Obtain and plot the residuals against the fitted values for
this regression. Do you seem to have solved the problem with the
previous residual plot?


Solution


As we did before, treating the regression object as if it were a
data frame:
```{r }
ggplot(soc.3, aes(x = .fitted, y = .resid)) + geom_point()
```

       

That, to my mind, is a horizontal band of points, so I would say yes,
I have solved the fanning out.

One concern I have about the residuals is that there seem to be a
couple of very negative values: that is, are the residuals normally
distributed as they should be? Well, that's easy enough to check:

```{r }
ggplot(soc.3, aes(sample = .resid)) + stat_qq() + stat_qq_line()
```

 

The issues here are that those bottom two values are a bit too low,
and the top few values are a bit bunched up (that curve at the top).
It is really not bad, though, so I am making the call that I don't
think I needed to worry.
Note that the transformation we found here is the same as the
log-salary used by the management consultants in the
backward-elimination question, and with the same effect: an extra year
of experience goes with a *percent* increase in salary.

What increase? Well, the slope is about 0.05, so adding a year of
experience is predicted to increase log-salary by 0.05, or to
multiply actual salary by 

```{r }
exp(0.05)
```

 

or to increase salary by about 5\%.^[Mathematically,  $e^x$ is approximately $1+x$ for small $x$, which winds up meaning that the  slope in a model like this, if it is small, indicates about the  percent increase in the response associated with a 1-unit change in  the explanatory variable. Note that this only works with $e^x$ and  natural logs, not base 10 logs or anything like that.]


$\blacksquare$


