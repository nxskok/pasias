##   The Interpersonal Circumplex


 The "IPIP Interpersonal Circumplex" (see
[link](http://ipip.ori.org/newIPIP-IPCSurvey.htm)) is a personal
behaviour survey, where respondents have to rate how accurately a
number of statements about behaviour apply to them, on a scale from
1 ("very inaccurate") to 5 ("very accurate"). A survey was done
on 459 people, using a 44-item variant of the above questionnaire,
where the statements were as follows. Put an "I" or an "I am" in
front of each one:
\begin{multicols}{3}


* talkative

* find fault

* do a thorough job

* depressed

* original

* reserved

* helpful

* careless

* relaxed

* curious

* full of energy

* start quarrels

* reliable

* tense

* ingenious

* generate enthusiasm in others

* forgiving

* disorganized

* worry

* imaginative

* quiet

* trusting

* lazy

* emotionally stable

* inventive

* assertive

* cold and aloof

* persevere

* moody

* value artistic experiences

* shy

* considerate

* efficient

* calm in tense situations

* prefer routine work

* outgoing

* sometimes rude

* stick to plans

* nervous

* reflective

* have few artistic interests

* co-operative

* distractible

* sophisticated in art and music

\end{multicols}
I don't know what a "circumplex" is, but I know it's not one of those "hat" accents that they have in French.
The data are in
[link](http://www.utsc.utoronto.ca/~butler/d29/personality.txt). The
columns `PERS01` through `PERS44` represent the above traits.


(a) Read in the data and check that you have the right number
of rows and columns.

Solution


Separated by single spaces. 
```{r pers_conn}
my_url="http://www.utsc.utoronto.ca/~butler/d29/personality.txt"
pers=read_delim(my_url," ")
pers
```

       

Yep, 459 people (in rows), and 44 items (in columns), plus one column
of `id`s for the people who took the survey.

In case you were wondering about the "I" vs.\ "I am" thing, the
story seems to be that each behaviour needs to have a verb. If the
behaviour has a verb, "I" is all you need, but if it doesn't, you
have to add one, ie.\ "I am". 

Another thing you might be concerned about is whether these data are
"tidy" or not. To some extent, it depends on what you are going to
do with it. You could say that the `PERS` columns are all
survey-responses, just to different questions, and you might think of
doing something like this:

```{r }
pers %>% gather(item,response,-id)
```

 

to get a *really* long and skinny data frame. It all depends on
what you are doing with it. Long-and-skinny is ideal if you are going
to summarize the responses by survey item, or draw something like bar
charts of responses facetted by item:

```{r }
pers %>% gather(item,response,-id) %>%
ggplot(aes(x=response))+geom_bar()+facet_wrap(~item)
```

 

The first time I did this, item `PERS36` appeared out of order
at the end, and I was wondering what happened, until I realized it was
actually misspelled as `PES36`! I corrected it in the data
file, and it should be good now (though I wonder how many years that
error persisted for).

For us, in this problem, though, we need the wide format.


(b) There are some missing values among these responses. We
are going to eliminate all the individuals with any missing values
(since `princomp` can't handle them). Do this in two steps:
first run `complete.cases` on the data frame, which will
give `TRUE` if the row has no `NA`s and
`FALSE` otherwise. Save this result. Then use it to create
a new data frame that contains only rows without missing values
(that is, only the "complete cases").

Solution


Thus:
```{r }
v=complete.cases(pers)
table(v)
```

       

which shows that there are 26 rows that have missing values in them
somewhere. 

A way of asking "are there any missing values anywhere?" is:

```{r }
any(is.na(pers))
```

 

and the answer is "yes, there are". We'll come back to this after we
think we've removed all the missing values.

To create a new data frame with no missing values, we can use
`filter`. The thing we're filtering on doesn't have to be part
of the data frame we're working with:

```{r }
pers.ok = pers %>% filter(v) 
```

 

To check that all the missing values have gone:

```{r }
any(is.na(pers.ok))
```

 

and they have indeed gone.

The way I'd probably do this now (as opposed to a couple of years ago
when I wrote the above) is to create the new variable in the data frame,
like this:

```{r }
pers %>% mutate(v=complete.cases(pers)) %>% 
filter(v)
```

 

The `lgl` (true/false) variable `v` is at the end.

This way keeps everything better organized, to my mind, but they are
equally effective as a solution to the problem.

Extra: you might also have thought of the "tidy, remove, untidy"
strategy here. The trouble with that here is that you want to remove
*all* the observations for a subject who has *any* missing
ones. This is unlike the multidimensional scaling one in class where
we wanted to remove all the distances for two cities \emph{that we
knew ahead of time}. 

That gives me an idea, though. 

```{r }
pers %>%
gather(item,rating,-id)
```

 

To find out which *subjects* have any missing values, we can do a
`group_by` and `summarize` on *subjects* (that
means, the `id` column; the `PERS` in the column I
called `item` means "personality", not "person"!). What do
we summarize? Any one of the standard things like `mean` will
return `NA` if the thing whose mean you are finding has any NA
values in it anywhere, and a number if it's "complete", so this kind
of thing, adding to my pipeline:

```{r }
pers %>%
gather(item,rating,-id) %>%
group_by(id) %>%
summarize(m=mean(rating)) %>%
filter(is.na(m))
```

 

Most of the subjects had an actual numerical  mean here, whose value
we don't care about; all we care about here is whether the mean is
missing, which implies that one (or more) of the responses was
missing. 

So now we define a column `has_missing` that is true if the
subject has any missing values and false otherwise:

```{r }
pers.hm = pers %>%
gather(item,rating,-id) %>%
group_by(id) %>%
summarize(m=mean(rating)) %>%
mutate(has_missing=is.na(m))
pers.hm %>% print(n=15)
```

 

This data frame `pers.hm` has the same number of rows as the
original data frame `pers`, one per subject, so we can just
glue it onto the end of that:

```{r }
pers %>% bind_cols(pers.hm)
```

 

and then filter out the rows for which `has_missing` is true,
the same way as we used `complete.cases` above. (What we did
here is really a way of mimicking `complete.cases`.)


(c) Carry out a principal components analysis and obtain a
scree plot. 

Solution


This ought to be straightforward, but we've got to remember to
use only the columns with actual data in them: that is,
`PERS01` through `PERS44`:
```{r saljhsajd}
pers.1 = pers.ok %>% select(starts_with("PERS")) %>%
princomp(cor=T)
ggscreeplot(pers.1)
```

       


(d) How many components/factors should you use? Explain briefly.

Solution


I think the clearest elbow is at 7, so we should use 6
components/factors. You could make a case that the elbow at 6 is
also part of the scree, and therefore you should use 5
components/factors. Another one of those judgement calls.
Ignore the "backwards elbow"  at 5: this is
definitely part of the mountain rather than the scree. Backwards
elbows, as you'll recall, don't count as elbows anyway.
When I drew this in R Studio, the elbow at 6 was clearer than
the one at 7, so I went with 5 components/factors below.
The other way to go is to take the number of eigenvalues bigger
than 1: 
```{r size="small"}
summary(pers.1)
```

       
There are actually 10 of these. But if you look at the scree plot,
there really seems to be no reason to take 11 factors rather than,
say, 12 or 13. There are a lot of eigenvalues (standard deviations)
close to (but just below) 1, and no obvious "bargains" in terms of
variance explained: the "cumulative proportion" just keeps going
gradually up.


(e) <a name="part:score">*</a> Using your preferred number of factors, run a factor
analysis. Obtain "r"-type factor scores, as in class. You don't need to look at any output yet.

Solution


I'm going to do the 5 factors that I preferred the first time I
looked at this. Don't forget to grab only the appropriate
columns from `pers.ok`:
```{r }
pers.ok.1 = pers.ok %>% select(starts_with("PERS")) %>%
factanal(5,scores="r")
```

       

If you think 6 is better, you should feel free to use that here.


(f) Obtain the factor loadings. How much of the variability
does your chosen number of factors explain?

Solution


```{r size="footnotesize"}
pers.ok.1$loadings
```

       

The Cumulative Var line at the bottom says that our five factors
together have
explained 37\% of the variability. This is not great, but is the kind
of thing we have to live with in this kind of analysis (like the
personality one in class).


(g) Interpret each of your chosen number of factors. That is,
for each factor, identify the items that load heavily on it (you
can be fairly crude about this, eg. use a cutoff like 0.4 in
absolute value), and translate these items into the statements
given in each item. Then, if you can, name what the items loading
heavily on each factor have in common. Interpret a negative
loading as "not" whatever the item says.

Solution


This is a lot of work, but I figure you should go through it at
least once! If you have some number of factors other than 5,
your results will be different from mine. Keep going as long as
you reasonably can.
Factor 1: 3, 8 (negatively), 13, 18
(negative), 23 (negative), 28, 33, 38 and maybe 43
(negatively). These are: do a thorough job, not-careless,
reliable, not-disorganized, not-lazy, persevere, efficient,
stick to plans, not-distractible. These have the common theme of
paying attention to detail and getting the job done properly.
Factor 2: 1, not-6, 16, not-21, 26, not-31, 36. Talkative,
not-reserved, generates enthusiasm, not-quiet, assertive,
not-shy, outgoing. "Extravert" seems to capture all of those.
Factor 3: 4, not-9, 14, 19, not-24, 29, not-34, 39. Depressed,
not-relaxed, tense, worried, not emotionally stable, moody,
not-calm-when-tense, nervous. "Not happy" or something like that.
Notice how these seem to be jumping in steps of 5? The
psychology scoring of assessments like this is that a person's
score on some dimension is found by adding up their scores on
certain of the questions and subtracting their scores on others
("reverse-coded questions"). I'm guessing that these guys have
5 dimensions they are testing for, corresponding presumably to
my five factors. The questionnaire at
[link](http://ipip.ori.org/newIPIP-IPCScoringKey.htm) is
different, but you see the same idea there. (The jumps there seem to
be in steps of 8, since they have 8 dimensions.)
Factor 4: not-2, 7, not-12 (just), 22, not-27, 32, not-37,
42. Doesn't find fault, helpful, doesn't start quarrels,
trusting, not-cold-and-aloof, considerate, not-sometimes-rude,
co-operative. "Helps without judgement" or similar.
Factor 5: 5, 10, 15, 20, 25, 30, 40, 44. Original, curious,
ingenious, imaginative, inventive, values artistic experiences,
reflective, sophisticated in art and music. Creative.
I remembered that psychologists like to talk about the "big 5"
personality traits. These are extraversion (factor 2 here),
agreeableness (factor 4), openness (factor 5?),
conscientiousness (factor 1), and neuroticism (factor 3). The
correspondence appears to be pretty good. (I wrote my answers
above last year without thinking about "big 5" at all.)
I wonder whether 6 factors is different?
```{r size="footnotesize"}
pers.ok.2 = pers.ok %>% select(starts_with("PERS")) %>%
factanal(6,scores="r")
pers.ok.2$loadings
```

       
Much of the same kind of thing seems to be happening, though it's a
bit fuzzier. I suspect the devisers of this survey were adherents to
the "big 5" theory. The factor 6 here is items 11, 16 and 26, which
would be expected to belong to factor 2 here, given what we found
before. I think these particular items are about generating enthusiasm
in others, rather than (necessarily) about being extraverted oneself.


(h) Find a person who is extreme on each of your first three
factors (one at a time). For each of these people, what kind of
data should they have for the relevant ones of variables
`PERS01` through `PERS44`? Do they? Explain
reasonably briefly.

Solution


For this, we need the factor scores obtained in part
(<a href="#part:score">here</a>).\endnote{There are two types of scores here:
a person's scores on the psychological test, 1 through 5, and
their factor scores, which are decimal numbers centred at
zero. Try not to get these confused.}
I'm thinking that I will create a data frame
with the original data (with the missing values removed) and the
factor scores together, and then look in there. This will have a
lot of columns, but we'll only need to display some each time.
This is based on my 5-factor solution. I'm adding a column
`id` so that I know which of the individuals (with no
missing data) we are looking at:
```{r }
scores.1 = as_tibble(pers.ok.1$scores) %>% 
bind_cols(pers.ok) %>%
mutate(id=row_number()) 
scores.1
```

   
I did it this way, rather than using `data.frame`, so that I
would end up with a `tibble` that would display nicely rather
than running off the page. This meant turning the matrix of factor
scores into a `tibble` first and then gluing everything onto
it.  (There's no problem in using `data.frame` here if you
prefer. You could even begin with `data.frame` and pipe the
final result into `as_tibble` to end up with a
`tibble`.)
Let's start with factor 1. There are several ways to find the person
who scores highest and/or lowest on that factor:

```{r }
scores.1 %>% filter(Factor1==max(Factor1))
```

 

to display the maximum, or

```{r }
scores.1 %>% arrange(Factor1) %>% print(n=5)
```

 

to display the minimum (and in this case the five smallest ones), or

```{r }
scores.1 %>% filter(abs(Factor1)==max(abs(Factor1)))
```

 

to display the largest one in size, whether positive or negative. The
code is a bit obfuscated because I have to take absolute values
twice. Maybe it would be clearer to create a column with the absolute
values in it and look at that:

```{r }
scores.1 %>% mutate(abso=abs(Factor1)) %>% 
filter(abso==max(abso))
```

 

Does
this work too?

```{r }
scores.1 %>% arrange(desc(abs(Factor1))) %>% print(n=5)
```

 

It looks as if it does: "sort the Factor 1 scores in descending order by absolute value, and display the first few". The most extreme
scores on Factor 1 are all negative: the most positive one (found
above) was only about 1.70. 

For you, you don't have to be this sophisticated. It's enough to
eyeball the factor scores on factor 1 and find one that's reasonably
far away from zero. Then you note its row and `slice` that row,
later. 

I think I like the idea of creating a new column with the absolute
values in it and finding the largest of that. Before we pursue that,
though, remember that we don't need to look at *all* the
`PERS` columns, because only some of them load highly on each
factor. These are the ones I defined into `f1` first; the first
ones have positive loadings and the last three have negative loadings:

```{r }
f1=c(3,13,28,33,38,8,18,23,43)
scores.1 %>% mutate(abso=abs(Factor1)) %>% 
filter(abso==max(abso)) %>%
select(id,Factor1,num_range("PERS",f1,width=2)) %>%
print(width=Inf)
```

 

I don't think I've used `num_range` before, like, ever. It is
one of the select-helpers like `starts_with`. It is used when
you have column names that are some constant text followed by variable
numbers, which is exactly what we have here: we want to select the
`PERS` columns with numbers that we
specify. `num_range` requires (at least) two things: the text prefix,
followed by a vector of numbers that are to be glued onto the
prefix. I defined that first so as not to clutter up the
`select` line. The third thing here is `width`: all the
`PERS` columns have a name that ends with two digits, so
`PERS03` rather than `PERS3`, and using `width`
makes sure that the zero gets inserted.

Individual 340 is a low scorer on factor 1, so they should have low
scores on the first five items (the ones with positive loading on
factor 1) and high scores on the last four. This is indeed what
happened: 1s, 2s and 3s on the first five items and 4s and 5s on the
last four. 
Having struggled through that, factors 2 and 3 are repeats of
this. The high loaders on factor 2 are the ones shown in `f2`
below, with the first five loading positively and the last three
negatively.\endnote{I think the last four items in the entire survey
are different; otherwise the total number of items would be a
multiple of 5.}

```{r }
f2=c(1,11,16,26,36,6,21,31)
scores.1 %>% mutate(abso=abs(Factor2)) %>% 
filter(abso==max(abso)) %>%
select(id,Factor2,num_range("PERS",f2,width=2)) %>%
print(width=Inf)
```

 

What kind of idiot, I was thinking, named the data frame of scores
`scores.1` when there are going to be three factors to assess?

The first five scores are lowish, but the last three are definitely
high (three 5s). This idea of a low score on the positive-loading
items and a high score on the negatively-loading ones is entirely
consistent with a negative factor score.

Finally, factor 3, which loads highly on  items 4, 9 (neg), 14, 19, 24
(neg), 29, 34 (neg), 39. (Item 44, which you'd expect to be part of
this factor, is actually in factor 5.) First we see which individual
this is:

```{r }
f3=c(4,14,19,29,39,9,24,34)
scores.1 %>% mutate(abso=abs(Factor3)) %>% 
filter(abso==max(abso)) %>%
select(id,Factor3,num_range("PERS",f3,width=2)) %>%
print(width=Inf)
```

 

The only mysterious one there is item 19, which ought to be low,
because it has a positive loading and the factor score is unusually
negative. But it is 4 on a 5-point scale. The others that are supposed
to be low are 1 and the ones that are supposed to be high are 4 or 5, so
those all match up.


(i) Check the uniquenesses. Which one(s) seem unusually
high? Check these against the factor loadings. Are these what you
would expect?

Solution


Mine are
```{r }
pers.ok.1$uniquenesses
```

       

Yours will be different if you used a different number of factors. But
the procedure you follow will be the same as mine.

I think the highest of these is 0.9307, for item 35. Also high is item
17, 0.8979. If you look back at the table of loadings, item 35 has low
loadings on *all* the factors: the largest in size is only
0.180. The largest loading for item 17 is 0.277, on factor 4. This is
not high either. 

Looking down the loadings table, also item 41 has only a loading of
$-0.326$ on factor 5, so its uniqueness ought to be pretty high as
well. At 0.8561, it is.



