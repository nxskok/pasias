##  Crimes in San Francisco



The data in
[link](http://ritsokiguess.site/datafiles/sfcrime1.csv) is a subset
of a huge
dataset of crimes committed in San Francisco between 2003 and
2015. The variables are:



* `Dates`: the date and time of the crime

* `Category`: the category of crime, eg. "larceny" or
"vandalism" (response).

* `Descript`: detailed description of crime.

* `DayOfWeek`: the day of the week of the crime.

* `PdDistrict`: the name of the San Francisco Police
Department district in which the crime took place.

* `Resolution`: how the crime was resolved

* `Address`: approximate street address of crime

* `X`: longitude

* `Y`: latitude


Our aim is to see whether the category of crime depends on the day of
the week and the district in which it occurred. However, there are a
lot of crime categories, so we will focus on the top four
"interesting" ones, which are the ones included in this data file.

Some of the model-fitting takes a while (you'll see why below). If
you're using R Markdown, you can wait for the models to fit each time
you re-run your document, or insert `cache=T` in the top line
of your code chunk (the one with `r` in curly brackets in it,
above the actual code). Put a comma and the `cache=T` inside
the curly brackets.  What that does is to re-run that code chunk only
if it changes; if it hasn't changed it will use the saved results from
last time it was run. That can save you a lot of waiting around.



(a) Read in the data and display the dataset (or, at least,
part of it).


Solution


The usual:
```{r napoli,cache=T}
my_url <- "http://utsc.utoronto.ca/~butler/d29/sfcrime1.csv"
sfcrime <- read_csv(my_url)
sfcrime
```

     

This is a tidied-up version of the data, with only the variables we'll
look at, and only the observations from one of the "big four"
crimes, a mere 300,000 of them. This is the data set we created earlier.
    
$\blacksquare$

(b) Fit a multinomial logistic
regression that predicts crime category from day of week and
district. (You don't need to look at it.) The model-fitting produces
some output. (If you're using R Markdown, that will
come with it.)



Solution


The modelling part is easy enough, as long as you can get the
uppercase letters in the right places:

```{r roma,cache=T}
sfcrime.1 <- multinom(Category~DayOfWeek+PdDistrict,data=sfcrime)
```

 
  
$\blacksquare$

(c) Fit a model that predicts Category from only the
district. Hand in the output from the fitting process as well. 



Solution


Same idea. Write it out, or use `update`:
```{r sfcrime-1 }
sfcrime.2 <- update(sfcrime.1,.~.-DayOfWeek)
```

   
  
$\blacksquare$

(d) Use `anova` to compare the two models you just
obtained. What does the `anova` tell you?



Solution


This:

```{r sfcrime-2 }
anova(sfcrime.2,sfcrime.1)
```

 

This is a very small P-value. The null hypothesis is that the two
models are equally good, and this is clearly rejected. We need the
bigger model: that is, we need to keep `DayOfWeek` in there,
because the pattern of crimes (in each district) differs over day of
week. 

One reason the P-value came out so small is that we have a ton of
data, so that even a very small difference between days of the week
could come out very strongly significant. The Machine Learning people
(this is a machine learning dataset) don't worry so much about tests
for that reason: they are more concerned with predicting things well,
so they just throw everything into the model and see what comes out.
  

$\blacksquare$

(e) Using your preferred model, obtain predicted probabilities
that a crime will be of each of these four categories for each day of
the week in the `TENDERLOIN` district (the name is ALL
CAPS). This will mean constructing a data frame to predict from,
obtaining the predictions and then displaying them suitably.



Solution

There is an easier way, which takes a bit of adapting (and takes a bit of time to run):

```{r, cache=TRUE}
p <- predictions(sfcrime.1, variables = c("DayOfWeek", "PdDistrict"), type = "probs")
p
```

This is because it includes all the districts, not just the one we want, so:

- grab only the district we want
- (as usual for these) grab only the columns we want
- pivot wider to get a column for each group (crime) we want a predicted probability for:

```{r}
p %>% filter(PdDistrict == "TENDERLOIN") %>% 
  select(group, predicted, DayOfWeek) %>% 
  pivot_wider(names_from = group, values_from = predicted)
```

A better way to do this is to use `datagrid` first to get the combinations you want (and only those), namely all the days of the week, but only the district called TENDERLOIN.^[Better, because it doesn't do predictions for all the other districts as well, only to throw them away, which is what we did above. When the predictions take so long, it's worth trying to speed it up.]

So, let's get the days of the week. The easiest way is to count them and ignore the counts:^[I know I just said not to do this kind of thing, but counting is really fast, while unnecessary predictions are really slow.]

```{r}
sfcrime %>% count(DayOfWeek) %>% 
  pull(DayOfWeek) -> daysofweek
daysofweek
```

Now we can use these in `datagrid`:^[There is only one district, so we can just put that in here.]

```{r}
new <- datagrid(model = sfcrime.1, DayOfWeek = daysofweek, PdDistrict = "TENDERLOIN")
new
```

Good. And then predict for just these. This is slow, but not as slow as before. I'm saving the result of this slow part, so that it doesn't matter if I change my mind later about what to do with it. I want to make sure that I don't have to do it again, is all:^[The same thing applies if you are doing something like webscraping, that is to say downloading stuff from the web that you then want to do something with. Download it *once* and save it, then you can take as long as you need to decide what you're doing with it.]

```{r, cache=TRUE}
p <- predictions(sfcrime.1, newdata = new, type = "probs")
p
```

This, as you remember, is long format, so grab the columns you need from it and pivot wider. The easiest grab is to just get rid of the standard error column (since you want only one number to pivot-wider):

```{r}
p %>% 
  select(-std.error) %>% 
  pivot_wider(names_from = group, values_from = predicted)
```

Success. (If you don't get rid of the standard errors, you still have 28 rows and a bunch of missing values; the standard errors are unique, and so `pivot_wider` will infer that everything should be in its own row.)

  
$\blacksquare$

(f) Describe briefly how the weekend days Saturday and Sunday
differ from the rest.



Solution


The days ended up in some quasi-random order, but Saturday and Sunday are
still together, so we can still easily compare them with the rest.
My take is that the last two columns don't differ much between
weekday and weekend, but the first two columns do: the probability
of a crime being an assault is a bit higher on the weekend, and the
probability of a crime being drug-related is a bit lower.
I will accept anything reasonable supported by the predictions you got.
We said there was a strongly significant day-of-week effect, but the
changes from weekday to weekend are actually pretty small (but the
changes from one weekday to another are even smaller). This supports
what I guessed before, that with this much data even a small effect
(the one shown here) is statistically
significant.^[Statistical significance as an idea grew up in    the days before "big data".]


Extra: I want to compare another district. What districts do we have?

```{r sfcrime-8 }
sfcrime %>% count(PdDistrict)
```

   

This is the number of our "big four" crimes committed in each
district. Let's look at the lowest-crime district `RICHMOND`. I
copy and paste my code. Since I want to compare two districts, I
include both:


```{r}
new <- datagrid(model = sfcrime.1, DayOfWeek = daysofweek, PdDistrict = c("TENDERLOIN", "RICHMOND"))
new
```

and then as we just did. I'm going to be a bit more selective about the columns I keep this time, since the display will be a bit wider and I don't want it to be too big for the page:

```{r, cache=TRUE}
p <- predictions(sfcrime.1, newdata = new, type = "probs")
p
```



```{r sfcrime-9, echo=F}
options(width=90)
```

 

```{r sfcrime-10, size="footnotesize"}
DayOfWeeks <- sfcrime %>% distinct(DayOfWeek) %>% pull(DayOfWeek)
PdDistricts <- c("RICHMOND","TENDERLOIN")
sfcrime.new <- crossing(DayOfWeek=DayOfWeeks,PdDistrict=PdDistricts)
sfcrime.new
p1 <- predict(sfcrime.1,sfcrime.new,type="probs")
d1 <- cbind(sfcrime.new,p1) ; d1
```

 

Richmond is obviously not a drug-dealing kind of place; most of its
crimes are theft of one kind or another. But the predicted effect of
weekday vs.\ weekend is the same: Richmond doesn't have many assaults
or drug crimes, but it also has more assaults and fewer drug crimes on
the weekend than during the week. There is not much effect of day of
the week on the other two crime types in either place.

The consistency of *pattern*, even though the prevalence of the
different crime types differs by location, is a feature of the model:
we fitted an additive model, that says there is an effect of weekday,
and *independently* there is an effect of location. The
*pattern* over weekday is the same for each location, implied by
the model. This may or may not be supported by the actual data.

The way to assess this is to fit a model with interaction (we will see
more of this when we revisit ANOVA later), and compare the fit:

```{r model-three-one,cache=T} 
sfcrime.3 <- update(sfcrime.1,.~.+DayOfWeek*PdDistrict)
# anova(sfcrime.1,sfcrime.3)
```

 

This one didn't actually complete the fitting process: it got to 100
times around and stopped (since that's the default limit). We can make
it go a bit further thus:

```{r model-three-two,cache=T}
sfcrime.3 <- update(sfcrime.1,.~.+DayOfWeek*PdDistrict,maxit=300)
anova(sfcrime.1,sfcrime.3)
```

 

This time, we got to the end. (The `maxit=300` gets passed on
to `multinom`, and says "go around up to 300 times if needed".) 
As you will see if you try it, this takes a bit of time to
run. 

This `anova` is also strongly significant, but in the light of
the previous discussion, the differential effect of day of week in
different districts might not be very big. We can even assess that; we
have all the machinery for the predictions, and we just have to apply
them to this model:

```{r sfcrime-11, size="footnotesize"}
p3 <- predict(sfcrime.3,sfcrime.new,type="probs")
d3 <- cbind(sfcrime.new,p3) ; d3
```

 

It doesn't *look* much different. Maybe the Tenderloin has a
larger weekend increase in assaults than Richmond does. I saved these
predictions in data frames so that I could compare them. Columns 3
through 6 contain the actual predictions;  let's take the difference
between them, and glue it back onto the district and day of week. I
rounded the prediction differences off to 4 decimals to make the
largest ones easier to find:

```{r sfcrime-12, size="footnotesize"}
pdiff <- round(p3-p1, 4)
cbind(sfcrime.new, pdiff)
```

 

None of the predicted probabilities differ by more than 0.04, which is
consistent with the size of the interaction effect being small even
though significant. Some of the differences largest in size are
Drugs-Narcotics in Tenderloin, but even they are not big.

The programmer in me wants to find a way to display the largest and
smallest few differences. My idea is to use `pivot_longer` to make
*one* column of differences and sort it, then 
display the top and bottom few values. Note that
I have to put those little "backticks" around `VEHICLE THEFT`
since the variable name has a space in it. My code inefficiently sorts
the differences twice:

```{r sfcrime-13 }
cbind(sfcrime.new,pdiff) %>% 
  pivot_longer(ASSAULT:`VEHICLE THEFT`, names_to="crimetype", values_to = "difference") -> d1
d1 %>% arrange(difference) %>% slice(1:6)
d1 %>% arrange(desc(difference)) %>% slice(1:6)
```

 

I don't know what, if anything, you make of those.

Extra: there is a better way of doing those in one go:

```{r sfcrime-14 }
d1 %>% top_n(6, difference)
d1 %>% top_n(6, -difference)
```

 

that is just slicker. What it
actually does is almost the same as my code, but it saves you worrying about
the details. (It picks out the top 6, but without putting them in
order.) 
  
$\blacksquare$


